{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO7f9fKxMNMxN/GD8u83a+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankeawthong/Project-1-Lita-Chatbot/blob/main/%5B20250920%5D%20IDS%20Domain-Shift%20Pipeline%20(v3-fast)%20LR-BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3) — Leakage-safe + Expanded MQTT filter\n",
        "--------------------------------------------------------------------\n",
        "- Drops label-like columns from features to prevent leakage\n",
        "- Aligns feature schemas between train and test (categorical one-hot + scaler from train)\n",
        "- Expanded MQTT filtering for NF-ToN-IoT (checks DST and SRC ports; common fallbacks)\n",
        "- Defaults to RESAMPLING=\"smote_tomek\" and LOSS_MODE=\"focal\"\n",
        "- Keeps ablations: LR-only, BiLSTM-only, LR→BiLSTM; FGSM/PGD adversarial mixing for sequence models\n",
        "- Metrics: Macro-F1, AUROC/PR-AUC, ECE/MCE, FPR@1e-3/1e-4; Per-class recall/F1; Reliability bins\n",
        "\n",
        "Outputs under ./outputs_v3/\n",
        "\n",
        "Author: Sine & Mentor"
      ],
      "metadata": {
        "id": "75WvVDW2Wgl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3 • FIXED)\n",
        "=====================================\n",
        "Key fixes based on crash/stall & 0.0 val-accuracy symptoms:\n",
        "- **Unified label mapping** (no LabelEncoder sorting surprises). We use a fixed\n",
        "  CLASS2IDX for the 6 families across *all* domains & stages.\n",
        "- **Hard guards for label range** and per-split class summaries (fail fast if a\n",
        "  label falls outside 0..C-1).\n",
        "- **Stable validation**: explicit internal train/val split instead of\n",
        "  `validation_split`, avoiding misaligned batches when concatenating adversarial\n",
        "  examples.\n",
        "- **Empty MQTT split**: skip gracefully.\n",
        "- **CPU-stable training**: default to CPU-only to avoid cuInit(303) & duplicate\n",
        "  CUDA plugin registrations seen in notebooks/containers. Can be toggled.\n",
        "- **Keras stability**: `workers=1`, `use_multiprocessing=False`,\n",
        "  `categorical_accuracy` metric, and tf.data pipelines.\n",
        "- **Benign index** based on CLASS2IDX (not list position); evaluation is\n",
        "  consistent with model output ordering.\n",
        "\n",
        "Outputs are written to ./outputs_v3/ as before.\n",
        "\n",
        "IDS Domain-Shift Pipeline (v3 • FIXED, Keras3-compatible)\n",
        "=========================================================\n",
        "This revision fixes the crash:\n",
        "  TypeError: TensorFlowTrainer.fit() got an unexpected keyword argument 'workers'\n",
        "\n",
        "Key changes\n",
        "-----------\n",
        "- **Removed** unsupported `workers`/`use_multiprocessing` args (Keras 3).\n",
        "- **Guarded resampling**: `SMOTETomek` is skipped for huge splits; use class\n",
        "  weights or light undersampling instead to avoid stalls/OOM.\n",
        "- **Adversarial cap**: upper-bound adversarial sample count to keep memory stable.\n",
        "- **CPU-stable default** retained (toggle via CONFIG[\"FORCE_CPU\"]).\n",
        "- **Unified label mapping** and per-split summaries kept.\n",
        "\n",
        "Outputs go to `./outputs_v3/`."
      ],
      "metadata": {
        "id": "EsdLCbTDXVd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3p, patched)\n",
        "Fixes:\n",
        "- Align scikit-learn predict_proba outputs (which only include present classes) to the\n",
        "  GLOBAL family schema before any downstream step (temperature scaling, LSTM features).\n",
        "- Temperature scaling now uses the global num_classes for one-hot (no index errors).\n",
        "- Keeps AUROC/PR-AUC \"present-class\" computation, threshold tuning, and MQTT relabel.\n",
        "\n",
        "Defaults: RESAMPLING=\"smote_tomek\", LOSS_MODE=\"focal\", EPOCHS=30\n",
        "Outputs: ./outputs_v3p/domain_shift_results_v3p.csv, *_perclass_v3p.csv, *_reliability_bins_v3p.csv\n",
        "\n",
        "IDS Domain-Shift Pipeline (v3-fast)\n",
        "==================================\n",
        "Performance/stability-focused revision of v3/v3p series.\n",
        "\n",
        "Key improvements\n",
        "----------------\n",
        "1) **Feature build is bounded**: only low-cardinality categoricals are one-hot\n",
        "   encoded (<= 50 uniques) to prevent 100k+ dummy columns from IP/MAC/etc.\n",
        "2) **Sparse LR path**: uses CSR matrices and 'saga' solver (n_jobs=-1) to\n",
        "   accelerate large, high-dim fits.\n",
        "3) **Adversarial generation**: only on a *random subset* (capped) and in\n",
        "   batches—no more generating adversarial examples for the full train split.\n",
        "4) **Resampling guard**: SMOTE-Tomek skipped above a cap; optional per-class\n",
        "   undersampling keeps memory bounded.\n",
        "5) **Progress logging**: timestamped logs for each stage so you can see where\n",
        "   time is spent.\n",
        "6) **Same metrics/outputs** as v3: Macro-F1, Macro-Recall, AUROC(PR-OVR),\n",
        "   calibration (ECE/MCE), FPR@1e-3/1e-4, per-class tables, reliability bins.\n",
        "\n",
        "Outputs: ./outputs_v3fast/"
      ],
      "metadata": {
        "id": "bdi98Sc3zViW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wd8_af7NWfpy"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os, time, math, warnings, gc\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import f1_score, recall_score, roc_auc_score, average_precision_score, roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "CONFIG = {\n",
        "    \"NF_TON_IOT_CSV\": \"Dataset_NF-ToN-IoT.csv\",\n",
        "    \"CIC_IOMT_TRAIN_CSV\": \"CIC_IoMT_2024_WiFi_MQTT_train.csv\",\n",
        "    \"CIC_IOMT_TEST_CSV\":  \"CIC_IoMT_2024_WiFi_MQTT_test.csv\",\n",
        "\n",
        "    \"NF_NUMERIC_CLASS_COL\": \"Class\",\n",
        "    \"NF_TEXT_LABEL_COL\": \"Attack\",\n",
        "    \"CIC_CLASS_COL\": \"Class\",\n",
        "    \"CIC_TEXT_LABEL_COL\": \"label\",\n",
        "\n",
        "    \"NF_DST_PORT_CANDIDATES\": [\"L4_DST_PORT\", \"Dst Port\", \"dst_port\", \"Destination Port\", \"dport\", \"dstport\"],\n",
        "    \"NF_SRC_PORT_CANDIDATES\": [\"L4_SRC_PORT\", \"Src Port\", \"src_port\", \"Source Port\", \"sport\", \"srcport\"],\n",
        "    \"MQTT_PORTS\": [1883, 8883],\n",
        "\n",
        "    \"SEED\": 42,\n",
        "    \"EPOCHS\": 30,\n",
        "    \"BATCH_SIZE\": 128,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"PATIENCE\": 5,\n",
        "\n",
        "    \"RESAMPLING\": \"smote_tomek\",            # \"none\" | \"smote_tomek\"\n",
        "    \"LOSS_MODE\": \"focal\",                   # \"ce\" | \"class_balanced_ce\" | \"focal\"\n",
        "    \"RESAMPLING_MAX_N\": 200_000,            # skip SMOTE if above\n",
        "    \"UNDERSAMPLE_MAX_PER_CLASS\": 100_000,   # light undersample cap/class\n",
        "\n",
        "    \"USE_ADV_TRAINING\": True,\n",
        "    \"ADV_METHOD\": \"fgsm\",                   # \"fgsm\" | \"pgd\"\n",
        "    \"FGSM_EPS\": 0.05,\n",
        "    \"PGD_EPS\": 0.03,\n",
        "    \"PGD_STEPS\": 5,\n",
        "    \"PGD_ALPHA\": 0.01,\n",
        "    \"ADV_RATIO\": 0.5,\n",
        "    \"ADV_MAX_SAMPLES\": 40_000,              # generate at most this many\n",
        "    \"ADV_BATCH\": 8_000,                     # batch size for adversarial synth\n",
        "\n",
        "    \"FORCE_CPU\": True,\n",
        "    \"OUTDIR\": \"./outputs_v3fast\",\n",
        "\n",
        "    # New: categorical guard\n",
        "    \"CATEGORICAL_MAX_CARD\": 50,             # one-hot only if <= 50 uniques\n",
        "    \"MAX_DUMMIES\": 800                      # overall dummy cap; else drop cats\n",
        "}\n",
        "\n",
        "np.random.seed(CONFIG[\"SEED\"])\n",
        "tf.random.set_seed(CONFIG[\"SEED\"])\n",
        "\n",
        "if CONFIG[\"FORCE_CPU\"]:\n",
        "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
        "    try:\n",
        "        tf.config.set_visible_devices([], 'GPU')\n",
        "    except Exception:\n",
        "        pass\n",
        "os.makedirs(CONFIG[\"OUTDIR\"], exist_ok=True)\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S1ilr6qZWp0h",
        "outputId": "c754ff57-8206-4abc-d9a1-1c39cc5a74d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Families\n",
        "# --------------------\n",
        "FAMILIES = [\"Benign\", \"DoS_DDoS\", \"Recon_Scan\", \"MQTT\", \"Spoof\", \"Other\"]\n",
        "CLASS2IDX = {n:i for i,n in enumerate(FAMILIES)}\n",
        "IDX2CLASS = {i:n for n,i in CLASS2IDX.items()}\n",
        "NUM_CLASSES = len(FAMILIES)\n",
        "BENIGN_IDX = CLASS2IDX[\"Benign\"]"
      ],
      "metadata": {
        "id": "oxNUuGomWpxu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Utils\n",
        "# --------------------\n",
        "def log(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
        "\n",
        "def map_label_string_to_family(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"Other\"\n",
        "    st = s.lower()\n",
        "    if \"benign\" in st or st.strip()==\"normal\": return \"Benign\"\n",
        "    if \"mqtt\" in st: return \"MQTT\"\n",
        "    if \"ddos\" in st or \"dos\" in st: return \"DoS_DDoS\"\n",
        "    if \"scan\" in st or \"recon\" in st or \"portscan\" in st: return \"Recon_Scan\"\n",
        "    if \"spoof\" in st or \"mitm\" in st or \"impersonat\" in st or \"man-in-the-middle\" in st: return \"Spoof\"\n",
        "    return \"Other\"\n",
        "\n",
        "def _first_col(df: pd.DataFrame, candidates: List[str]):\n",
        "    for c in candidates:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def mqtt_filter_nf(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ports = set(map(str, CONFIG[\"MQTT_PORTS\"]))\n",
        "    dcol = _first_col(df, CONFIG[\"NF_DST_PORT_CANDIDATES\"])\n",
        "    scol = _first_col(df, CONFIG[\"NF_SRC_PORT_CANDIDATES\"])\n",
        "    mask = np.zeros(len(df), dtype=bool)\n",
        "    if dcol is not None: mask |= df[dcol].astype(str).isin(ports).values\n",
        "    if scol is not None: mask |= df[scol].astype(str).isin(ports).values\n",
        "    f = df[mask]\n",
        "    if len(f)==0:\n",
        "        log(\"[WARN] MQTT filter produced empty set; using full NF-ToN-IoT.\")\n",
        "        return df.copy()\n",
        "    return f.copy()\n",
        "\n",
        "def _drop_label_like_columns(Xdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    KEYS = ['label','Label','labels','Labels','Class','class','Labal','labal',\n",
        "            'Attack','attack','Target','target','Family','family','Benign','benign']\n",
        "    return Xdf.drop(columns=[c for c in Xdf.columns if any(k in c for k in KEYS)], errors='ignore')\n",
        "\n",
        "def build_features_train_lowcard(df: pd.DataFrame, y_col: str):\n",
        "    \"\"\"Build bounded features: numeric + low-card categoricals only.\"\"\"\n",
        "    df = df.copy()\n",
        "    y = df[y_col].values\n",
        "    Xdf = _drop_label_like_columns(df.drop(columns=[y_col]))\n",
        "\n",
        "    Xnum = Xdf.select_dtypes(include=[np.number]).copy().fillna(0)\n",
        "\n",
        "    Xcat_raw = Xdf.select_dtypes(exclude=[np.number]).astype(str)\n",
        "    keep = []\n",
        "    if Xcat_raw.shape[1]>0:\n",
        "        for c in Xcat_raw.columns:\n",
        "            u = Xcat_raw[c].nunique(dropna=False)\n",
        "            if 1 < u <= CONFIG[\"CATEGORICAL_MAX_CARD\"]:\n",
        "                keep.append(c)\n",
        "    Xcat = Xcat_raw[keep] if keep else pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    if Xcat.shape[1]>0:\n",
        "        Xcat = pd.get_dummies(Xcat, dummy_na=False, drop_first=False)\n",
        "    else:\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    # If dummy explosion still too big, drop categoricals altogether\n",
        "    if Xcat.shape[1] > CONFIG[\"MAX_DUMMIES\"]:\n",
        "        log(f\"[INFO] Dropping categoricals (dummies={Xcat.shape[1]} > {CONFIG['MAX_DUMMIES']}).\")\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    Xall = pd.concat([Xnum, Xcat], axis=1)\n",
        "    scaler = StandardScaler().fit(Xall.values)\n",
        "    X = scaler.transform(Xall.values)\n",
        "    cols = Xall.columns.tolist()\n",
        "    return X, y, scaler, cols\n",
        "\n",
        "def build_features_apply_lowcard(df: pd.DataFrame, y_col: str, scaler: StandardScaler, cols_schema: list):\n",
        "    df = df.copy()\n",
        "    y = df[y_col].values\n",
        "    Xdf = _drop_label_like_columns(df.drop(columns=[y_col]))\n",
        "\n",
        "    Xnum = Xdf.select_dtypes(include=[np.number]).copy().fillna(0)\n",
        "\n",
        "    Xcat_raw = Xdf.select_dtypes(exclude=[np.number]).astype(str)\n",
        "    keep = []\n",
        "    if Xcat_raw.shape[1]>0:\n",
        "        for c in Xcat_raw.columns:\n",
        "            u = Xcat_raw[c].nunique(dropna=False)\n",
        "            if 1 < u <= CONFIG[\"CATEGORICAL_MAX_CARD\"]:\n",
        "                keep.append(c)\n",
        "    Xcat = Xcat_raw[keep] if keep else pd.DataFrame(index=Xdf.index)\n",
        "    if Xcat.shape[1]>0:\n",
        "        Xcat = pd.get_dummies(Xcat, dummy_na=False, drop_first=False)\n",
        "    else:\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    Xall = pd.concat([Xnum, Xcat], axis=1).reindex(columns=cols_schema, fill_value=0)\n",
        "    X = scaler.transform(Xall.values)\n",
        "    return X, y\n",
        "\n",
        "def encode_family_series(series: pd.Series) -> np.ndarray:\n",
        "    vals = [CLASS2IDX[map_label_string_to_family(v)] for v in series.astype(str).values]\n",
        "    return np.asarray(vals, dtype=np.int32)\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=None):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n",
        "        ce = -y_true * tf.math.log(y_pred)\n",
        "        if alpha is not None: ce = alpha * ce\n",
        "        weight = tf.pow(1.0 - y_pred, gamma)\n",
        "        return tf.reduce_sum(weight * ce, axis=1)\n",
        "    return loss\n",
        "\n",
        "def build_bilstm(input_shape, num_classes):\n",
        "    return models.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(1e-4)), input_shape=input_shape),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Bidirectional(layers.LSTM(32, kernel_regularizer=regularizers.l2(1e-4))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=regularizers.l2(1e-4))\n",
        "    ])\n",
        "\n",
        "def fgsm(model, x, y, eps=0.05):\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        pred = model(x, training=False)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y, pred)\n",
        "    grad = tape.gradient(loss, x)\n",
        "    x_adv = x + eps * tf.sign(grad)\n",
        "    return tf.clip_by_value(x_adv, -10, 10).numpy()\n",
        "\n",
        "def pgd(model, x, y, eps=0.03, alpha=0.01, steps=5):\n",
        "    x0 = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    x_adv = tf.identity(x0)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x_adv)\n",
        "            pred = model(x_adv, training=False)\n",
        "            loss = tf.keras.losses.categorical_crossentropy(y, pred)\n",
        "        grad = tape.gradient(loss, x_adv)\n",
        "        x_adv = x_adv + alpha * tf.sign(grad)\n",
        "        x_adv = tf.clip_by_value(x_adv, x0 - eps, x0 + eps)\n",
        "    return x_adv.numpy()\n",
        "\n",
        "def calibration_bins(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15):\n",
        "    conf = probs.max(axis=1); preds = probs.argmax(axis=1)\n",
        "    correct = (preds==y_true).astype(int)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    ece = mce = 0.0; rows = []\n",
        "    for i in range(n_bins):\n",
        "        lo, hi = bins[i], bins[i+1]\n",
        "        idx = np.where((conf>=lo) & (conf<hi))[0]\n",
        "        if len(idx)==0:\n",
        "            rows.append((0.5*(lo+hi), np.nan, 0)); continue\n",
        "        acc = correct[idx].mean(); conf_mean = conf[idx].mean()\n",
        "        gap = abs(acc-conf_mean)\n",
        "        ece += (len(idx)/len(conf))*gap; mce = max(mce, gap)\n",
        "        rows.append((conf_mean, acc, len(idx)))\n",
        "    return ece, mce, rows\n",
        "\n",
        "def evaluate_multiclass(y_true, proba) -> Tuple[float, float, float, float]:\n",
        "    classes = np.arange(NUM_CLASSES)\n",
        "    Yb = label_binarize(y_true, classes=classes)\n",
        "    try:\n",
        "        roc = roc_auc_score(Yb, proba, average='macro', multi_class='ovr')\n",
        "    except Exception:\n",
        "        roc = float('nan')\n",
        "    try:\n",
        "        pr = average_precision_score(Yb, proba, average='macro')\n",
        "    except Exception:\n",
        "        pr = float('nan')\n",
        "    macro_f1 = f1_score(y_true, proba.argmax(axis=1), average='macro')\n",
        "    macro_rec = recall_score(y_true, proba.argmax(axis=1), average='macro')\n",
        "    return macro_f1, macro_rec, roc, pr\n",
        "\n",
        "def fpr_at_threshold(y_true_bin: np.ndarray, attack_scores: np.ndarray, target_fpr: float):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, attack_scores)[:3]\n",
        "    idx = np.where(fpr <= target_fpr)[0]\n",
        "    if len(idx)==0:\n",
        "        j = int(np.argmin(fpr))\n",
        "        return float(thr[j]), float(fpr[j]), float(tpr[j])\n",
        "    j = idx[-1]\n",
        "    return float(thr[j]), float(fpr[j]), float(tpr[j])\n",
        "\n",
        "@dataclass\n",
        "class RunResult:\n",
        "    setting: str\n",
        "    model_name: str\n",
        "    use_adv: bool\n",
        "    resampling: str\n",
        "    loss_mode: str\n",
        "    macro_f1: float\n",
        "    macro_recall: float\n",
        "    roc_auc_ovr: float\n",
        "    pr_auc_ovr: float\n",
        "    ece: float\n",
        "    mce: float\n",
        "    fpr1e3: float\n",
        "    tpr_at_fpr1e3: float\n",
        "    fpr1e4: float\n",
        "    tpr_at_fpr1e4: float"
      ],
      "metadata": {
        "id": "D0h8A5-EWpu3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Main\n",
        "# --------------------\n",
        "def main():\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    log(\"Loading CSVs...\")\n",
        "    nf = pd.read_csv(CONFIG[\"NF_TON_IOT_CSV\"])\n",
        "    cic_tr = pd.read_csv(CONFIG[\"CIC_IOMT_TRAIN_CSV\"])\n",
        "    cic_te = pd.read_csv(CONFIG[\"CIC_IOMT_TEST_CSV\"])\n",
        "\n",
        "    log(\"Mapping families...\")\n",
        "    nf_family_all = nf.get(CONFIG[\"NF_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(nf))).astype(str).apply(map_label_string_to_family)\n",
        "    cic_tr_family = cic_tr.get(CONFIG[\"CIC_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(cic_tr))).astype(str).apply(map_label_string_to_family)\n",
        "    cic_te_family = cic_te.get(CONFIG[\"CIC_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(cic_te))).astype(str).apply(map_label_string_to_family)\n",
        "\n",
        "    cic_tr2 = cic_tr.copy(); cic_tr2[\"Family\"] = cic_tr_family.values\n",
        "    cic_te2 = cic_te.copy(); cic_te2[\"Family\"] = cic_te_family.values\n",
        "\n",
        "    log(\"Applying MQTT filter to NF-ToN-IoT...\")\n",
        "    nf_mqtt = mqtt_filter_nf(nf)\n",
        "    nf2 = nf_mqtt.copy()\n",
        "    nf2[\"Family\"] = nf_family_all.loc[nf_mqtt.index].values\n",
        "\n",
        "    # splits\n",
        "    y_nf_enc = encode_family_series(nf2[\"Family\"])\n",
        "    log(\"Splitting NF train/test...\")\n",
        "    nf_train, nf_test = train_test_split(nf2, test_size=0.2, random_state=CONFIG[\"SEED\"], stratify=y_nf_enc)\n",
        "\n",
        "    results = []\n",
        "    perclass_rows = []\n",
        "    relbin_rows = []\n",
        "\n",
        "    def run_setting(train_domain: str, test_domain: str, model_kind: str, use_adv: bool) -> RunResult:\n",
        "        log(f\"=== Setting: {train_domain} → {test_domain} | {model_kind} | adv={use_adv} ===\")\n",
        "\n",
        "        df_tr = cic_tr2 if train_domain==\"IoMT\" else nf_train\n",
        "        df_te = cic_te2 if test_domain==\"IoMT\" else (nf_test if (train_domain==\"IoT\" and test_domain==\"IoT\") else nf2)\n",
        "\n",
        "        df_tr = df_tr.copy(); df_te = df_te.copy()\n",
        "        df_tr[\"y_enc\"] = encode_family_series(df_tr[\"Family\"])\n",
        "        df_te[\"y_enc\"] = encode_family_series(df_te[\"Family\"])\n",
        "\n",
        "        # Features\n",
        "        log(\"Building features (train)...\")\n",
        "        Xtr, ytr, scaler, cols_schema = build_features_train_lowcard(df_tr.drop(columns=[\"Family\"]).rename(columns={\"y_enc\":\"Target\"}), \"Target\")\n",
        "        log(f\"Train matrix: {Xtr.shape}\")\n",
        "\n",
        "        log(\"Building features (test)...\")\n",
        "        Xte, yte = build_features_apply_lowcard(df_te.drop(columns=[\"Family\"]).rename(columns={\"y_enc\":\"Target\"}), \"Target\", scaler, cols_schema)\n",
        "        log(f\"Test  matrix: {Xte.shape}\")\n",
        "\n",
        "        # Resampling (guarded)\n",
        "        if CONFIG[\"RESAMPLING\"]==\"smote_tomek\" and len(ytr) <= CONFIG[\"RESAMPLING_MAX_N\"]:\n",
        "            log(\"Applying SMOTE-Tomek...\")\n",
        "            Xtr, ytr = SMOTETomek(random_state=CONFIG[\"SEED\"]).fit_resample(Xtr, ytr)\n",
        "            log(f\"Resampled train: {Xtr.shape}\")\n",
        "        elif CONFIG[\"RESAMPLING\"]==\"smote_tomek\":\n",
        "            log(\"[INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\")\n",
        "            rng = np.random.default_rng(CONFIG[\"SEED\"])\n",
        "            Xo=[]; yo=[]\n",
        "            for c in np.unique(ytr):\n",
        "                idx = np.where(ytr==c)[0]\n",
        "                k = min(len(idx), CONFIG[\"UNDERSAMPLE_MAX_PER_CLASS\"])\n",
        "                sel = rng.choice(idx, size=k, replace=False)\n",
        "                Xo.append(Xtr[sel]); yo.append(ytr[sel])\n",
        "            Xtr = np.concatenate(Xo, axis=0); ytr = np.concatenate(yo, axis=0)\n",
        "            log(f\"Undersampled train: {Xtr.shape}\")\n",
        "\n",
        "        # Branches\n",
        "        if model_kind==\"LR-only\":\n",
        "            log(\"Fitting LogisticRegression (sparse, saga)...\")\n",
        "            Xtr_csr, Xte_csr = csr_matrix(Xtr), csr_matrix(Xte)\n",
        "            lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
        "            lr.fit(Xtr_csr, ytr)\n",
        "            proba_te = lr.predict_proba(Xte_csr)\n",
        "\n",
        "        else:\n",
        "            # Sequence preparation\n",
        "            Xtr_seq = Xtr.reshape((Xtr.shape[0],1,Xtr.shape[1]))\n",
        "            Xte_seq = Xte.reshape((Xte.shape[0],1,Xte.shape[1]))\n",
        "            Xtr_seq_tr, Xtr_seq_val, ytr_tr, ytr_val = train_test_split(Xtr_seq, ytr, test_size=0.2, random_state=CONFIG[\"SEED\"], stratify=ytr)\n",
        "            ytr_tr_oh = to_categorical(ytr_tr, num_classes=NUM_CLASSES)\n",
        "            ytr_val_oh = to_categorical(ytr_val, num_classes=NUM_CLASSES)\n",
        "\n",
        "            if CONFIG[\"LOSS_MODE\"]==\"class_balanced_ce\":\n",
        "                classes = np.arange(NUM_CLASSES)\n",
        "                weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytr_tr)\n",
        "                cw = {int(c): float(w) for c, w in zip(classes, weights)}\n",
        "                loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "            elif CONFIG[\"LOSS_MODE\"]==\"focal\":\n",
        "                cw=None; loss_fn=focal_loss(gamma=2.0)\n",
        "            else:\n",
        "                cw=None; loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "            if model_kind==\"BiLSTM-only\":\n",
        "                model = build_bilstm(Xtr_seq.shape[1:], NUM_CLASSES)\n",
        "                Xtrain_base = Xtr_seq_tr\n",
        "            elif model_kind==\"LR->BiLSTM\":\n",
        "                log(\"Pre-fitting LR for LR->BiLSTM features...\")\n",
        "                lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
        "                lr.fit(csr_matrix(Xtr), ytr)\n",
        "                Xlr_tr = lr.predict_proba(Xtr)[:, np.newaxis, :]\n",
        "                Xlr_te = lr.predict_proba(Xte)[:, np.newaxis, :]\n",
        "                Xtr_seq = Xlr_tr; Xte_seq = Xlr_te\n",
        "                Xtr_seq_tr, Xtr_seq_val, ytr_tr, ytr_val = train_test_split(Xtr_seq, ytr, test_size=0.2, random_state=CONFIG[\"SEED\"], stratify=ytr)\n",
        "                ytr_tr_oh = to_categorical(ytr_tr, num_classes=NUM_CLASSES)\n",
        "                ytr_val_oh = to_categorical(ytr_val, num_classes=NUM_CLASSES)\n",
        "                model = build_bilstm(Xtr_seq_tr.shape[1:], NUM_CLASSES)\n",
        "                Xtrain_base = Xtr_seq_tr\n",
        "            else:\n",
        "                raise ValueError(\"Unknown model_kind\")\n",
        "\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(CONFIG[\"LEARNING_RATE\"]), loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "            es = callbacks.EarlyStopping(patience=CONFIG[\"PATIENCE\"], restore_best_weights=True, monitor='val_loss')\n",
        "\n",
        "            # Optional adversarial training (subset + batched)\n",
        "            Xtrain_in, ytrain_in = Xtrain_base, ytr_tr_oh\n",
        "            if CONFIG[\"USE_ADV_TRAINING\"] and use_adv:\n",
        "                log(\"Warmup (3 epochs) before adversarial mix...\")\n",
        "                model.fit(Xtr_seq_tr, ytr_tr_oh, validation_data=(Xtr_seq_val, ytr_val_oh), epochs=3, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
        "\n",
        "                rng = np.random.default_rng(CONFIG[\"SEED\"])\n",
        "                n_target = min(int(CONFIG[\"ADV_RATIO\"] * Xtr_seq_tr.shape[0]), CONFIG[\"ADV_MAX_SAMPLES\"])\n",
        "                if n_target > 0:\n",
        "                    sel = rng.choice(Xtr_seq_tr.shape[0], size=n_target, replace=False)\n",
        "                    log(f\"Generating adversarial subset: n={n_target} (batched)...\")\n",
        "                    adv_list = []\n",
        "                    for i in range(0, n_target, CONFIG[\"ADV_BATCH\"]):\n",
        "                        j = min(i+CONFIG[\"ADV_BATCH\"], n_target)\n",
        "                        batch_x = Xtr_seq_tr[sel[i:j]]\n",
        "                        batch_y = ytr_tr_oh[sel[i:j]]\n",
        "                        if CONFIG[\"ADV_METHOD\"]==\"fgsm\":\n",
        "                            adv = fgsm(model, batch_x, batch_y, eps=CONFIG[\"FGSM_EPS\"])\n",
        "                        else:\n",
        "                            adv = pgd(model, batch_x, batch_y, eps=CONFIG[\"PGD_EPS\"], alpha=CONFIG[\"PGD_ALPHA\"], steps=CONFIG[\"PGD_STEPS\"])\n",
        "                        adv_list.append(adv)\n",
        "                    X_adv = np.concatenate(adv_list, axis=0)\n",
        "                    y_adv = ytr_tr_oh[sel[:X_adv.shape[0]]]\n",
        "                    Xtrain_in = np.concatenate([Xtr_seq_tr, X_adv], axis=0)\n",
        "                    ytrain_in = np.concatenate([ytr_tr_oh, y_adv], axis=0)\n",
        "                    del adv_list, X_adv, y_adv; gc.collect()\n",
        "\n",
        "            log(\"Training sequence model...\")\n",
        "            model.fit(Xtrain_in, ytrain_in, validation_data=(Xtr_seq_val, ytr_val_oh), epochs=CONFIG[\"EPOCHS\"], batch_size=CONFIG[\"BATCH_SIZE\"], callbacks=[es], verbose=2)\n",
        "\n",
        "            log(\"Predicting on test...\")\n",
        "            proba_te = model.predict(Xte_seq, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
        "\n",
        "        # Metrics\n",
        "        log(\"Scoring...\")\n",
        "        yte_enc = df_te[\"y_enc\"].values.astype(int)\n",
        "        macro_f1, macro_rec, roc_ovr, pr_ovr = evaluate_multiclass(yte_enc, proba_te)\n",
        "        ece, mce, rel = calibration_bins(proba_te, yte_enc, n_bins=15)\n",
        "\n",
        "        atk_scores = 1.0 - proba_te[:, BENIGN_IDX]\n",
        "        y_bin = (yte_enc != BENIGN_IDX).astype(int)\n",
        "        thr1, fpr1, tpr1 = fpr_at_threshold(y_bin, atk_scores, 1e-3)\n",
        "        thr2, fpr2, tpr2 = fpr_at_threshold(y_bin, atk_scores, 1e-4)\n",
        "\n",
        "        # Per-class\n",
        "        y_pred = proba_te.argmax(axis=1)\n",
        "        for c in range(NUM_CLASSES):\n",
        "            idxs = np.where(yte_enc==c)[0]\n",
        "            if len(idxs)==0:\n",
        "                rec=f1=np.nan; sup=0\n",
        "            else:\n",
        "                tp=int(np.sum(y_pred[idxs]==c)); fn=int(len(idxs)-tp); fp=int(np.sum((y_pred==c)&(yte_enc!=c)))\n",
        "                rec = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "                prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "                f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
        "                sup=len(idxs)\n",
        "            perclass_rows.append({\"setting\": f\"{train_domain}->{test_domain}\", \"model_name\": model_kind, \"use_adv\": use_adv, \"class\": IDX2CLASS[c], \"recall\": float(rec), \"f1\": float(f1), \"support\": int(sup)})\n",
        "\n",
        "        for (cm, ac, ct) in rel:\n",
        "            relbin_rows.append({\"setting\": f\"{train_domain}->{test_domain}\", \"model_name\": model_kind, \"use_adv\": use_adv, \"conf_mean\": float(0.0 if cm!=cm else cm), \"acc\": float(0.0 if ac!=ac else ac), \"count\": int(ct)})\n",
        "\n",
        "        res = RunResult(setting=f\"{train_domain}->{test_domain}\", model_name=model_kind, use_adv=use_adv,\n",
        "                        resampling=CONFIG[\"RESAMPLING\"], loss_mode=CONFIG[\"LOSS_MODE\"],\n",
        "                        macro_f1=float(macro_f1), macro_recall=float(macro_rec),\n",
        "                        roc_auc_ovr=float(roc_ovr) if not np.isnan(roc_ovr) else np.nan,\n",
        "                        pr_auc_ovr=float(pr_ovr) if not np.isnan(pr_ovr) else np.nan,\n",
        "                        ece=float(ece), mce=float(mce), fpr1e3=float(fpr1), tpr_at_fpr1e3=float(tpr1),\n",
        "                        fpr1e4=float(fpr2), tpr_at_fpr1e4=float(tpr2))\n",
        "        log(f\"Done setting: {res}\")\n",
        "        return res\n",
        "\n",
        "    all_results = []\n",
        "    for model_kind in [\"LR-only\", \"BiLSTM-only\", \"LR->BiLSTM\"]:\n",
        "        all_results.append(run_setting(\"IoMT\",\"IoMT\",model_kind, use_adv=False))\n",
        "        all_results.append(run_setting(\"IoT\",\"IoT\",model_kind, use_adv=False))\n",
        "        all_results.append(run_setting(\"IoMT\",\"IoT\",model_kind, use_adv=(model_kind!=\"LR-only\")))\n",
        "        all_results.append(run_setting(\"IoT\",\"IoMT\",model_kind, use_adv=(model_kind!=\"LR-only\")))\n",
        "\n",
        "    df = pd.DataFrame([asdict(r) for r in all_results])\n",
        "    out_csv = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_results_v3fast.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "    out_pc = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_perclass_v3fast.csv\")\n",
        "    pd.DataFrame(perclass_rows).to_csv(out_pc, index=False)\n",
        "\n",
        "    out_rel = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_reliability_bins_v3fast.csv\")\n",
        "    pd.DataFrame(relbin_rows).to_csv(out_rel, index=False)\n",
        "\n",
        "    log(f\"Saved: {out_csv}\")\n",
        "    log(f\"Saved: {out_pc}\")\n",
        "    log(f\"Saved: {out_rel}\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9LxqpPC4eX",
        "outputId": "5017746d-4ce5-4a46-b347-652b7a54b5b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12:58:16] Loading CSVs...\n",
            "[12:58:21] Mapping families...\n",
            "[12:58:22] Applying MQTT filter to NF-ToN-IoT...\n",
            "[12:58:22] [WARN] MQTT filter produced empty set; using full NF-ToN-IoT.\n",
            "[12:58:22] Splitting NF train/test...\n",
            "[12:58:23] === Setting: IoMT → IoMT | LR-only | adv=False ===\n",
            "[12:58:23] Building features (train)...\n",
            "[12:58:24] Train matrix: (1048575, 45)\n",
            "[12:58:24] Building features (test)...\n",
            "[12:58:25] Test  matrix: (1048575, 45)\n",
            "[12:58:25] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[12:58:25] Undersampled train: (140396, 45)\n",
            "[12:58:25] Fitting LogisticRegression (sparse, saga)...\n",
            "[13:01:15] Scoring...\n",
            "[13:01:16] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.004003680868845366, macro_recall=0.04167851648725686, roc_auc_ovr=0.1882807087385367, pr_auc_ovr=0.4569332374828654, ece=0.8584862241959691, mce=0.9927770030081721, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[13:01:16] === Setting: IoT → IoT | LR-only | adv=False ===\n",
            "[13:01:17] Building features (train)...\n",
            "[13:01:17] Train matrix: (838860, 10)\n",
            "[13:01:17] Building features (test)...\n",
            "[13:01:17] Test  matrix: (209715, 10)\n",
            "[13:01:17] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:01:17] Undersampled train: (313772, 10)\n",
            "[13:01:17] Fitting LogisticRegression (sparse, saga)...\n",
            "[13:06:38] Scoring...\n",
            "[13:06:38] Done setting: RunResult(setting='IoT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.23452979339042768, macro_recall=0.25760888908038737, roc_auc_ovr=nan, pr_auc_ovr=0.31043207506023063, ece=0.4646707130742263, mce=0.7093220382110016, fpr1e3=3.0327843993570496e-05, tpr_at_fpr1e3=0.0, fpr1e4=3.0327843993570496e-05, tpr_at_fpr1e4=0.0)\n",
            "[13:06:38] === Setting: IoMT → IoT | LR-only | adv=False ===\n",
            "[13:06:39] Building features (train)...\n",
            "[13:06:40] Train matrix: (1048575, 45)\n",
            "[13:06:40] Building features (test)...\n",
            "[13:06:41] Test  matrix: (1048575, 45)\n",
            "[13:06:41] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:06:41] Undersampled train: (140396, 45)\n",
            "[13:06:41] Fitting LogisticRegression (sparse, saga)...\n",
            "[13:09:31] Scoring...\n",
            "[13:09:32] Done setting: RunResult(setting='IoMT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.15506902224447466, ece=0.0, mce=0.0, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[13:09:32] === Setting: IoT → IoMT | LR-only | adv=False ===\n",
            "[13:09:32] Building features (train)...\n",
            "[13:09:33] Train matrix: (838860, 10)\n",
            "[13:09:33] Building features (test)...\n",
            "[13:09:33] Test  matrix: (1048575, 10)\n",
            "[13:09:33] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:09:33] Undersampled train: (313772, 10)\n",
            "[13:09:33] Fitting LogisticRegression (sparse, saga)...\n",
            "[13:15:03] Scoring...\n",
            "[13:15:04] Done setting: RunResult(setting='IoT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.18704153611581187, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.2, ece=0.2978338472242301, mce=0.2978338472242301, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[13:15:04] === Setting: IoMT → IoMT | BiLSTM-only | adv=False ===\n",
            "[13:15:04] Building features (train)...\n",
            "[13:15:06] Train matrix: (1048575, 45)\n",
            "[13:15:06] Building features (test)...\n",
            "[13:15:06] Test  matrix: (1048575, 45)\n",
            "[13:15:06] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:15:06] Undersampled train: (140396, 45)\n",
            "[13:15:07] Training sequence model...\n",
            "Epoch 1/30\n",
            "878/878 - 11s - 12ms/step - categorical_accuracy: 0.9920 - loss: 0.0709 - val_categorical_accuracy: 0.9996 - val_loss: 0.0131\n",
            "Epoch 2/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0092 - val_categorical_accuracy: 0.9997 - val_loss: 0.0067\n",
            "Epoch 3/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0050 - val_categorical_accuracy: 0.9997 - val_loss: 0.0042\n",
            "Epoch 4/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0035 - val_categorical_accuracy: 0.9997 - val_loss: 0.0032\n",
            "Epoch 5/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0028 - val_categorical_accuracy: 0.9998 - val_loss: 0.0024\n",
            "Epoch 6/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0024 - val_categorical_accuracy: 0.9999 - val_loss: 0.0019\n",
            "Epoch 7/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0020 - val_categorical_accuracy: 0.9999 - val_loss: 0.0017\n",
            "Epoch 8/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0019 - val_categorical_accuracy: 0.9998 - val_loss: 0.0016\n",
            "Epoch 9/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9998 - val_loss: 0.0015\n",
            "Epoch 10/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0015 - val_categorical_accuracy: 0.9999 - val_loss: 0.0013\n",
            "Epoch 11/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9997 - val_loss: 0.0016\n",
            "Epoch 12/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9999 - val_loss: 0.0011\n",
            "Epoch 13/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0013 - val_categorical_accuracy: 0.9999 - val_loss: 0.0010\n",
            "Epoch 14/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0012 - val_categorical_accuracy: 0.9998 - val_loss: 0.0012\n",
            "Epoch 15/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9996 - val_loss: 0.0012\n",
            "Epoch 16/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 17/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.1176e-04\n",
            "Epoch 18/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 19/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 9.1277e-04\n",
            "Epoch 20/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 9.5486e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.1667e-04\n",
            "Epoch 21/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9998 - loss: 8.7985e-04 - val_categorical_accuracy: 0.9999 - val_loss: 7.0276e-04\n",
            "Epoch 22/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9998 - loss: 7.9856e-04 - val_categorical_accuracy: 0.9997 - val_loss: 9.9711e-04\n",
            "Epoch 23/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 8.9551e-04\n",
            "Epoch 24/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9998 - loss: 7.8847e-04 - val_categorical_accuracy: 0.9998 - val_loss: 8.7793e-04\n",
            "Epoch 25/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 8.9168e-04 - val_categorical_accuracy: 0.9998 - val_loss: 7.9233e-04\n",
            "Epoch 26/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 8.3429e-04 - val_categorical_accuracy: 0.9997 - val_loss: 9.7167e-04\n",
            "[13:17:38] Predicting on test...\n",
            "[13:18:01] Scoring...\n",
            "[13:18:03] Done setting: RunResult(setting='IoMT->IoMT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.3508835054903348, macro_recall=0.39646924689702717, roc_auc_ovr=nan, pr_auc_ovr=0.4587320984680317, ece=0.033179393586513756, mce=0.6264573419060941, fpr1e3=0.0009572685936128912, tpr_at_fpr1e3=0.5457195479975627, fpr1e4=7.977238280107427e-05, tpr_at_fpr1e4=0.5438589549817601)\n",
            "[13:18:03] === Setting: IoT → IoT | BiLSTM-only | adv=False ===\n",
            "[13:18:03] Building features (train)...\n",
            "[13:18:04] Train matrix: (838860, 10)\n",
            "[13:18:04] Building features (test)...\n",
            "[13:18:04] Test  matrix: (209715, 10)\n",
            "[13:18:04] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:18:04] Undersampled train: (313772, 10)\n",
            "[13:18:04] Training sequence model...\n",
            "Epoch 1/30\n",
            "1962/1962 - 17s - 9ms/step - categorical_accuracy: 0.7497 - loss: 0.3265 - val_categorical_accuracy: 0.7618 - val_loss: 0.2764\n",
            "Epoch 2/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7645 - loss: 0.2765 - val_categorical_accuracy: 0.7698 - val_loss: 0.2686\n",
            "Epoch 3/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7675 - loss: 0.2705 - val_categorical_accuracy: 0.7702 - val_loss: 0.2636\n",
            "Epoch 4/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7685 - loss: 0.2655 - val_categorical_accuracy: 0.7697 - val_loss: 0.2596\n",
            "Epoch 5/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7688 - loss: 0.2620 - val_categorical_accuracy: 0.7699 - val_loss: 0.2568\n",
            "Epoch 6/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7694 - loss: 0.2596 - val_categorical_accuracy: 0.7710 - val_loss: 0.2546\n",
            "Epoch 7/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7690 - loss: 0.2583 - val_categorical_accuracy: 0.7708 - val_loss: 0.2530\n",
            "Epoch 8/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7691 - loss: 0.2571 - val_categorical_accuracy: 0.7715 - val_loss: 0.2521\n",
            "Epoch 9/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7694 - loss: 0.2564 - val_categorical_accuracy: 0.7704 - val_loss: 0.2512\n",
            "Epoch 10/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7694 - loss: 0.2553 - val_categorical_accuracy: 0.7699 - val_loss: 0.2504\n",
            "Epoch 11/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7697 - loss: 0.2549 - val_categorical_accuracy: 0.7712 - val_loss: 0.2496\n",
            "Epoch 12/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7693 - loss: 0.2544 - val_categorical_accuracy: 0.7702 - val_loss: 0.2489\n",
            "Epoch 13/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7694 - loss: 0.2539 - val_categorical_accuracy: 0.7703 - val_loss: 0.2484\n",
            "Epoch 14/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7699 - loss: 0.2533 - val_categorical_accuracy: 0.7710 - val_loss: 0.2481\n",
            "Epoch 15/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7702 - loss: 0.2526 - val_categorical_accuracy: 0.7718 - val_loss: 0.2471\n",
            "Epoch 16/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7700 - loss: 0.2524 - val_categorical_accuracy: 0.7714 - val_loss: 0.2473\n",
            "Epoch 17/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7703 - loss: 0.2519 - val_categorical_accuracy: 0.7713 - val_loss: 0.2466\n",
            "Epoch 18/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7705 - loss: 0.2518 - val_categorical_accuracy: 0.7702 - val_loss: 0.2462\n",
            "Epoch 19/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7708 - loss: 0.2515 - val_categorical_accuracy: 0.7720 - val_loss: 0.2465\n",
            "Epoch 20/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7705 - loss: 0.2513 - val_categorical_accuracy: 0.7729 - val_loss: 0.2461\n",
            "Epoch 21/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7709 - loss: 0.2511 - val_categorical_accuracy: 0.7726 - val_loss: 0.2455\n",
            "Epoch 22/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7708 - loss: 0.2509 - val_categorical_accuracy: 0.7713 - val_loss: 0.2458\n",
            "Epoch 23/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7715 - loss: 0.2506 - val_categorical_accuracy: 0.7721 - val_loss: 0.2456\n",
            "Epoch 24/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7717 - loss: 0.2506 - val_categorical_accuracy: 0.7702 - val_loss: 0.2452\n",
            "Epoch 25/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7718 - loss: 0.2503 - val_categorical_accuracy: 0.7708 - val_loss: 0.2451\n",
            "Epoch 26/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7722 - loss: 0.2502 - val_categorical_accuracy: 0.7732 - val_loss: 0.2454\n",
            "Epoch 27/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7725 - loss: 0.2498 - val_categorical_accuracy: 0.7727 - val_loss: 0.2445\n",
            "Epoch 28/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7726 - loss: 0.2498 - val_categorical_accuracy: 0.7704 - val_loss: 0.2448\n",
            "Epoch 29/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7730 - loss: 0.2496 - val_categorical_accuracy: 0.7698 - val_loss: 0.2446\n",
            "Epoch 30/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7732 - loss: 0.2494 - val_categorical_accuracy: 0.7716 - val_loss: 0.2444\n",
            "[13:24:21] Predicting on test...\n",
            "[13:24:25] Scoring...\n",
            "[13:24:26] Done setting: RunResult(setting='IoT->IoT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.4751701354080364, macro_recall=0.48655211964095046, roc_auc_ovr=nan, pr_auc_ovr=0.4469528733724733, ece=0.24832821216412834, mce=0.3240474163032906, fpr1e3=0.0009704910077942559, tpr_at_fpr1e3=0.5533376333865182, fpr1e4=9.09835319807115e-05, tpr_at_fpr1e4=0.018280884000407373)\n",
            "[13:24:26] === Setting: IoMT → IoT | BiLSTM-only | adv=True ===\n",
            "[13:24:27] Building features (train)...\n",
            "[13:24:28] Train matrix: (1048575, 45)\n",
            "[13:24:28] Building features (test)...\n",
            "[13:24:28] Test  matrix: (1048575, 45)\n",
            "[13:24:28] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:24:28] Undersampled train: (140396, 45)\n",
            "[13:24:28] Warmup (3 epochs) before adversarial mix...\n",
            "[13:24:49] Generating adversarial subset: n=40000 (batched)...\n",
            "[13:24:51] Training sequence model...\n",
            "Epoch 1/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0034 - val_categorical_accuracy: 0.9996 - val_loss: 0.0028\n",
            "Epoch 2/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0027 - val_categorical_accuracy: 0.9998 - val_loss: 0.0023\n",
            "Epoch 3/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0023 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 4/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9998 - val_loss: 0.0018\n",
            "Epoch 5/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9999 - val_loss: 0.0013\n",
            "Epoch 6/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9997 - val_loss: 0.0015\n",
            "Epoch 7/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0012\n",
            "Epoch 8/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0013 - val_categorical_accuracy: 0.9996 - val_loss: 0.0013\n",
            "Epoch 9/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0013 - val_categorical_accuracy: 0.9997 - val_loss: 0.0011\n",
            "Epoch 10/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0012 - val_categorical_accuracy: 0.9997 - val_loss: 0.0012\n",
            "Epoch 11/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9996 - val_loss: 0.0011\n",
            "Epoch 12/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0010 - val_categorical_accuracy: 0.9999 - val_loss: 7.3181e-04\n",
            "Epoch 13/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.5913e-04 - val_categorical_accuracy: 0.9998 - val_loss: 6.6734e-04\n",
            "Epoch 14/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 9.2533e-04 - val_categorical_accuracy: 0.9998 - val_loss: 8.4355e-04\n",
            "Epoch 15/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 9.5903e-04 - val_categorical_accuracy: 0.9996 - val_loss: 0.0011\n",
            "Epoch 16/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 8.8064e-04 - val_categorical_accuracy: 0.9996 - val_loss: 9.8747e-04\n",
            "Epoch 17/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9998 - loss: 8.1529e-04 - val_categorical_accuracy: 0.9998 - val_loss: 8.8076e-04\n",
            "Epoch 18/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 9.0914e-04 - val_categorical_accuracy: 0.9998 - val_loss: 6.9506e-04\n",
            "[13:27:05] Predicting on test...\n",
            "[13:27:28] Scoring...\n",
            "[13:27:29] Done setting: RunResult(setting='IoMT->IoT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666667572216554, ece=0.35675308426911856, mce=0.35675308426911856, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[13:27:29] === Setting: IoT → IoMT | BiLSTM-only | adv=True ===\n",
            "[13:27:30] Building features (train)...\n",
            "[13:27:30] Train matrix: (838860, 10)\n",
            "[13:27:30] Building features (test)...\n",
            "[13:27:30] Test  matrix: (1048575, 10)\n",
            "[13:27:30] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:27:31] Undersampled train: (313772, 10)\n",
            "[13:27:31] Warmup (3 epochs) before adversarial mix...\n",
            "[13:28:13] Generating adversarial subset: n=40000 (batched)...\n",
            "[13:28:15] Training sequence model...\n",
            "Epoch 1/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7772 - loss: 0.2628 - val_categorical_accuracy: 0.7659 - val_loss: 0.2650\n",
            "Epoch 2/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7903 - loss: 0.2495 - val_categorical_accuracy: 0.7658 - val_loss: 0.2622\n",
            "Epoch 3/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7933 - loss: 0.2441 - val_categorical_accuracy: 0.7655 - val_loss: 0.2605\n",
            "Epoch 4/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7950 - loss: 0.2413 - val_categorical_accuracy: 0.7654 - val_loss: 0.2599\n",
            "Epoch 5/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7957 - loss: 0.2393 - val_categorical_accuracy: 0.7696 - val_loss: 0.2575\n",
            "Epoch 6/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7956 - loss: 0.2386 - val_categorical_accuracy: 0.7670 - val_loss: 0.2575\n",
            "Epoch 7/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7963 - loss: 0.2372 - val_categorical_accuracy: 0.7671 - val_loss: 0.2565\n",
            "Epoch 8/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7960 - loss: 0.2372 - val_categorical_accuracy: 0.7673 - val_loss: 0.2561\n",
            "Epoch 9/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7960 - loss: 0.2364 - val_categorical_accuracy: 0.7696 - val_loss: 0.2562\n",
            "Epoch 10/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7964 - loss: 0.2358 - val_categorical_accuracy: 0.7676 - val_loss: 0.2549\n",
            "Epoch 11/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7966 - loss: 0.2354 - val_categorical_accuracy: 0.7672 - val_loss: 0.2551\n",
            "Epoch 12/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7966 - loss: 0.2350 - val_categorical_accuracy: 0.7656 - val_loss: 0.2555\n",
            "Epoch 13/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7964 - loss: 0.2347 - val_categorical_accuracy: 0.7679 - val_loss: 0.2539\n",
            "Epoch 14/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7969 - loss: 0.2342 - val_categorical_accuracy: 0.7673 - val_loss: 0.2542\n",
            "Epoch 15/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7972 - loss: 0.2340 - val_categorical_accuracy: 0.7677 - val_loss: 0.2538\n",
            "Epoch 16/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7971 - loss: 0.2337 - val_categorical_accuracy: 0.7666 - val_loss: 0.2537\n",
            "Epoch 17/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7968 - loss: 0.2336 - val_categorical_accuracy: 0.7675 - val_loss: 0.2533\n",
            "Epoch 18/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7973 - loss: 0.2333 - val_categorical_accuracy: 0.7694 - val_loss: 0.2528\n",
            "Epoch 19/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7973 - loss: 0.2332 - val_categorical_accuracy: 0.7685 - val_loss: 0.2524\n",
            "Epoch 20/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7975 - loss: 0.2330 - val_categorical_accuracy: 0.7698 - val_loss: 0.2529\n",
            "Epoch 21/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7971 - loss: 0.2327 - val_categorical_accuracy: 0.7687 - val_loss: 0.2522\n",
            "Epoch 22/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7979 - loss: 0.2328 - val_categorical_accuracy: 0.7718 - val_loss: 0.2513\n",
            "Epoch 23/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7980 - loss: 0.2317 - val_categorical_accuracy: 0.7686 - val_loss: 0.2516\n",
            "Epoch 24/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7979 - loss: 0.2323 - val_categorical_accuracy: 0.7699 - val_loss: 0.2513\n",
            "Epoch 25/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7989 - loss: 0.2319 - val_categorical_accuracy: 0.7688 - val_loss: 0.2517\n",
            "Epoch 26/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7983 - loss: 0.2317 - val_categorical_accuracy: 0.7665 - val_loss: 0.2517\n",
            "Epoch 27/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7995 - loss: 0.2315 - val_categorical_accuracy: 0.7740 - val_loss: 0.2512\n",
            "Epoch 28/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7995 - loss: 0.2314 - val_categorical_accuracy: 0.7768 - val_loss: 0.2508\n",
            "Epoch 29/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.8005 - loss: 0.2309 - val_categorical_accuracy: 0.7722 - val_loss: 0.2503\n",
            "Epoch 30/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.8001 - loss: 0.2312 - val_categorical_accuracy: 0.7723 - val_loss: 0.2499\n",
            "[13:35:27] Predicting on test...\n",
            "[13:35:49] Scoring...\n",
            "[13:35:50] Done setting: RunResult(setting='IoT->IoMT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.010286076389243773, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666677922540332, ece=0.6292124138890118, mce=0.6292124138890118, fpr1e3=0.0, tpr_at_fpr1e3=1.9783019838412294e-06, fpr1e4=0.0, tpr_at_fpr1e4=1.9783019838412294e-06)\n",
            "[13:35:50] === Setting: IoMT → IoMT | LR->BiLSTM | adv=False ===\n",
            "[13:35:50] Building features (train)...\n",
            "[13:35:52] Train matrix: (1048575, 45)\n",
            "[13:35:52] Building features (test)...\n",
            "[13:35:52] Test  matrix: (1048575, 45)\n",
            "[13:35:52] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:35:52] Undersampled train: (140396, 45)\n",
            "[13:35:53] Pre-fitting LR for LR->BiLSTM features...\n",
            "[13:38:50] Training sequence model...\n",
            "Epoch 1/30\n",
            "878/878 - 10s - 11ms/step - categorical_accuracy: 0.9974 - loss: 0.0767 - val_categorical_accuracy: 0.9996 - val_loss: 0.0116\n",
            "Epoch 2/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0094 - val_categorical_accuracy: 0.9996 - val_loss: 0.0076\n",
            "Epoch 3/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0069 - val_categorical_accuracy: 0.9996 - val_loss: 0.0061\n",
            "Epoch 4/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0056 - val_categorical_accuracy: 0.9996 - val_loss: 0.0052\n",
            "Epoch 5/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0048 - val_categorical_accuracy: 0.9996 - val_loss: 0.0044\n",
            "Epoch 6/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0040 - val_categorical_accuracy: 0.9996 - val_loss: 0.0038\n",
            "Epoch 7/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0035 - val_categorical_accuracy: 0.9996 - val_loss: 0.0034\n",
            "Epoch 8/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0031 - val_categorical_accuracy: 0.9996 - val_loss: 0.0030\n",
            "Epoch 9/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0028 - val_categorical_accuracy: 0.9996 - val_loss: 0.0027\n",
            "Epoch 10/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0026 - val_categorical_accuracy: 0.9996 - val_loss: 0.0026\n",
            "Epoch 11/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9996 - loss: 0.0024 - val_categorical_accuracy: 0.9996 - val_loss: 0.0024\n",
            "Epoch 12/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0023 - val_categorical_accuracy: 0.9996 - val_loss: 0.0023\n",
            "Epoch 13/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0021 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 14/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0020 - val_categorical_accuracy: 0.9996 - val_loss: 0.0021\n",
            "Epoch 15/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0020 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 16/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 17/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 18/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0018 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 19/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 20/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 21/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 22/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 23/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 24/30\n",
            "878/878 - 6s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 25/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 26/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 27/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 28/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 29/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 30/30\n",
            "878/878 - 5s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "[13:41:40] Predicting on test...\n",
            "[13:42:01] Scoring...\n",
            "[13:42:03] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.3511673303541884, macro_recall=0.39846728474608356, roc_auc_ovr=nan, pr_auc_ovr=0.3127846187311076, ece=0.025690899803679206, mce=0.8055004925318004, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[13:42:03] === Setting: IoT → IoT | LR->BiLSTM | adv=False ===\n",
            "[13:42:03] Building features (train)...\n",
            "[13:42:03] Train matrix: (838860, 10)\n",
            "[13:42:03] Building features (test)...\n",
            "[13:42:03] Test  matrix: (209715, 10)\n",
            "[13:42:03] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:42:03] Undersampled train: (313772, 10)\n",
            "[13:42:03] Pre-fitting LR for LR->BiLSTM features...\n",
            "[13:47:28] Training sequence model...\n",
            "Epoch 1/30\n",
            "1962/1962 - 17s - 9ms/step - categorical_accuracy: 0.7386 - loss: 0.3564 - val_categorical_accuracy: 0.7416 - val_loss: 0.3179\n",
            "Epoch 2/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7454 - loss: 0.3131 - val_categorical_accuracy: 0.7559 - val_loss: 0.3026\n",
            "Epoch 3/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7553 - loss: 0.3016 - val_categorical_accuracy: 0.7608 - val_loss: 0.2949\n",
            "Epoch 4/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7583 - loss: 0.2960 - val_categorical_accuracy: 0.7619 - val_loss: 0.2911\n",
            "Epoch 5/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7599 - loss: 0.2927 - val_categorical_accuracy: 0.7623 - val_loss: 0.2883\n",
            "Epoch 6/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7608 - loss: 0.2899 - val_categorical_accuracy: 0.7628 - val_loss: 0.2864\n",
            "Epoch 7/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7615 - loss: 0.2881 - val_categorical_accuracy: 0.7633 - val_loss: 0.2851\n",
            "Epoch 8/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7623 - loss: 0.2866 - val_categorical_accuracy: 0.7634 - val_loss: 0.2840\n",
            "Epoch 9/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7623 - loss: 0.2856 - val_categorical_accuracy: 0.7641 - val_loss: 0.2832\n",
            "Epoch 10/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7626 - loss: 0.2851 - val_categorical_accuracy: 0.7634 - val_loss: 0.2827\n",
            "Epoch 11/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7626 - loss: 0.2847 - val_categorical_accuracy: 0.7634 - val_loss: 0.2825\n",
            "Epoch 12/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7626 - loss: 0.2845 - val_categorical_accuracy: 0.7634 - val_loss: 0.2825\n",
            "Epoch 13/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7628 - loss: 0.2841 - val_categorical_accuracy: 0.7635 - val_loss: 0.2825\n",
            "Epoch 14/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7633 - loss: 0.2839 - val_categorical_accuracy: 0.7641 - val_loss: 0.2819\n",
            "Epoch 15/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7632 - loss: 0.2834 - val_categorical_accuracy: 0.7644 - val_loss: 0.2820\n",
            "Epoch 16/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7632 - loss: 0.2837 - val_categorical_accuracy: 0.7637 - val_loss: 0.2821\n",
            "Epoch 17/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7629 - loss: 0.2833 - val_categorical_accuracy: 0.7639 - val_loss: 0.2819\n",
            "Epoch 18/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7636 - loss: 0.2832 - val_categorical_accuracy: 0.7640 - val_loss: 0.2817\n",
            "Epoch 19/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7629 - loss: 0.2833 - val_categorical_accuracy: 0.7637 - val_loss: 0.2820\n",
            "Epoch 20/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7630 - loss: 0.2828 - val_categorical_accuracy: 0.7639 - val_loss: 0.2809\n",
            "Epoch 21/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2827 - val_categorical_accuracy: 0.7649 - val_loss: 0.2811\n",
            "Epoch 22/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2829 - val_categorical_accuracy: 0.7639 - val_loss: 0.2811\n",
            "Epoch 23/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2827 - val_categorical_accuracy: 0.7651 - val_loss: 0.2811\n",
            "Epoch 24/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7632 - loss: 0.2825 - val_categorical_accuracy: 0.7656 - val_loss: 0.2808\n",
            "Epoch 25/30\n",
            "1962/1962 - 13s - 6ms/step - categorical_accuracy: 0.7632 - loss: 0.2826 - val_categorical_accuracy: 0.7640 - val_loss: 0.2809\n",
            "Epoch 26/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7637 - loss: 0.2821 - val_categorical_accuracy: 0.7646 - val_loss: 0.2808\n",
            "Epoch 27/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7637 - loss: 0.2822 - val_categorical_accuracy: 0.7652 - val_loss: 0.2803\n",
            "Epoch 28/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2822 - val_categorical_accuracy: 0.7654 - val_loss: 0.2806\n",
            "Epoch 29/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7631 - loss: 0.2820 - val_categorical_accuracy: 0.7648 - val_loss: 0.2804\n",
            "Epoch 30/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7632 - loss: 0.2821 - val_categorical_accuracy: 0.7654 - val_loss: 0.2803\n",
            "[13:54:04] Predicting on test...\n",
            "[13:54:11] Scoring...\n",
            "[13:54:11] Done setting: RunResult(setting='IoT->IoT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.47325114215169417, macro_recall=0.4830915046143347, roc_auc_ovr=nan, pr_auc_ovr=0.4288231712277802, ece=0.26756359735485186, mce=0.6689796447753906, fpr1e3=0.0009704910077942559, tpr_at_fpr1e3=0.3160652250172568, fpr1e4=9.09835319807115e-05, tpr_at_fpr1e4=0.028736802797297754)\n",
            "[13:54:11] === Setting: IoMT → IoT | LR->BiLSTM | adv=True ===\n",
            "[13:54:12] Building features (train)...\n",
            "[13:54:13] Train matrix: (1048575, 45)\n",
            "[13:54:13] Building features (test)...\n",
            "[13:54:13] Test  matrix: (1048575, 45)\n",
            "[13:54:13] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[13:54:13] Undersampled train: (140396, 45)\n",
            "[13:54:13] Pre-fitting LR for LR->BiLSTM features...\n",
            "[13:57:13] Warmup (3 epochs) before adversarial mix...\n",
            "[13:57:35] Generating adversarial subset: n=40000 (batched)...\n",
            "[13:57:37] Training sequence model...\n",
            "Epoch 1/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0055 - val_categorical_accuracy: 0.9996 - val_loss: 0.0049\n",
            "Epoch 2/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0043 - val_categorical_accuracy: 0.9996 - val_loss: 0.0040\n",
            "Epoch 3/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0035 - val_categorical_accuracy: 0.9996 - val_loss: 0.0033\n",
            "Epoch 4/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0030 - val_categorical_accuracy: 0.9996 - val_loss: 0.0028\n",
            "Epoch 5/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0026 - val_categorical_accuracy: 0.9996 - val_loss: 0.0026\n",
            "Epoch 6/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0024 - val_categorical_accuracy: 0.9996 - val_loss: 0.0024\n",
            "Epoch 7/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0022 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 8/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0021 - val_categorical_accuracy: 0.9996 - val_loss: 0.0021\n",
            "Epoch 9/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 10/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 11/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0018 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 12/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 13/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 14/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 15/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 16/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 17/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 18/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 19/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 20/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 21/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 22/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 23/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 24/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 25/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 26/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 27/30\n",
            "1190/1190 - 8s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 28/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 29/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 30/30\n",
            "1190/1190 - 7s - 6ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "[14:01:23] Predicting on test...\n",
            "[14:01:45] Scoring...\n",
            "[14:01:46] Done setting: RunResult(setting='IoMT->IoT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.0, macro_recall=0.0, roc_auc_ovr=nan, pr_auc_ovr=0.16666666557051013, ece=0.9385166168212891, mce=0.9385166168212891, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:01:46] === Setting: IoT → IoMT | LR->BiLSTM | adv=True ===\n",
            "[14:01:47] Building features (train)...\n",
            "[14:01:47] Train matrix: (838860, 10)\n",
            "[14:01:47] Building features (test)...\n",
            "[14:01:47] Test  matrix: (1048575, 10)\n",
            "[14:01:47] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:01:48] Undersampled train: (313772, 10)\n",
            "[14:01:48] Pre-fitting LR for LR->BiLSTM features...\n",
            "[14:07:25] Warmup (3 epochs) before adversarial mix...\n",
            "[14:08:08] Generating adversarial subset: n=40000 (batched)...\n",
            "[14:08:10] Training sequence model...\n",
            "Epoch 1/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7508 - loss: 0.3088 - val_categorical_accuracy: 0.7460 - val_loss: 0.2960\n",
            "Epoch 2/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7581 - loss: 0.3028 - val_categorical_accuracy: 0.7437 - val_loss: 0.2954\n",
            "Epoch 3/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7638 - loss: 0.2988 - val_categorical_accuracy: 0.7426 - val_loss: 0.2951\n",
            "Epoch 4/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7663 - loss: 0.2960 - val_categorical_accuracy: 0.7460 - val_loss: 0.2938\n",
            "Epoch 5/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7674 - loss: 0.2940 - val_categorical_accuracy: 0.7454 - val_loss: 0.2927\n",
            "Epoch 6/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7683 - loss: 0.2922 - val_categorical_accuracy: 0.7513 - val_loss: 0.2916\n",
            "Epoch 7/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7688 - loss: 0.2901 - val_categorical_accuracy: 0.7478 - val_loss: 0.2910\n",
            "Epoch 8/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7695 - loss: 0.2891 - val_categorical_accuracy: 0.7465 - val_loss: 0.2909\n",
            "Epoch 9/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7709 - loss: 0.2873 - val_categorical_accuracy: 0.7515 - val_loss: 0.2901\n",
            "Epoch 10/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7712 - loss: 0.2868 - val_categorical_accuracy: 0.7512 - val_loss: 0.2900\n",
            "Epoch 11/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7720 - loss: 0.2860 - val_categorical_accuracy: 0.7538 - val_loss: 0.2894\n",
            "Epoch 12/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7726 - loss: 0.2857 - val_categorical_accuracy: 0.7553 - val_loss: 0.2889\n",
            "Epoch 13/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7726 - loss: 0.2852 - val_categorical_accuracy: 0.7562 - val_loss: 0.2885\n",
            "Epoch 14/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7736 - loss: 0.2844 - val_categorical_accuracy: 0.7559 - val_loss: 0.2888\n",
            "Epoch 15/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7738 - loss: 0.2844 - val_categorical_accuracy: 0.7564 - val_loss: 0.2876\n",
            "Epoch 16/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7737 - loss: 0.2837 - val_categorical_accuracy: 0.7584 - val_loss: 0.2874\n",
            "Epoch 17/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7743 - loss: 0.2834 - val_categorical_accuracy: 0.7566 - val_loss: 0.2880\n",
            "Epoch 18/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7742 - loss: 0.2834 - val_categorical_accuracy: 0.7584 - val_loss: 0.2870\n",
            "Epoch 19/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7753 - loss: 0.2826 - val_categorical_accuracy: 0.7573 - val_loss: 0.2868\n",
            "Epoch 20/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7762 - loss: 0.2821 - val_categorical_accuracy: 0.7573 - val_loss: 0.2860\n",
            "Epoch 21/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7760 - loss: 0.2821 - val_categorical_accuracy: 0.7574 - val_loss: 0.2860\n",
            "Epoch 22/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7761 - loss: 0.2816 - val_categorical_accuracy: 0.7590 - val_loss: 0.2849\n",
            "Epoch 23/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7766 - loss: 0.2811 - val_categorical_accuracy: 0.7618 - val_loss: 0.2849\n",
            "Epoch 24/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7769 - loss: 0.2806 - val_categorical_accuracy: 0.7608 - val_loss: 0.2851\n",
            "Epoch 25/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7769 - loss: 0.2808 - val_categorical_accuracy: 0.7572 - val_loss: 0.2854\n",
            "Epoch 26/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7773 - loss: 0.2806 - val_categorical_accuracy: 0.7604 - val_loss: 0.2838\n",
            "Epoch 27/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7765 - loss: 0.2807 - val_categorical_accuracy: 0.7598 - val_loss: 0.2836\n",
            "Epoch 28/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7775 - loss: 0.2805 - val_categorical_accuracy: 0.7602 - val_loss: 0.2842\n",
            "Epoch 29/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7776 - loss: 0.2801 - val_categorical_accuracy: 0.7622 - val_loss: 0.2844\n",
            "Epoch 30/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7781 - loss: 0.2798 - val_categorical_accuracy: 0.7634 - val_loss: 0.2837\n",
            "[14:15:32] Predicting on test...\n",
            "[14:15:54] Scoring...\n",
            "[14:15:55] Done setting: RunResult(setting='IoT->IoMT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.18704153611581187, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666693354147497, ece=0.18878656940512728, mce=0.18878656940512728, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:15:55] Saved: ./outputs_v3fast/domain_shift_results_v3fast.csv\n",
            "[14:15:55] Saved: ./outputs_v3fast/domain_shift_perclass_v3fast.csv\n",
            "[14:15:55] Saved: ./outputs_v3fast/domain_shift_reliability_bins_v3fast.csv\n",
            "   setting  model_name  use_adv  resampling loss_mode  macro_f1  macro_recall  roc_auc_ovr  pr_auc_ovr      ece      mce   fpr1e3  tpr_at_fpr1e3   fpr1e4  tpr_at_fpr1e4\n",
            "IoMT->IoMT     LR-only    False smote_tomek     focal  0.004004      0.041679     0.188281    0.456933 0.858486 0.992777 0.000000       0.000000 0.000000       0.000000\n",
            "  IoT->IoT     LR-only    False smote_tomek     focal  0.234530      0.257609          NaN    0.310432 0.464671 0.709322 0.000030       0.000000 0.000030       0.000000\n",
            " IoMT->IoT     LR-only    False smote_tomek     focal  0.053052      0.200000     0.500000    0.155069 0.000000 0.000000 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT     LR-only    False smote_tomek     focal  0.187042      0.200000     0.500000    0.200000 0.297834 0.297834 0.000000       0.000000 0.000000       0.000000\n",
            "IoMT->IoMT BiLSTM-only    False smote_tomek     focal  0.350884      0.396469          NaN    0.458732 0.033179 0.626457 0.000957       0.545720 0.000080       0.543859\n",
            "  IoT->IoT BiLSTM-only    False smote_tomek     focal  0.475170      0.486552          NaN    0.446953 0.248328 0.324047 0.000970       0.553338 0.000091       0.018281\n",
            " IoMT->IoT BiLSTM-only     True smote_tomek     focal  0.053052      0.200000          NaN    0.166667 0.356753 0.356753 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT BiLSTM-only     True smote_tomek     focal  0.010286      0.200000          NaN    0.166667 0.629212 0.629212 0.000000       0.000002 0.000000       0.000002\n",
            "IoMT->IoMT  LR->BiLSTM    False smote_tomek     focal  0.351167      0.398467          NaN    0.312785 0.025691 0.805500 0.000000       0.000000 0.000000       0.000000\n",
            "  IoT->IoT  LR->BiLSTM    False smote_tomek     focal  0.473251      0.483092          NaN    0.428823 0.267564 0.668980 0.000970       0.316065 0.000091       0.028737\n",
            " IoMT->IoT  LR->BiLSTM     True smote_tomek     focal  0.000000      0.000000          NaN    0.166667 0.938517 0.938517 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT  LR->BiLSTM     True smote_tomek     focal  0.187042      0.200000          NaN    0.166667 0.188787 0.188787 0.000000       0.000000 0.000000       0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def zip_and_download_results():\n",
        "    output_dir = CONFIG[\"OUTDIR\"]\n",
        "    csv_files = [\n",
        "        os.path.join(output_dir, \"domain_shift_results_v3fast.csv\"),\n",
        "        os.path.join(output_dir, \"domain_shift_perclass_v3fast.csv\"),\n",
        "        os.path.join(output_dir, \"domain_shift_reliability_bins_v3fast.csv\")\n",
        "    ]\n",
        "    zip_filename = os.path.join(output_dir, \"domain_shift_results_v3fast.zip\")\n",
        "\n",
        "    print(\"\\nZipping and downloading results...\")\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "            for file in csv_files:\n",
        "                if os.path.exists(file):\n",
        "                    zipf.write(file, os.path.basename(file))\n",
        "                else:\n",
        "                    print(f\"Warning: File not found and will not be included in the zip: {file}\")\n",
        "\n",
        "        if os.path.exists(zip_filename):\n",
        "            files.download(zip_filename)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(\"Zip file was not created.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during zipping or downloading: {e}\")\n",
        "\n",
        "# Call the main training function first, then the download function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    zip_and_download_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XdbcCYiIFkj4",
        "outputId": "94078b36-1b47-4065-d07f-eb2e243bded6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14:15:57] Loading CSVs...\n",
            "[14:16:03] Mapping families...\n",
            "[14:16:04] Applying MQTT filter to NF-ToN-IoT...\n",
            "[14:16:05] [WARN] MQTT filter produced empty set; using full NF-ToN-IoT.\n",
            "[14:16:05] Splitting NF train/test...\n",
            "[14:16:06] === Setting: IoMT → IoMT | LR-only | adv=False ===\n",
            "[14:16:08] Building features (train)...\n",
            "[14:16:09] Train matrix: (1048575, 45)\n",
            "[14:16:09] Building features (test)...\n",
            "[14:16:09] Test  matrix: (1048575, 45)\n",
            "[14:16:09] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:16:09] Undersampled train: (140396, 45)\n",
            "[14:16:09] Fitting LogisticRegression (sparse, saga)...\n",
            "[14:19:06] Scoring...\n",
            "[14:19:07] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.004003284884619037, macro_recall=0.04167829932350235, roc_auc_ovr=0.18822919633720475, pr_auc_ovr=0.4568993410158293, ece=0.8586107954982654, mce=0.9927775266699238, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:19:07] === Setting: IoT → IoT | LR-only | adv=False ===\n",
            "[14:19:08] Building features (train)...\n",
            "[14:19:08] Train matrix: (838860, 10)\n",
            "[14:19:08] Building features (test)...\n",
            "[14:19:08] Test  matrix: (209715, 10)\n",
            "[14:19:08] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:19:08] Undersampled train: (313772, 10)\n",
            "[14:19:08] Fitting LogisticRegression (sparse, saga)...\n",
            "[14:24:50] Scoring...\n",
            "[14:24:50] Done setting: RunResult(setting='IoT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.23452979339042768, macro_recall=0.25760888908038737, roc_auc_ovr=nan, pr_auc_ovr=0.3104317458130958, ece=0.46467073650303337, mce=0.7093247841481167, fpr1e3=3.0327843993570496e-05, tpr_at_fpr1e3=0.0, fpr1e4=3.0327843993570496e-05, tpr_at_fpr1e4=0.0)\n",
            "[14:24:50] === Setting: IoMT → IoT | LR-only | adv=False ===\n",
            "[14:24:51] Building features (train)...\n",
            "[14:24:52] Train matrix: (1048575, 45)\n",
            "[14:24:52] Building features (test)...\n",
            "[14:24:53] Test  matrix: (1048575, 45)\n",
            "[14:24:53] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:24:53] Undersampled train: (140396, 45)\n",
            "[14:24:53] Fitting LogisticRegression (sparse, saga)...\n",
            "[14:27:53] Scoring...\n",
            "[14:27:53] Done setting: RunResult(setting='IoMT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.15506902224447466, ece=0.0, mce=0.0, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:27:53] === Setting: IoT → IoMT | LR-only | adv=False ===\n",
            "[14:27:54] Building features (train)...\n",
            "[14:27:54] Train matrix: (838860, 10)\n",
            "[14:27:54] Building features (test)...\n",
            "[14:27:55] Test  matrix: (1048575, 10)\n",
            "[14:27:55] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:27:55] Undersampled train: (313772, 10)\n",
            "[14:27:55] Fitting LogisticRegression (sparse, saga)...\n",
            "[14:33:54] Scoring...\n",
            "[14:33:54] Done setting: RunResult(setting='IoT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.18704153611581187, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.2, ece=0.2978300031133355, mce=0.2978300031133355, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:33:54] === Setting: IoMT → IoMT | BiLSTM-only | adv=False ===\n",
            "[14:33:55] Building features (train)...\n",
            "[14:33:56] Train matrix: (1048575, 45)\n",
            "[14:33:56] Building features (test)...\n",
            "[14:33:57] Test  matrix: (1048575, 45)\n",
            "[14:33:57] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:33:57] Undersampled train: (140396, 45)\n",
            "[14:33:57] Training sequence model...\n",
            "Epoch 1/30\n",
            "878/878 - 12s - 14ms/step - categorical_accuracy: 0.9925 - loss: 0.0685 - val_categorical_accuracy: 0.9995 - val_loss: 0.0131\n",
            "Epoch 2/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0093 - val_categorical_accuracy: 0.9997 - val_loss: 0.0066\n",
            "Epoch 3/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0049 - val_categorical_accuracy: 0.9997 - val_loss: 0.0043\n",
            "Epoch 4/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0036 - val_categorical_accuracy: 0.9996 - val_loss: 0.0037\n",
            "Epoch 5/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9997 - loss: 0.0029 - val_categorical_accuracy: 0.9998 - val_loss: 0.0024\n",
            "Epoch 6/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9997 - loss: 0.0024 - val_categorical_accuracy: 0.9999 - val_loss: 0.0019\n",
            "Epoch 7/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0021 - val_categorical_accuracy: 0.9999 - val_loss: 0.0019\n",
            "Epoch 8/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9998 - loss: 0.0019 - val_categorical_accuracy: 0.9999 - val_loss: 0.0015\n",
            "Epoch 9/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9998 - loss: 0.0016 - val_categorical_accuracy: 0.9998 - val_loss: 0.0014\n",
            "Epoch 10/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9998 - val_loss: 0.0014\n",
            "Epoch 11/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0014 - val_categorical_accuracy: 0.9998 - val_loss: 0.0012\n",
            "Epoch 12/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9998 - loss: 0.0013 - val_categorical_accuracy: 0.9999 - val_loss: 0.0011\n",
            "Epoch 13/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0013 - val_categorical_accuracy: 0.9998 - val_loss: 0.0012\n",
            "Epoch 14/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9999 - val_loss: 8.8445e-04\n",
            "Epoch 15/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9999 - val_loss: 9.1347e-04\n",
            "Epoch 16/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0012 - val_categorical_accuracy: 0.9999 - val_loss: 9.0423e-04\n",
            "Epoch 17/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 18/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 19/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 9.3486e-04\n",
            "[14:36:07] Predicting on test...\n",
            "[14:36:33] Scoring...\n",
            "[14:36:35] Done setting: RunResult(setting='IoMT->IoMT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.351935240062854, macro_recall=0.397355403154336, roc_auc_ovr=nan, pr_auc_ovr=0.42822674178701875, ece=0.027361255892043402, mce=0.47497990432118936, fpr1e3=0.0009572685936128912, tpr_at_fpr1e3=9.891509919206147e-07, fpr1e4=2.6590794267024755e-05, tpr_at_fpr1e4=0.0)\n",
            "[14:36:35] === Setting: IoT → IoT | BiLSTM-only | adv=False ===\n",
            "[14:36:35] Building features (train)...\n",
            "[14:36:36] Train matrix: (838860, 10)\n",
            "[14:36:36] Building features (test)...\n",
            "[14:36:36] Test  matrix: (209715, 10)\n",
            "[14:36:36] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:36:36] Undersampled train: (313772, 10)\n",
            "[14:36:36] Training sequence model...\n",
            "Epoch 1/30\n",
            "1962/1962 - 19s - 10ms/step - categorical_accuracy: 0.7521 - loss: 0.3252 - val_categorical_accuracy: 0.7650 - val_loss: 0.2755\n",
            "Epoch 2/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7654 - loss: 0.2756 - val_categorical_accuracy: 0.7693 - val_loss: 0.2676\n",
            "Epoch 3/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7679 - loss: 0.2685 - val_categorical_accuracy: 0.7692 - val_loss: 0.2618\n",
            "Epoch 4/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7688 - loss: 0.2636 - val_categorical_accuracy: 0.7701 - val_loss: 0.2572\n",
            "Epoch 5/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7691 - loss: 0.2610 - val_categorical_accuracy: 0.7687 - val_loss: 0.2549\n",
            "Epoch 6/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7691 - loss: 0.2589 - val_categorical_accuracy: 0.7687 - val_loss: 0.2532\n",
            "Epoch 7/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7694 - loss: 0.2573 - val_categorical_accuracy: 0.7696 - val_loss: 0.2518\n",
            "Epoch 8/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7695 - loss: 0.2563 - val_categorical_accuracy: 0.7699 - val_loss: 0.2510\n",
            "Epoch 9/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7695 - loss: 0.2556 - val_categorical_accuracy: 0.7726 - val_loss: 0.2502\n",
            "Epoch 10/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7696 - loss: 0.2548 - val_categorical_accuracy: 0.7702 - val_loss: 0.2498\n",
            "Epoch 11/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7697 - loss: 0.2544 - val_categorical_accuracy: 0.7711 - val_loss: 0.2489\n",
            "Epoch 12/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7696 - loss: 0.2536 - val_categorical_accuracy: 0.7698 - val_loss: 0.2483\n",
            "Epoch 13/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7696 - loss: 0.2535 - val_categorical_accuracy: 0.7694 - val_loss: 0.2477\n",
            "Epoch 14/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7697 - loss: 0.2527 - val_categorical_accuracy: 0.7698 - val_loss: 0.2477\n",
            "Epoch 15/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7698 - loss: 0.2526 - val_categorical_accuracy: 0.7703 - val_loss: 0.2473\n",
            "Epoch 16/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7702 - loss: 0.2522 - val_categorical_accuracy: 0.7710 - val_loss: 0.2471\n",
            "Epoch 17/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7701 - loss: 0.2520 - val_categorical_accuracy: 0.7702 - val_loss: 0.2468\n",
            "Epoch 18/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7702 - loss: 0.2516 - val_categorical_accuracy: 0.7714 - val_loss: 0.2469\n",
            "Epoch 19/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7701 - loss: 0.2517 - val_categorical_accuracy: 0.7720 - val_loss: 0.2467\n",
            "Epoch 20/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7702 - loss: 0.2513 - val_categorical_accuracy: 0.7713 - val_loss: 0.2467\n",
            "Epoch 21/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7704 - loss: 0.2512 - val_categorical_accuracy: 0.7698 - val_loss: 0.2464\n",
            "Epoch 22/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7701 - loss: 0.2508 - val_categorical_accuracy: 0.7708 - val_loss: 0.2458\n",
            "Epoch 23/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7710 - loss: 0.2507 - val_categorical_accuracy: 0.7711 - val_loss: 0.2457\n",
            "Epoch 24/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7707 - loss: 0.2506 - val_categorical_accuracy: 0.7717 - val_loss: 0.2457\n",
            "Epoch 25/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7704 - loss: 0.2505 - val_categorical_accuracy: 0.7743 - val_loss: 0.2454\n",
            "Epoch 26/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7710 - loss: 0.2503 - val_categorical_accuracy: 0.7708 - val_loss: 0.2452\n",
            "Epoch 27/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7709 - loss: 0.2503 - val_categorical_accuracy: 0.7708 - val_loss: 0.2451\n",
            "Epoch 28/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7710 - loss: 0.2502 - val_categorical_accuracy: 0.7710 - val_loss: 0.2451\n",
            "Epoch 29/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7710 - loss: 0.2501 - val_categorical_accuracy: 0.7718 - val_loss: 0.2450\n",
            "Epoch 30/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7713 - loss: 0.2502 - val_categorical_accuracy: 0.7719 - val_loss: 0.2452\n",
            "[14:43:22] Predicting on test...\n",
            "[14:43:27] Scoring...\n",
            "[14:43:28] Done setting: RunResult(setting='IoT->IoT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.47480173481115867, macro_recall=0.48660445659417456, roc_auc_ovr=nan, pr_auc_ovr=0.44523296185726213, ece=0.23786433471805835, mce=0.3307444369533351, fpr1e3=0.0009704910077942559, tpr_at_fpr1e3=0.5380045490036324, fpr1e4=9.09835319807115e-05, tpr_at_fpr1e4=0.023814373493566895)\n",
            "[14:43:28] === Setting: IoMT → IoT | BiLSTM-only | adv=True ===\n",
            "[14:43:29] Building features (train)...\n",
            "[14:43:30] Train matrix: (1048575, 45)\n",
            "[14:43:30] Building features (test)...\n",
            "[14:43:30] Test  matrix: (1048575, 45)\n",
            "[14:43:30] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:43:30] Undersampled train: (140396, 45)\n",
            "[14:43:30] Warmup (3 epochs) before adversarial mix...\n",
            "[14:43:54] Generating adversarial subset: n=40000 (batched)...\n",
            "[14:43:56] Training sequence model...\n",
            "Epoch 1/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0034 - val_categorical_accuracy: 0.9997 - val_loss: 0.0028\n",
            "Epoch 2/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0027 - val_categorical_accuracy: 0.9999 - val_loss: 0.0019\n",
            "Epoch 3/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0022 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 4/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0018 - val_categorical_accuracy: 0.9999 - val_loss: 0.0014\n",
            "Epoch 5/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9998 - val_loss: 0.0016\n",
            "Epoch 6/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 7/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9999 - val_loss: 0.0010\n",
            "Epoch 8/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0013 - val_categorical_accuracy: 0.9996 - val_loss: 0.0013\n",
            "Epoch 9/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0012 - val_categorical_accuracy: 0.9997 - val_loss: 0.0012\n",
            "Epoch 10/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 8.6947e-04\n",
            "Epoch 11/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9999 - val_loss: 7.8725e-04\n",
            "Epoch 12/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 9.7865e-04 - val_categorical_accuracy: 0.9996 - val_loss: 0.0015\n",
            "Epoch 13/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 9.6567e-04 - val_categorical_accuracy: 0.9997 - val_loss: 0.0010\n",
            "Epoch 14/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 7.1950e-04\n",
            "Epoch 15/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.7873e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.7000e-04\n",
            "Epoch 16/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.8388e-04 - val_categorical_accuracy: 0.9998 - val_loss: 7.0064e-04\n",
            "Epoch 17/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 9.1990e-04 - val_categorical_accuracy: 0.9996 - val_loss: 0.0010\n",
            "Epoch 18/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.3198e-04 - val_categorical_accuracy: 0.9997 - val_loss: 0.0011\n",
            "Epoch 19/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 9.5119e-04 - val_categorical_accuracy: 0.9998 - val_loss: 7.0630e-04\n",
            "Epoch 20/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 7.9779e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.4531e-04\n",
            "Epoch 21/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 9.1880e-04 - val_categorical_accuracy: 0.9996 - val_loss: 9.5962e-04\n",
            "Epoch 22/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.6613e-04 - val_categorical_accuracy: 0.9997 - val_loss: 8.0314e-04\n",
            "Epoch 23/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.1857e-04 - val_categorical_accuracy: 0.9998 - val_loss: 7.1228e-04\n",
            "Epoch 24/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 7.2391e-04 - val_categorical_accuracy: 0.9998 - val_loss: 6.6509e-04\n",
            "Epoch 25/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9998 - loss: 8.6019e-04 - val_categorical_accuracy: 0.9998 - val_loss: 5.8226e-04\n",
            "[14:47:18] Predicting on test...\n",
            "[14:47:43] Scoring...\n",
            "[14:47:43] Done setting: RunResult(setting='IoMT->IoT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666667630347268, ece=0.4696646066575768, mce=0.4696646066575768, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:47:43] === Setting: IoT → IoMT | BiLSTM-only | adv=True ===\n",
            "[14:47:44] Building features (train)...\n",
            "[14:47:44] Train matrix: (838860, 10)\n",
            "[14:47:44] Building features (test)...\n",
            "[14:47:45] Test  matrix: (1048575, 10)\n",
            "[14:47:45] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:47:45] Undersampled train: (313772, 10)\n",
            "[14:47:45] Warmup (3 epochs) before adversarial mix...\n",
            "[14:48:32] Generating adversarial subset: n=40000 (batched)...\n",
            "[14:48:33] Training sequence model...\n",
            "Epoch 1/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7779 - loss: 0.2635 - val_categorical_accuracy: 0.7652 - val_loss: 0.2651\n",
            "Epoch 2/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7916 - loss: 0.2497 - val_categorical_accuracy: 0.7652 - val_loss: 0.2626\n",
            "Epoch 3/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7943 - loss: 0.2442 - val_categorical_accuracy: 0.7660 - val_loss: 0.2604\n",
            "Epoch 4/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7954 - loss: 0.2415 - val_categorical_accuracy: 0.7653 - val_loss: 0.2585\n",
            "Epoch 5/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7964 - loss: 0.2396 - val_categorical_accuracy: 0.7669 - val_loss: 0.2569\n",
            "Epoch 6/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7960 - loss: 0.2383 - val_categorical_accuracy: 0.7677 - val_loss: 0.2559\n",
            "Epoch 7/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7965 - loss: 0.2372 - val_categorical_accuracy: 0.7656 - val_loss: 0.2556\n",
            "Epoch 8/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7961 - loss: 0.2369 - val_categorical_accuracy: 0.7689 - val_loss: 0.2555\n",
            "Epoch 9/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7965 - loss: 0.2362 - val_categorical_accuracy: 0.7715 - val_loss: 0.2536\n",
            "Epoch 10/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7965 - loss: 0.2355 - val_categorical_accuracy: 0.7697 - val_loss: 0.2537\n",
            "Epoch 11/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7967 - loss: 0.2350 - val_categorical_accuracy: 0.7667 - val_loss: 0.2536\n",
            "Epoch 12/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7967 - loss: 0.2348 - val_categorical_accuracy: 0.7697 - val_loss: 0.2528\n",
            "Epoch 13/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7970 - loss: 0.2346 - val_categorical_accuracy: 0.7695 - val_loss: 0.2522\n",
            "Epoch 14/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7972 - loss: 0.2341 - val_categorical_accuracy: 0.7683 - val_loss: 0.2524\n",
            "Epoch 15/30\n",
            "2274/2274 - 17s - 7ms/step - categorical_accuracy: 0.7974 - loss: 0.2338 - val_categorical_accuracy: 0.7690 - val_loss: 0.2524\n",
            "Epoch 16/30\n",
            "2274/2274 - 17s - 7ms/step - categorical_accuracy: 0.7971 - loss: 0.2335 - val_categorical_accuracy: 0.7708 - val_loss: 0.2523\n",
            "Epoch 17/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7975 - loss: 0.2331 - val_categorical_accuracy: 0.7698 - val_loss: 0.2520\n",
            "Epoch 18/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7976 - loss: 0.2331 - val_categorical_accuracy: 0.7703 - val_loss: 0.2512\n",
            "Epoch 19/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7979 - loss: 0.2327 - val_categorical_accuracy: 0.7701 - val_loss: 0.2511\n",
            "Epoch 20/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7978 - loss: 0.2328 - val_categorical_accuracy: 0.7690 - val_loss: 0.2510\n",
            "Epoch 21/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7981 - loss: 0.2323 - val_categorical_accuracy: 0.7690 - val_loss: 0.2508\n",
            "Epoch 22/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7981 - loss: 0.2321 - val_categorical_accuracy: 0.7698 - val_loss: 0.2507\n",
            "Epoch 23/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7978 - loss: 0.2321 - val_categorical_accuracy: 0.7686 - val_loss: 0.2507\n",
            "Epoch 24/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7990 - loss: 0.2318 - val_categorical_accuracy: 0.7716 - val_loss: 0.2511\n",
            "Epoch 25/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7989 - loss: 0.2317 - val_categorical_accuracy: 0.7731 - val_loss: 0.2506\n",
            "Epoch 26/30\n",
            "2274/2274 - 17s - 7ms/step - categorical_accuracy: 0.7993 - loss: 0.2315 - val_categorical_accuracy: 0.7755 - val_loss: 0.2503\n",
            "Epoch 27/30\n",
            "2274/2274 - 17s - 8ms/step - categorical_accuracy: 0.7994 - loss: 0.2317 - val_categorical_accuracy: 0.7688 - val_loss: 0.2513\n",
            "Epoch 28/30\n",
            "2274/2274 - 17s - 7ms/step - categorical_accuracy: 0.7993 - loss: 0.2315 - val_categorical_accuracy: 0.7736 - val_loss: 0.2510\n",
            "Epoch 29/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7995 - loss: 0.2312 - val_categorical_accuracy: 0.7714 - val_loss: 0.2510\n",
            "Epoch 30/30\n",
            "2274/2274 - 16s - 7ms/step - categorical_accuracy: 0.7998 - loss: 0.2312 - val_categorical_accuracy: 0.7728 - val_loss: 0.2497\n",
            "[14:56:36] Predicting on test...\n",
            "[14:57:02] Scoring...\n",
            "[14:57:03] Done setting: RunResult(setting='IoT->IoMT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.010286076389243773, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666662797942697, ece=0.621819947820744, mce=0.621819947820744, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[14:57:03] === Setting: IoMT → IoMT | LR->BiLSTM | adv=False ===\n",
            "[14:57:03] Building features (train)...\n",
            "[14:57:05] Train matrix: (1048575, 45)\n",
            "[14:57:05] Building features (test)...\n",
            "[14:57:06] Test  matrix: (1048575, 45)\n",
            "[14:57:06] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[14:57:06] Undersampled train: (140396, 45)\n",
            "[14:57:06] Pre-fitting LR for LR->BiLSTM features...\n",
            "[15:00:21] Training sequence model...\n",
            "Epoch 1/30\n",
            "878/878 - 12s - 13ms/step - categorical_accuracy: 0.9800 - loss: 0.0755 - val_categorical_accuracy: 0.9996 - val_loss: 0.0117\n",
            "Epoch 2/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0094 - val_categorical_accuracy: 0.9996 - val_loss: 0.0077\n",
            "Epoch 3/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0069 - val_categorical_accuracy: 0.9996 - val_loss: 0.0061\n",
            "Epoch 4/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0056 - val_categorical_accuracy: 0.9996 - val_loss: 0.0052\n",
            "Epoch 5/30\n",
            "878/878 - 7s - 8ms/step - categorical_accuracy: 0.9996 - loss: 0.0047 - val_categorical_accuracy: 0.9996 - val_loss: 0.0044\n",
            "Epoch 6/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0040 - val_categorical_accuracy: 0.9996 - val_loss: 0.0038\n",
            "Epoch 7/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0035 - val_categorical_accuracy: 0.9996 - val_loss: 0.0033\n",
            "Epoch 8/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0031 - val_categorical_accuracy: 0.9996 - val_loss: 0.0030\n",
            "Epoch 9/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0028 - val_categorical_accuracy: 0.9996 - val_loss: 0.0027\n",
            "Epoch 10/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0026 - val_categorical_accuracy: 0.9996 - val_loss: 0.0025\n",
            "Epoch 11/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0024 - val_categorical_accuracy: 0.9996 - val_loss: 0.0024\n",
            "Epoch 12/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0023 - val_categorical_accuracy: 0.9996 - val_loss: 0.0023\n",
            "Epoch 13/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0022 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 14/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0021 - val_categorical_accuracy: 0.9996 - val_loss: 0.0021\n",
            "Epoch 15/30\n",
            "878/878 - 7s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0020 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 16/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 17/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 18/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0018 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 19/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0018 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 20/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 21/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 22/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 23/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 24/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 25/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 26/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 27/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 28/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 29/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 30/30\n",
            "878/878 - 6s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "[15:03:36] Predicting on test...\n",
            "[15:04:01] Scoring...\n",
            "[15:04:03] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.35116954410938944, macro_recall=0.3984677190735926, roc_auc_ovr=nan, pr_auc_ovr=0.3128313374496952, ece=0.026962036122762545, mce=0.7995773986509276, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[15:04:03] === Setting: IoT → IoT | LR->BiLSTM | adv=False ===\n",
            "[15:04:03] Building features (train)...\n",
            "[15:04:03] Train matrix: (838860, 10)\n",
            "[15:04:03] Building features (test)...\n",
            "[15:04:03] Test  matrix: (209715, 10)\n",
            "[15:04:03] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[15:04:03] Undersampled train: (313772, 10)\n",
            "[15:04:03] Pre-fitting LR for LR->BiLSTM features...\n",
            "[15:10:09] Training sequence model...\n",
            "Epoch 1/30\n",
            "1962/1962 - 18s - 9ms/step - categorical_accuracy: 0.7395 - loss: 0.3560 - val_categorical_accuracy: 0.7438 - val_loss: 0.3186\n",
            "Epoch 2/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7463 - loss: 0.3141 - val_categorical_accuracy: 0.7554 - val_loss: 0.3030\n",
            "Epoch 3/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7541 - loss: 0.3025 - val_categorical_accuracy: 0.7604 - val_loss: 0.2958\n",
            "Epoch 4/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7577 - loss: 0.2965 - val_categorical_accuracy: 0.7605 - val_loss: 0.2910\n",
            "Epoch 5/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7596 - loss: 0.2926 - val_categorical_accuracy: 0.7618 - val_loss: 0.2883\n",
            "Epoch 6/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7606 - loss: 0.2900 - val_categorical_accuracy: 0.7624 - val_loss: 0.2861\n",
            "Epoch 7/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7612 - loss: 0.2883 - val_categorical_accuracy: 0.7620 - val_loss: 0.2852\n",
            "Epoch 8/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7617 - loss: 0.2870 - val_categorical_accuracy: 0.7623 - val_loss: 0.2843\n",
            "Epoch 9/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7620 - loss: 0.2863 - val_categorical_accuracy: 0.7626 - val_loss: 0.2840\n",
            "Epoch 10/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7625 - loss: 0.2854 - val_categorical_accuracy: 0.7641 - val_loss: 0.2832\n",
            "Epoch 11/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7620 - loss: 0.2852 - val_categorical_accuracy: 0.7630 - val_loss: 0.2832\n",
            "Epoch 12/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7628 - loss: 0.2844 - val_categorical_accuracy: 0.7640 - val_loss: 0.2827\n",
            "Epoch 13/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7625 - loss: 0.2845 - val_categorical_accuracy: 0.7640 - val_loss: 0.2824\n",
            "Epoch 14/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7629 - loss: 0.2839 - val_categorical_accuracy: 0.7643 - val_loss: 0.2819\n",
            "Epoch 15/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7629 - loss: 0.2837 - val_categorical_accuracy: 0.7645 - val_loss: 0.2818\n",
            "Epoch 16/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7634 - loss: 0.2834 - val_categorical_accuracy: 0.7644 - val_loss: 0.2816\n",
            "Epoch 17/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7634 - loss: 0.2835 - val_categorical_accuracy: 0.7648 - val_loss: 0.2819\n",
            "Epoch 18/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7634 - loss: 0.2833 - val_categorical_accuracy: 0.7648 - val_loss: 0.2813\n",
            "Epoch 19/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7631 - loss: 0.2829 - val_categorical_accuracy: 0.7655 - val_loss: 0.2814\n",
            "Epoch 20/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7633 - loss: 0.2829 - val_categorical_accuracy: 0.7652 - val_loss: 0.2813\n",
            "Epoch 21/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7633 - loss: 0.2828 - val_categorical_accuracy: 0.7661 - val_loss: 0.2810\n",
            "Epoch 22/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7634 - loss: 0.2828 - val_categorical_accuracy: 0.7650 - val_loss: 0.2810\n",
            "Epoch 23/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2825 - val_categorical_accuracy: 0.7661 - val_loss: 0.2809\n",
            "Epoch 24/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7634 - loss: 0.2825 - val_categorical_accuracy: 0.7660 - val_loss: 0.2807\n",
            "Epoch 25/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7635 - loss: 0.2823 - val_categorical_accuracy: 0.7650 - val_loss: 0.2808\n",
            "Epoch 26/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7637 - loss: 0.2823 - val_categorical_accuracy: 0.7662 - val_loss: 0.2804\n",
            "Epoch 27/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7638 - loss: 0.2820 - val_categorical_accuracy: 0.7652 - val_loss: 0.2802\n",
            "Epoch 28/30\n",
            "1962/1962 - 14s - 7ms/step - categorical_accuracy: 0.7637 - loss: 0.2824 - val_categorical_accuracy: 0.7651 - val_loss: 0.2811\n",
            "Epoch 29/30\n",
            "1962/1962 - 13s - 7ms/step - categorical_accuracy: 0.7637 - loss: 0.2818 - val_categorical_accuracy: 0.7661 - val_loss: 0.2801\n",
            "Epoch 30/30\n",
            "1962/1962 - 12s - 6ms/step - categorical_accuracy: 0.7642 - loss: 0.2817 - val_categorical_accuracy: 0.7664 - val_loss: 0.2798\n",
            "[15:16:55] Predicting on test...\n",
            "[15:17:00] Scoring...\n",
            "[15:17:00] Done setting: RunResult(setting='IoT->IoT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.47365125933763536, macro_recall=0.4836502574635215, roc_auc_ovr=nan, pr_auc_ovr=0.4297171476517974, ece=0.2733259782264369, mce=0.6678373515605927, fpr1e3=0.0009704910077942559, tpr_at_fpr1e3=0.27525998347874303, fpr1e4=9.09835319807115e-05, tpr_at_fpr1e4=0.029721288658043927)\n",
            "[15:17:00] === Setting: IoMT → IoT | LR->BiLSTM | adv=True ===\n",
            "[15:17:01] Building features (train)...\n",
            "[15:17:02] Train matrix: (1048575, 45)\n",
            "[15:17:02] Building features (test)...\n",
            "[15:17:02] Test  matrix: (1048575, 45)\n",
            "[15:17:02] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[15:17:02] Undersampled train: (140396, 45)\n",
            "[15:17:02] Pre-fitting LR for LR->BiLSTM features...\n",
            "[15:20:06] Warmup (3 epochs) before adversarial mix...\n",
            "[15:20:29] Generating adversarial subset: n=40000 (batched)...\n",
            "[15:20:31] Training sequence model...\n",
            "Epoch 1/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9996 - loss: 0.0055 - val_categorical_accuracy: 0.9996 - val_loss: 0.0050\n",
            "Epoch 2/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0044 - val_categorical_accuracy: 0.9996 - val_loss: 0.0041\n",
            "Epoch 3/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0036 - val_categorical_accuracy: 0.9996 - val_loss: 0.0034\n",
            "Epoch 4/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0031 - val_categorical_accuracy: 0.9996 - val_loss: 0.0029\n",
            "Epoch 5/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0027 - val_categorical_accuracy: 0.9996 - val_loss: 0.0026\n",
            "Epoch 6/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0024 - val_categorical_accuracy: 0.9996 - val_loss: 0.0024\n",
            "Epoch 7/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0022 - val_categorical_accuracy: 0.9996 - val_loss: 0.0022\n",
            "Epoch 8/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0020 - val_categorical_accuracy: 0.9996 - val_loss: 0.0021\n",
            "Epoch 9/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0020 - val_categorical_accuracy: 0.9996 - val_loss: 0.0020\n",
            "Epoch 10/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0019 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 11/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0018 - val_categorical_accuracy: 0.9996 - val_loss: 0.0019\n",
            "Epoch 12/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 13/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0017 - val_categorical_accuracy: 0.9996 - val_loss: 0.0018\n",
            "Epoch 14/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 15/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 16/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 17/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0016 - val_categorical_accuracy: 0.9996 - val_loss: 0.0017\n",
            "Epoch 18/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 19/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 20/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 21/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 22/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 23/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 24/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 25/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 26/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 27/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 28/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 29/30\n",
            "1190/1190 - 8s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0014 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "Epoch 30/30\n",
            "1190/1190 - 9s - 7ms/step - categorical_accuracy: 0.9997 - loss: 0.0015 - val_categorical_accuracy: 0.9996 - val_loss: 0.0016\n",
            "[15:24:42] Predicting on test...\n",
            "[15:25:07] Scoring...\n",
            "[15:25:08] Done setting: RunResult(setting='IoMT->IoT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.0, macro_recall=0.0, roc_auc_ovr=nan, pr_auc_ovr=0.16667349084109104, ece=0.9377289414405823, mce=0.9377289414405823, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[15:25:08] === Setting: IoT → IoMT | LR->BiLSTM | adv=True ===\n",
            "[15:25:09] Building features (train)...\n",
            "[15:25:09] Train matrix: (838860, 10)\n",
            "[15:25:09] Building features (test)...\n",
            "[15:25:10] Test  matrix: (1048575, 10)\n",
            "[15:25:10] [INFO] Skipping SMOTE-Tomek (too many samples). Light undersampling per class.\n",
            "[15:25:10] Undersampled train: (313772, 10)\n",
            "[15:25:10] Pre-fitting LR for LR->BiLSTM features...\n",
            "[15:31:27] Warmup (3 epochs) before adversarial mix...\n",
            "[15:32:11] Generating adversarial subset: n=40000 (batched)...\n",
            "[15:32:13] Training sequence model...\n",
            "Epoch 1/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7487 - loss: 0.3098 - val_categorical_accuracy: 0.7456 - val_loss: 0.2965\n",
            "Epoch 2/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7561 - loss: 0.3032 - val_categorical_accuracy: 0.7435 - val_loss: 0.2949\n",
            "Epoch 3/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7624 - loss: 0.2988 - val_categorical_accuracy: 0.7435 - val_loss: 0.2943\n",
            "Epoch 4/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7661 - loss: 0.2957 - val_categorical_accuracy: 0.7437 - val_loss: 0.2935\n",
            "Epoch 5/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7673 - loss: 0.2942 - val_categorical_accuracy: 0.7483 - val_loss: 0.2931\n",
            "Epoch 6/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7681 - loss: 0.2926 - val_categorical_accuracy: 0.7468 - val_loss: 0.2925\n",
            "Epoch 7/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7687 - loss: 0.2915 - val_categorical_accuracy: 0.7480 - val_loss: 0.2919\n",
            "Epoch 8/30\n",
            "2274/2274 - 14s - 6ms/step - categorical_accuracy: 0.7686 - loss: 0.2903 - val_categorical_accuracy: 0.7489 - val_loss: 0.2914\n",
            "Epoch 9/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7684 - loss: 0.2892 - val_categorical_accuracy: 0.7508 - val_loss: 0.2908\n",
            "Epoch 10/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7693 - loss: 0.2886 - val_categorical_accuracy: 0.7536 - val_loss: 0.2901\n",
            "Epoch 11/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7690 - loss: 0.2877 - val_categorical_accuracy: 0.7537 - val_loss: 0.2901\n",
            "Epoch 12/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7704 - loss: 0.2871 - val_categorical_accuracy: 0.7543 - val_loss: 0.2896\n",
            "Epoch 13/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7707 - loss: 0.2864 - val_categorical_accuracy: 0.7524 - val_loss: 0.2895\n",
            "Epoch 14/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7720 - loss: 0.2855 - val_categorical_accuracy: 0.7510 - val_loss: 0.2890\n",
            "Epoch 15/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7718 - loss: 0.2847 - val_categorical_accuracy: 0.7526 - val_loss: 0.2888\n",
            "Epoch 16/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7720 - loss: 0.2844 - val_categorical_accuracy: 0.7556 - val_loss: 0.2884\n",
            "Epoch 17/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7724 - loss: 0.2835 - val_categorical_accuracy: 0.7543 - val_loss: 0.2880\n",
            "Epoch 18/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7727 - loss: 0.2837 - val_categorical_accuracy: 0.7580 - val_loss: 0.2873\n",
            "Epoch 19/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7728 - loss: 0.2831 - val_categorical_accuracy: 0.7573 - val_loss: 0.2874\n",
            "Epoch 20/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7738 - loss: 0.2830 - val_categorical_accuracy: 0.7553 - val_loss: 0.2871\n",
            "Epoch 21/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7738 - loss: 0.2828 - val_categorical_accuracy: 0.7574 - val_loss: 0.2861\n",
            "Epoch 22/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7739 - loss: 0.2822 - val_categorical_accuracy: 0.7559 - val_loss: 0.2862\n",
            "Epoch 23/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7741 - loss: 0.2821 - val_categorical_accuracy: 0.7569 - val_loss: 0.2861\n",
            "Epoch 24/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7753 - loss: 0.2815 - val_categorical_accuracy: 0.7584 - val_loss: 0.2860\n",
            "Epoch 25/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7751 - loss: 0.2812 - val_categorical_accuracy: 0.7601 - val_loss: 0.2854\n",
            "Epoch 26/30\n",
            "2274/2274 - 15s - 6ms/step - categorical_accuracy: 0.7751 - loss: 0.2815 - val_categorical_accuracy: 0.7584 - val_loss: 0.2854\n",
            "Epoch 27/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7759 - loss: 0.2812 - val_categorical_accuracy: 0.7574 - val_loss: 0.2865\n",
            "Epoch 28/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7762 - loss: 0.2806 - val_categorical_accuracy: 0.7594 - val_loss: 0.2851\n",
            "Epoch 29/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7759 - loss: 0.2805 - val_categorical_accuracy: 0.7602 - val_loss: 0.2850\n",
            "Epoch 30/30\n",
            "2274/2274 - 15s - 7ms/step - categorical_accuracy: 0.7769 - loss: 0.2801 - val_categorical_accuracy: 0.7610 - val_loss: 0.2851\n",
            "[15:39:36] Predicting on test...\n",
            "[15:40:00] Scoring...\n",
            "[15:40:00] Done setting: RunResult(setting='IoT->IoMT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.18704153611581187, macro_recall=0.2, roc_auc_ovr=nan, pr_auc_ovr=0.16666675410074383, ece=0.3198511035924442, mce=0.3198511035924442, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[15:40:00] Saved: ./outputs_v3fast/domain_shift_results_v3fast.csv\n",
            "[15:40:00] Saved: ./outputs_v3fast/domain_shift_perclass_v3fast.csv\n",
            "[15:40:00] Saved: ./outputs_v3fast/domain_shift_reliability_bins_v3fast.csv\n",
            "   setting  model_name  use_adv  resampling loss_mode  macro_f1  macro_recall  roc_auc_ovr  pr_auc_ovr      ece      mce   fpr1e3  tpr_at_fpr1e3   fpr1e4  tpr_at_fpr1e4\n",
            "IoMT->IoMT     LR-only    False smote_tomek     focal  0.004003      0.041678     0.188229    0.456899 0.858611 0.992778 0.000000   0.000000e+00 0.000000       0.000000\n",
            "  IoT->IoT     LR-only    False smote_tomek     focal  0.234530      0.257609          NaN    0.310432 0.464671 0.709325 0.000030   0.000000e+00 0.000030       0.000000\n",
            " IoMT->IoT     LR-only    False smote_tomek     focal  0.053052      0.200000     0.500000    0.155069 0.000000 0.000000 0.000000   0.000000e+00 0.000000       0.000000\n",
            " IoT->IoMT     LR-only    False smote_tomek     focal  0.187042      0.200000     0.500000    0.200000 0.297830 0.297830 0.000000   0.000000e+00 0.000000       0.000000\n",
            "IoMT->IoMT BiLSTM-only    False smote_tomek     focal  0.351935      0.397355          NaN    0.428227 0.027361 0.474980 0.000957   9.891510e-07 0.000027       0.000000\n",
            "  IoT->IoT BiLSTM-only    False smote_tomek     focal  0.474802      0.486604          NaN    0.445233 0.237864 0.330744 0.000970   5.380045e-01 0.000091       0.023814\n",
            " IoMT->IoT BiLSTM-only     True smote_tomek     focal  0.053052      0.200000          NaN    0.166667 0.469665 0.469665 0.000000   0.000000e+00 0.000000       0.000000\n",
            " IoT->IoMT BiLSTM-only     True smote_tomek     focal  0.010286      0.200000          NaN    0.166667 0.621820 0.621820 0.000000   0.000000e+00 0.000000       0.000000\n",
            "IoMT->IoMT  LR->BiLSTM    False smote_tomek     focal  0.351170      0.398468          NaN    0.312831 0.026962 0.799577 0.000000   0.000000e+00 0.000000       0.000000\n",
            "  IoT->IoT  LR->BiLSTM    False smote_tomek     focal  0.473651      0.483650          NaN    0.429717 0.273326 0.667837 0.000970   2.752600e-01 0.000091       0.029721\n",
            " IoMT->IoT  LR->BiLSTM     True smote_tomek     focal  0.000000      0.000000          NaN    0.166673 0.937729 0.937729 0.000000   0.000000e+00 0.000000       0.000000\n",
            " IoT->IoMT  LR->BiLSTM     True smote_tomek     focal  0.187042      0.200000          NaN    0.166667 0.319851 0.319851 0.000000   0.000000e+00 0.000000       0.000000\n",
            "\n",
            "Zipping and downloading results...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d379c4de-0263-4611-b71a-1501a17c8abc\", \"domain_shift_results_v3fast.zip\", 16211)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ]
    }
  ]
}