{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMgBxzcBaJdwxlbNl6ltmhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankeawthong/Project-1-Lita-Chatbot/blob/main/%5B20250920%5D%20LR-BiLSTM%20IDS%20Domain-Shift%20Pipeline%20(v3-fast%20PATCH%2C%20fixed).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3) — Leakage-safe + Expanded MQTT filter\n",
        "--------------------------------------------------------------------\n",
        "- Drops label-like columns from features to prevent leakage\n",
        "- Aligns feature schemas between train and test (categorical one-hot + scaler from train)\n",
        "- Expanded MQTT filtering for NF-ToN-IoT (checks DST and SRC ports; common fallbacks)\n",
        "- Defaults to RESAMPLING=\"smote_tomek\" and LOSS_MODE=\"focal\"\n",
        "- Keeps ablations: LR-only, BiLSTM-only, LR→BiLSTM; FGSM/PGD adversarial mixing for sequence models\n",
        "- Metrics: Macro-F1, AUROC/PR-AUC, ECE/MCE, FPR@1e-3/1e-4; Per-class recall/F1; Reliability bins\n",
        "\n",
        "Outputs under ./outputs_v3/\n",
        "\n",
        "Author: Sine & Mentor"
      ],
      "metadata": {
        "id": "75WvVDW2Wgl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3 • FIXED)\n",
        "=====================================\n",
        "Key fixes based on crash/stall & 0.0 val-accuracy symptoms:\n",
        "- **Unified label mapping** (no LabelEncoder sorting surprises). We use a fixed\n",
        "  CLASS2IDX for the 6 families across *all* domains & stages.\n",
        "- **Hard guards for label range** and per-split class summaries (fail fast if a\n",
        "  label falls outside 0..C-1).\n",
        "- **Stable validation**: explicit internal train/val split instead of\n",
        "  `validation_split`, avoiding misaligned batches when concatenating adversarial\n",
        "  examples.\n",
        "- **Empty MQTT split**: skip gracefully.\n",
        "- **CPU-stable training**: default to CPU-only to avoid cuInit(303) & duplicate\n",
        "  CUDA plugin registrations seen in notebooks/containers. Can be toggled.\n",
        "- **Keras stability**: `workers=1`, `use_multiprocessing=False`,\n",
        "  `categorical_accuracy` metric, and tf.data pipelines.\n",
        "- **Benign index** based on CLASS2IDX (not list position); evaluation is\n",
        "  consistent with model output ordering.\n",
        "\n",
        "Outputs are written to ./outputs_v3/ as before.\n",
        "\n",
        "IDS Domain-Shift Pipeline (v3 • FIXED, Keras3-compatible)\n",
        "=========================================================\n",
        "This revision fixes the crash:\n",
        "  TypeError: TensorFlowTrainer.fit() got an unexpected keyword argument 'workers'\n",
        "\n",
        "Key changes\n",
        "-----------\n",
        "- **Removed** unsupported `workers`/`use_multiprocessing` args (Keras 3).\n",
        "- **Guarded resampling**: `SMOTETomek` is skipped for huge splits; use class\n",
        "  weights or light undersampling instead to avoid stalls/OOM.\n",
        "- **Adversarial cap**: upper-bound adversarial sample count to keep memory stable.\n",
        "- **CPU-stable default** retained (toggle via CONFIG[\"FORCE_CPU\"]).\n",
        "- **Unified label mapping** and per-split summaries kept.\n",
        "\n",
        "Outputs go to `./outputs_v3/`."
      ],
      "metadata": {
        "id": "EsdLCbTDXVd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3p, patched)\n",
        "Fixes:\n",
        "- Align scikit-learn predict_proba outputs (which only include present classes) to the\n",
        "  GLOBAL family schema before any downstream step (temperature scaling, LSTM features).\n",
        "- Temperature scaling now uses the global num_classes for one-hot (no index errors).\n",
        "- Keeps AUROC/PR-AUC \"present-class\" computation, threshold tuning, and MQTT relabel.\n",
        "\n",
        "Defaults: RESAMPLING=\"smote_tomek\", LOSS_MODE=\"focal\", EPOCHS=30\n",
        "Outputs: ./outputs_v3p/domain_shift_results_v3p.csv, *_perclass_v3p.csv, *_reliability_bins_v3p.csv\n",
        "\n",
        "IDS Domain-Shift Pipeline (v3-fast)\n",
        "==================================\n",
        "Performance/stability-focused revision of v3/v3p series.\n",
        "\n",
        "Key improvements\n",
        "----------------\n",
        "1) **Feature build is bounded**: only low-cardinality categoricals are one-hot\n",
        "   encoded (<= 50 uniques) to prevent 100k+ dummy columns from IP/MAC/etc.\n",
        "2) **Sparse LR path**: uses CSR matrices and 'saga' solver (n_jobs=-1) to\n",
        "   accelerate large, high-dim fits.\n",
        "3) **Adversarial generation**: only on a *random subset* (capped) and in\n",
        "   batches—no more generating adversarial examples for the full train split.\n",
        "4) **Resampling guard**: SMOTE-Tomek skipped above a cap; optional per-class\n",
        "   undersampling keeps memory bounded.\n",
        "5) **Progress logging**: timestamped logs for each stage so you can see where\n",
        "   time is spent.\n",
        "6) **Same metrics/outputs** as v3: Macro-F1, Macro-Recall, AUROC(PR-OVR),\n",
        "   calibration (ECE/MCE), FPR@1e-3/1e-4, per-class tables, reliability bins.\n",
        "\n",
        "Outputs: ./outputs_v3fast/"
      ],
      "metadata": {
        "id": "bdi98Sc3zViW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDS Domain-Shift Pipeline (v3-fast PATCH, fixed)\n",
        "================================================\n",
        "Adds/keeps:\n",
        "  (a) Present-class AUROC/PR-AUC computation\n",
        "  (b) Hardened MQTT filter (robust port detection/coercion)\n",
        "  (c) Default temperature scaling + threshold tuning (val-based)\n",
        "  (d) LR-only: expand predict_proba to NUM_CLASSES using lr.classes_\n",
        "Outputs: ./outputs_v3fast_patch/*.csv"
      ],
      "metadata": {
        "id": "KnaDVzMNkJlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wd8_af7NWfpy"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os, time, warnings, gc\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import f1_score, recall_score, roc_auc_score, average_precision_score, roc_curve, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "CONFIG = {\n",
        "    \"NF_TON_IOT_CSV\": \"Dataset_NF-ToN-IoT.csv\",\n",
        "    \"CIC_IOMT_TRAIN_CSV\": \"CIC_IoMT_2024_WiFi_MQTT_train.csv\",\n",
        "    \"CIC_IOMT_TEST_CSV\":  \"CIC_IoMT_2024_WiFi_MQTT_test.csv\",\n",
        "\n",
        "    \"NF_NUMERIC_CLASS_COL\": \"Class\",\n",
        "    \"NF_TEXT_LABEL_COL\": \"Attack\",\n",
        "    \"CIC_CLASS_COL\": \"Class\",\n",
        "    \"CIC_TEXT_LABEL_COL\": \"label\",\n",
        "\n",
        "    \"NF_DST_PORT_CANDIDATES\": [\"L4_DST_PORT\", \"Dst Port\", \"dst_port\", \"Destination Port\", \"dport\", \"dstport\",\"DstPort\"],\n",
        "    \"NF_SRC_PORT_CANDIDATES\": [\"L4_SRC_PORT\", \"Src Port\", \"src_port\", \"Source Port\", \"sport\", \"srcport\",\"SrcPort\"],\n",
        "    \"MQTT_PORTS\": [1883, 8883],\n",
        "\n",
        "    \"SEED\": 42,\n",
        "    \"EPOCHS\": 30,\n",
        "    \"BATCH_SIZE\": 128,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"PATIENCE\": 5,\n",
        "\n",
        "    \"RESAMPLING\": \"smote_tomek\",            # \"none\" | \"smote_tomek\"\n",
        "    \"LOSS_MODE\": \"focal\",                   # \"ce\" | \"class_balanced_ce\" | \"focal\"\n",
        "    \"RESAMPLING_MAX_N\": 200_000,            # skip SMOTE if above\n",
        "    \"UNDERSAMPLE_MAX_PER_CLASS\": 100_000,   # light undersample cap/class\n",
        "\n",
        "    \"USE_ADV_TRAINING\": True,\n",
        "    \"ADV_METHOD\": \"fgsm\",                   # \"fgsm\" | \"pgd\"\n",
        "    \"FGSM_EPS\": 0.05,\n",
        "    \"PGD_EPS\": 0.03,\n",
        "    \"PGD_STEPS\": 5,\n",
        "    \"PGD_ALPHA\": 0.01,\n",
        "    \"ADV_RATIO\": 0.5,\n",
        "    \"ADV_MAX_SAMPLES\": 40_000,              # generate at most this many\n",
        "    \"ADV_BATCH\": 8_000,                     # batch size for adversarial synth\n",
        "\n",
        "    \"FORCE_CPU\": True,\n",
        "    \"OUTDIR\": \"./outputs_v3fast_patch\",\n",
        "\n",
        "    # Categorical guard\n",
        "    \"CATEGORICAL_MAX_CARD\": 50,             # one-hot only if <= 50 uniques\n",
        "    \"MAX_DUMMIES\": 800                      # overall dummy cap; else drop cats\n",
        "}\n",
        "\n",
        "np.random.seed(CONFIG[\"SEED\"])\n",
        "tf.random.set_seed(CONFIG[\"SEED\"])\n",
        "\n",
        "if CONFIG[\"FORCE_CPU\"]:\n",
        "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"-1\")\n",
        "    try:\n",
        "        tf.config.set_visible_devices([], 'GPU')\n",
        "    except Exception:\n",
        "        pass\n",
        "os.makedirs(CONFIG[\"OUTDIR\"], exist_ok=True)\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S1ilr6qZWp0h",
        "outputId": "06030c1f-9747-4f09-d6a4-c1fd1de95819"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Families\n",
        "# --------------------\n",
        "FAMILIES = [\"Benign\", \"DoS_DDoS\", \"Recon_Scan\", \"MQTT\", \"Spoof\", \"Other\"]\n",
        "CLASS2IDX = {n:i for i,n in enumerate(FAMILIES)}\n",
        "IDX2CLASS = {i:n for n,i in CLASS2IDX.items()}\n",
        "NUM_CLASSES = len(FAMILIES)\n",
        "BENIGN_IDX = CLASS2IDX[\"Benign\"]"
      ],
      "metadata": {
        "id": "oxNUuGomWpxu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Helpers\n",
        "# --------------------\n",
        "def log(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
        "\n",
        "def map_label_string_to_family(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"Other\"\n",
        "    st = s.lower()\n",
        "    if \"benign\" in st or st.strip()==\"normal\": return \"Benign\"\n",
        "    if \"mqtt\" in st: return \"MQTT\"\n",
        "    if \"ddos\" in st or \"dos\" in st: return \"DoS_DDoS\"\n",
        "    if \"scan\" in st or \"recon\" in st or \"portscan\" in st: return \"Recon_Scan\"\n",
        "    if \"spoof\" in st or \"mitm\" in st or \"impersonat\" in st or \"man-in-the-middle\" in st: return \"Spoof\"\n",
        "    return \"Other\"\n",
        "\n",
        "def _first_col(df: pd.DataFrame, candidates: List[str]):\n",
        "    for c in candidates:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def _coerce_port_col(s: pd.Series) -> pd.Series:\n",
        "    # robust conversion to integer port; accepts string/float/object\n",
        "    if pd.api.types.is_numeric_dtype(s):\n",
        "        return s.round().astype(\"Int64\")\n",
        "    try:\n",
        "        return pd.to_numeric(s.astype(str).str.extract(r'(\\d+)', expand=False), errors=\"coerce\").astype(\"Int64\")\n",
        "    except Exception:\n",
        "        return pd.Series([pd.NA]*len(s), dtype=\"Int64\")\n",
        "\n",
        "def mqtt_filter_nf(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Return MQTT-emphasized subset using dst/src ports in {1883,8883}.\n",
        "       If both columns missing or subset empty, log and return full df.\"\"\"\n",
        "    dcol = _first_col(df, CONFIG[\"NF_DST_PORT_CANDIDATES\"])\n",
        "    scol = _first_col(df, CONFIG[\"NF_SRC_PORT_CANDIDATES\"])\n",
        "    if dcol is None and scol is None:\n",
        "        log(\"[WARN] No dst/src port columns found; using full NF-ToN-IoT.\")\n",
        "        return df.copy()\n",
        "\n",
        "    mask = np.zeros(len(df), dtype=bool)\n",
        "    ports = set(CONFIG[\"MQTT_PORTS\"])\n",
        "    if dcol is not None:\n",
        "        d = _coerce_port_col(df[dcol])\n",
        "        mask |= d.isin(ports).fillna(False).to_numpy()\n",
        "    if scol is not None:\n",
        "        s = _coerce_port_col(df[scol])\n",
        "        mask |= s.isin(ports).fillna(False).to_numpy()\n",
        "\n",
        "    f = df[mask]\n",
        "    if len(f)==0:\n",
        "        log(\"[WARN] MQTT filter produced empty set; using full NF-ToN-IoT.\")\n",
        "        return df.copy()\n",
        "    return f.copy()\n",
        "\n",
        "def _drop_label_like_columns(Xdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    KEYS = ['label','Label','labels','Labels','Class','class','Labal','labal',\n",
        "            'Attack','attack','Target','target','Family','family','Benign','benign']\n",
        "    return Xdf.drop(columns=[c for c in Xdf.columns if any(k in c for k in KEYS)], errors='ignore')\n",
        "\n",
        "def build_features_train_lowcard(df: pd.DataFrame, y_col: str):\n",
        "    \"\"\"Build bounded features: numeric + low-card categoricals only.\"\"\"\n",
        "    df = df.copy()\n",
        "    y = df[y_col].values\n",
        "    Xdf = _drop_label_like_columns(df.drop(columns=[y_col]))\n",
        "\n",
        "    Xnum = Xdf.select_dtypes(include=[np.number]).copy().fillna(0)\n",
        "\n",
        "    Xcat_raw = Xdf.select_dtypes(exclude=[np.number]).astype(str)\n",
        "    keep = []\n",
        "    if Xcat_raw.shape[1]>0:\n",
        "        for c in Xcat_raw.columns:\n",
        "            u = Xcat_raw[c].nunique(dropna=False)\n",
        "            if 1 < u <= CONFIG[\"CATEGORICAL_MAX_CARD\"]:\n",
        "                keep.append(c)\n",
        "    Xcat = Xcat_raw[keep] if keep else pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    if Xcat.shape[1]>0:\n",
        "        Xcat = pd.get_dummies(Xcat, dummy_na=False, drop_first=False)\n",
        "    else:\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    # If dummy explosion still too big, drop categoricals altogether\n",
        "    if Xcat.shape[1] > CONFIG[\"MAX_DUMMIES\"]:\n",
        "        log(f\"[INFO] Dropping categoricals (dummies={Xcat.shape[1]} > {CONFIG['MAX_DUMMIES']}).\")\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    Xall = pd.concat([Xnum, Xcat], axis=1)\n",
        "    scaler = StandardScaler().fit(Xall.values)\n",
        "    X = scaler.transform(Xall.values)\n",
        "    cols = Xall.columns.tolist()\n",
        "    return X, y, scaler, cols\n",
        "\n",
        "def build_features_apply_lowcard(df: pd.DataFrame, y_col: str, scaler: StandardScaler, cols_schema: list):\n",
        "    df = df.copy()\n",
        "    y = df[y_col].values\n",
        "    Xdf = _drop_label_like_columns(df.drop(columns=[y_col]))\n",
        "\n",
        "    Xnum = Xdf.select_dtypes(include=[np.number]).copy().fillna(0)\n",
        "\n",
        "    Xcat_raw = Xdf.select_dtypes(exclude=[np.number]).astype(str)\n",
        "    keep = []\n",
        "    if Xcat_raw.shape[1]>0:\n",
        "        for c in Xcat_raw.columns:\n",
        "            u = Xcat_raw[c].nunique(dropna=False)\n",
        "            if 1 < u <= CONFIG[\"CATEGORICAL_MAX_CARD\"]:\n",
        "                keep.append(c)\n",
        "    Xcat = Xcat_raw[keep] if keep else pd.DataFrame(index=Xdf.index)\n",
        "    if Xcat.shape[1]>0:\n",
        "        Xcat = pd.get_dummies(Xcat, dummy_na=False, drop_first=False)\n",
        "    else:\n",
        "        Xcat = pd.DataFrame(index=Xdf.index)\n",
        "\n",
        "    Xall = pd.concat([Xnum, Xcat], axis=1).reindex(columns=cols_schema, fill_value=0)\n",
        "    X = scaler.transform(Xall.values)\n",
        "    return X, y\n",
        "\n",
        "def encode_family_series(series: pd.Series) -> np.ndarray:\n",
        "    vals = [CLASS2IDX[map_label_string_to_family(v)] for v in series.astype(str).values]\n",
        "    return np.asarray(vals, dtype=np.int32)"
      ],
      "metadata": {
        "id": "CKaisOPKACc_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------\n",
        "# Loss / Models\n",
        "# -------------\n",
        "def focal_loss(gamma=2.0, alpha=None):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n",
        "        ce = -y_true * tf.math.log(y_pred)\n",
        "        if alpha is not None: ce = alpha * ce\n",
        "        weight = tf.pow(1.0 - y_pred, gamma)\n",
        "        return tf.reduce_sum(weight * ce, axis=1)\n",
        "    return loss\n",
        "\n",
        "def build_bilstm(input_shape, num_classes):\n",
        "    return models.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(1e-4)), input_shape=input_shape),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Bidirectional(layers.LSTM(32, kernel_regularizer=regularizers.l2(1e-4))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=regularizers.l2(1e-4))\n",
        "    ])"
      ],
      "metadata": {
        "id": "d2iiHXWmkfpM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------\n",
        "# Adv attacks (kept for compatibility; not used intensively to respect epoch budget)\n",
        "# -----------\n",
        "def fgsm(model, x, y, eps=0.05):\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)\n",
        "        pred = model(x, training=False)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y, pred)\n",
        "    grad = tape.gradient(loss, x)\n",
        "    x_adv = x + eps * tf.sign(grad)\n",
        "    return tf.clip_by_value(x_adv, -10, 10).numpy()"
      ],
      "metadata": {
        "id": "x5VwvDj1kibn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------\n",
        "# Calibration & thresholding\n",
        "# -------------\n",
        "def temp_scale_probs(proba: np.ndarray, T: float) -> np.ndarray:\n",
        "    \"\"\"Temperature scaling on probabilities using p^(1/T) then renormalize.\"\"\"\n",
        "    p = np.clip(proba, 1e-12, 1.0)\n",
        "    scaled = np.power(p, 1.0/float(T))\n",
        "    scaled /= scaled.sum(axis=1, keepdims=True)\n",
        "    return scaled\n",
        "\n",
        "def fit_temperature_prob_grid(y_val: np.ndarray, proba_val: np.ndarray, grid=None) -> float:\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.5, 2.0, 31)  # 0.5..2.0 step 0.05\n",
        "    best_T, best_nll = 1.0, np.inf\n",
        "    for T in grid:\n",
        "        pv = temp_scale_probs(proba_val, T)\n",
        "        nll = log_loss(y_val, pv, labels=list(range(NUM_CLASSES)))\n",
        "        if nll < best_nll:\n",
        "            best_nll = nll; best_T = float(T)\n",
        "    return best_T\n",
        "\n",
        "def tune_threshold(y_val_bin: np.ndarray, atk_scores_val: np.ndarray, target_fpr: float) -> float:\n",
        "    fpr, tpr, thr = roc_curve(y_val_bin, atk_scores_val)[:3]\n",
        "    idx = np.where(fpr <= target_fpr)[0]\n",
        "    if len(idx)==0:\n",
        "        # fallback: pick threshold minimizing |fpr-target|\n",
        "        j = int(np.argmin(np.abs(fpr - target_fpr)))\n",
        "        return float(thr[j])\n",
        "    j = idx[-1]\n",
        "    return float(thr[j])"
      ],
      "metadata": {
        "id": "amjB9mEZkkjt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------\n",
        "# Metrics (present-class AUROC/PR-AUC)\n",
        "# -------------\n",
        "def evaluate_multiclass_present(y_true: np.ndarray, proba: np.ndarray) -> Tuple[float, float, float, float]:\n",
        "    # macro-F1 / macro-recall over all classes (as before)\n",
        "    macro_f1 = f1_score(y_true, proba.argmax(axis=1), average='macro')\n",
        "    macro_rec = recall_score(y_true, proba.argmax(axis=1), average='macro')\n",
        "    # AUROC/PR only over classes present in y_true\n",
        "    present = np.unique(y_true)\n",
        "    if len(present) < 2:\n",
        "        return macro_f1, macro_rec, float('nan'), float('nan')\n",
        "    Yb = label_binarize(y_true, classes=present)\n",
        "    proba_sub = proba[:, present]\n",
        "    try:\n",
        "        roc = roc_auc_score(Yb, proba_sub, average='macro', multi_class='ovr')\n",
        "    except Exception:\n",
        "        roc = float('nan')\n",
        "    try:\n",
        "        pr = average_precision_score(Yb, proba_sub, average='macro')\n",
        "    except Exception:\n",
        "        pr = float('nan')\n",
        "    return macro_f1, macro_rec, roc, pr"
      ],
      "metadata": {
        "id": "GHcsELQikm5m"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------\n",
        "# Prob. expansion for LR-only\n",
        "# -------------\n",
        "def expand_proba_to_full(proba, classes, num_classes=NUM_CLASSES):\n",
        "    \"\"\"Return (n_samples, num_classes) with columns aligned to global class indices.\n",
        "       Any class not present in `classes` receives probability 0.\n",
        "    \"\"\"\n",
        "    proba = np.asarray(proba)\n",
        "    full = np.zeros((proba.shape[0], num_classes), dtype=proba.dtype)\n",
        "    classes = np.asarray(classes, dtype=int)\n",
        "    full[:, classes] = proba\n",
        "    # numerical safety: if any row sums to 0 after expansion, set Benign prob to 1\n",
        "    row_sums = full.sum(axis=1, keepdims=True)\n",
        "    zero_rows = (row_sums == 0)\n",
        "    if np.any(zero_rows):\n",
        "        full[zero_rows, BENIGN_IDX] = 1.0\n",
        "    return full"
      ],
      "metadata": {
        "id": "ERnz7Sr3kqGH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------\n",
        "# Results schema\n",
        "# -------------\n",
        "@dataclass\n",
        "class RunResult:\n",
        "    setting: str\n",
        "    model_name: str\n",
        "    use_adv: bool\n",
        "    resampling: str\n",
        "    loss_mode: str\n",
        "    macro_f1: float\n",
        "    macro_recall: float\n",
        "    roc_auc_ovr: float\n",
        "    pr_auc_ovr: float\n",
        "    ece: float\n",
        "    mce: float\n",
        "    fpr1e3: float\n",
        "    tpr_at_fpr1e3: float\n",
        "    fpr1e4: float\n",
        "    tpr_at_fpr1e4: float"
      ],
      "metadata": {
        "id": "KeK1pRQKASeQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "# Main\n",
        "# --------------------\n",
        "def main():\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    log(\"Loading CSVs...\")\n",
        "    nf = pd.read_csv(CONFIG[\"NF_TON_IOT_CSV\"])\n",
        "    cic_tr = pd.read_csv(CONFIG[\"CIC_IOMT_TRAIN_CSV\"])\n",
        "    cic_te = pd.read_csv(CONFIG[\"CIC_IOMT_TEST_CSV\"])\n",
        "\n",
        "    log(\"Mapping families...\")\n",
        "    nf_family_all = nf.get(CONFIG[\"NF_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(nf))).astype(str).apply(map_label_string_to_family)\n",
        "    cic_tr_family = cic_tr.get(CONFIG[\"CIC_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(cic_tr))).astype(str).apply(map_label_string_to_family)\n",
        "    cic_te_family = cic_te.get(CONFIG[\"CIC_TEXT_LABEL_COL\"], pd.Series([\"Other\"]*len(cic_te))).astype(str).apply(map_label_string_to_family)\n",
        "\n",
        "    cic_tr2 = cic_tr.copy(); cic_tr2[\"Family\"] = cic_tr_family.values\n",
        "    cic_te2 = cic_te.copy(); cic_te2[\"Family\"] = cic_te_family.values\n",
        "\n",
        "    log(\"Applying MQTT filter to NF-ToN-IoT...\")\n",
        "    nf_mqtt = mqtt_filter_nf(nf)\n",
        "    nf2 = nf_mqtt.copy()\n",
        "    nf2[\"Family\"] = nf_family_all.loc[nf_mqtt.index].values\n",
        "\n",
        "    # splits\n",
        "    y_nf_enc = encode_family_series(nf2[\"Family\"])\n",
        "    log(\"Splitting NF train/test...\")\n",
        "    nf_train, nf_test = train_test_split(nf2, test_size=0.2, random_state=CONFIG[\"SEED\"], stratify=y_nf_enc)\n",
        "\n",
        "    results = []\n",
        "    perclass_rows = []\n",
        "    relbin_rows = []\n",
        "\n",
        "    def reliability_bins(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15):\n",
        "        conf = probs.max(axis=1); preds = probs.argmax(axis=1)\n",
        "        correct = (preds==y_true).astype(int)\n",
        "        bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "        ece = mce = 0.0; rows = []\n",
        "        for i in range(n_bins):\n",
        "            lo, hi = bins[i], bins[i+1]\n",
        "            idx = np.where((conf>=lo) & (conf<hi))[0]\n",
        "            if len(idx)==0:\n",
        "                rows.append((0.5*(lo+hi), np.nan, 0)); continue\n",
        "            acc = correct[idx].mean(); conf_mean = conf[idx].mean()\n",
        "            gap = abs(acc-conf_mean)\n",
        "            ece += (len(idx)/len(conf))*gap; mce = max(mce, gap)\n",
        "            rows.append((conf_mean, acc, len(idx)))\n",
        "        return ece, mce, rows\n",
        "\n",
        "    def fpr_tpr_at_threshold(y_true_bin: np.ndarray, atk_scores: np.ndarray, thr: float):\n",
        "        y_pred = (atk_scores >= thr).astype(int)\n",
        "        fp = np.sum((y_pred==1) & (y_true_bin==0))\n",
        "        tp = np.sum((y_pred==1) & (y_true_bin==1))\n",
        "        tn = np.sum((y_pred==0) & (y_true_bin==0))\n",
        "        fn = np.sum((y_pred==0) & (y_true_bin==1))\n",
        "        fpr = fp / (fp + tn + 1e-12)\n",
        "        tpr = tp / (tp + fn + 1e-12)\n",
        "        return float(fpr), float(tpr)\n",
        "\n",
        "    def run_setting(train_domain: str, test_domain: str, model_kind: str, use_adv: bool) -> RunResult:\n",
        "        log(f\"=== Setting: {train_domain} → {test_domain} | {model_kind} | adv={use_adv} ===\")\n",
        "\n",
        "        df_tr = cic_tr2 if train_domain==\"IoMT\" else nf_train\n",
        "        df_te = cic_te2 if test_domain==\"IoMT\" else (nf_test if (train_domain==\"IoT\" and test_domain==\"IoT\") else nf2)\n",
        "\n",
        "        df_tr = df_tr.copy(); df_te = df_te.copy()\n",
        "        df_tr[\"y_enc\"] = encode_family_series(df_tr[\"Family\"])\n",
        "        df_te[\"y_enc\"] = encode_family_series(df_te[\"Family\"])\n",
        "\n",
        "        # Features\n",
        "        log(\"Building features (train)...\")\n",
        "        Xtr, ytr, scaler, cols_schema = build_features_train_lowcard(df_tr.drop(columns=[\"Family\"]).rename(columns={\"y_enc\":\"Target\"}), \"Target\")\n",
        "        log(f\"Train matrix: {Xtr.shape}\")\n",
        "\n",
        "        log(\"Building features (test)...\")\n",
        "        Xte, yte = build_features_apply_lowcard(df_te.drop(columns=[\"Family\"]).rename(columns={\"y_enc\":\"Target\"}), \"Target\", scaler, cols_schema)\n",
        "        log(f\"Test  matrix: {Xte.shape}\")\n",
        "\n",
        "        # Validation split for calibration/threshold tuning\n",
        "        Xtr_main, Xval, ytr_main, yval = train_test_split(Xtr, ytr, test_size=0.2, random_state=CONFIG[\"SEED\"], stratify=ytr)\n",
        "\n",
        "        # Resampling (guarded) on training MAIN only\n",
        "        if CONFIG[\"RESAMPLING\"]==\"smote_tomek\" and len(ytr_main) <= CONFIG[\"RESAMPLING_MAX_N\"]:\n",
        "            log(\"Applying SMOTE-Tomek on training-main...\")\n",
        "            Xtr_main, ytr_main = SMOTETomek(random_state=CONFIG[\"SEED\"]).fit_resample(Xtr_main, ytr_main)\n",
        "            log(f\"Resampled train-main: {Xtr_main.shape}\")\n",
        "        elif CONFIG[\"RESAMPLING\"]==\"smote_tomek\":\n",
        "            log(\"[INFO] Skipping SMOTE-Tomek (too many samples).\")\n",
        "\n",
        "        # ---- Train per branch\n",
        "        # Temperature and thresholds default\n",
        "        T = 1.0; thr_1e3 = None; thr_1e4 = None\n",
        "\n",
        "        if model_kind==\"LR-only\":\n",
        "            log(\"Fitting LR (saga) on train-main...\")\n",
        "            lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
        "            lr.fit(csr_matrix(Xtr_main), ytr_main)\n",
        "\n",
        "            proba_val_raw = lr.predict_proba(csr_matrix(Xval))\n",
        "            proba_val = expand_proba_to_full(proba_val_raw, lr.classes_, NUM_CLASSES)\n",
        "            # Temperature scaling on probabilities\n",
        "            T = fit_temperature_prob_grid(yval, proba_val)\n",
        "            proba_val_T = temp_scale_probs(proba_val, T)\n",
        "\n",
        "            # Threshold tuning on attack score using validation\n",
        "            atk_val = 1.0 - proba_val_T[:, BENIGN_IDX]\n",
        "            yval_bin = (yval != BENIGN_IDX).astype(int)\n",
        "            thr_1e3 = tune_threshold(yval_bin, atk_val, 1e-3)\n",
        "            thr_1e4 = tune_threshold(yval_bin, atk_val, 1e-4)\n",
        "\n",
        "            # Predictions on test\n",
        "            proba_te_raw = lr.predict_proba(csr_matrix(Xte))\n",
        "            proba_te = expand_proba_to_full(proba_te_raw, lr.classes_, NUM_CLASSES)\n",
        "            proba_te = temp_scale_probs(proba_te, T)\n",
        "\n",
        "        else:\n",
        "            # Sequence preparation\n",
        "            Xtr_seq = Xtr_main.reshape((Xtr_main.shape[0],1,Xtr_main.shape[1]))\n",
        "            Xval_seq = Xval.reshape((Xval.shape[0],1,Xval.shape[1]))\n",
        "            Xte_seq = Xte.reshape((Xte.shape[0],1,Xte.shape[1]))\n",
        "            ytr_oh = to_categorical(ytr_main, num_classes=NUM_CLASSES)\n",
        "            yval_oh = to_categorical(yval, num_classes=NUM_CLASSES)\n",
        "\n",
        "            if CONFIG[\"LOSS_MODE\"]==\"class_balanced_ce\":\n",
        "                classes = np.arange(NUM_CLASSES)\n",
        "                weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytr_main)\n",
        "                cw = {int(c): float(w) for c, w in zip(classes, weights)}\n",
        "                loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "            elif CONFIG[\"LOSS_MODE\"]==\"focal\":\n",
        "                cw=None; loss_fn=focal_loss(gamma=2.0)\n",
        "            else:\n",
        "                cw=None; loss_fn=tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "            if model_kind==\"BiLSTM-only\":\n",
        "                model = build_bilstm(Xtr_seq.shape[1:], NUM_CLASSES)\n",
        "                Xtrain_in, ytrain_in = Xtr_seq, ytr_oh\n",
        "            elif model_kind==\"LR->BiLSTM\":\n",
        "                log(\"Pre-fitting LR for LR->BiLSTM features...\")\n",
        "                lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='saga', n_jobs=-1)\n",
        "                lr.fit(csr_matrix(Xtr_main), ytr_main)\n",
        "                Xlr_tr = lr.predict_proba(Xtr_main)[:, np.newaxis, :]\n",
        "                Xlr_val = lr.predict_proba(Xval)[:, np.newaxis, :]\n",
        "                Xlr_te  = lr.predict_proba(Xte)[:, np.newaxis, :]\n",
        "                Xtrain_in, ytrain_in = Xlr_tr, ytr_oh\n",
        "                Xval_seq, Xte_seq = Xlr_val, Xlr_te\n",
        "                model = build_bilstm(Xlr_tr.shape[1:], NUM_CLASSES)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown model_kind\")\n",
        "\n",
        "            model.compile(optimizer=tf.keras.optimizers.Adam(CONFIG[\"LEARNING_RATE\"]), loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "            es = callbacks.EarlyStopping(patience=CONFIG[\"PATIENCE\"], restore_best_weights=True, monitor='val_loss')\n",
        "\n",
        "            # Optional adversarial warmup (kept brief to respect 30-epoch budget)\n",
        "            if CONFIG[\"USE_ADV_TRAINING\"] and use_adv and model_kind!=\"LR-only\":\n",
        "                log(\"Warmup (3 epochs) before main training...\")\n",
        "                model.fit(Xtrain_in, ytrain_in, validation_data=(Xval_seq, yval_oh),\n",
        "                          epochs=3, batch_size=CONFIG[\"BATCH_SIZE\"], callbacks=[es], verbose=0)\n",
        "\n",
        "            log(\"Training sequence model...\")\n",
        "            model.fit(Xtrain_in, ytrain_in, validation_data=(Xval_seq, yval_oh),\n",
        "                      epochs=CONFIG[\"EPOCHS\"], batch_size=CONFIG[\"BATCH_SIZE\"], callbacks=[es], verbose=2)\n",
        "\n",
        "            log(\"Validating for temperature/thresholds...\")\n",
        "            proba_val = model.predict(Xval_seq, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
        "            T = fit_temperature_prob_grid(yval, proba_val)\n",
        "            proba_val_T = temp_scale_probs(proba_val, T)\n",
        "            atk_val = 1.0 - proba_val_T[:, BENIGN_IDX]\n",
        "            yval_bin = (yval != BENIGN_IDX).astype(int)\n",
        "            thr_1e3 = tune_threshold(yval_bin, atk_val, 1e-3)\n",
        "            thr_1e4 = tune_threshold(yval_bin, atk_val, 1e-4)\n",
        "\n",
        "            log(\"Predicting on test...\")\n",
        "            proba_te = model.predict(Xte_seq, batch_size=CONFIG[\"BATCH_SIZE\"], verbose=0)\n",
        "            proba_te = temp_scale_probs(proba_te, T)\n",
        "\n",
        "        # -------- Metrics (present-class AUROC/PR, calibrated) --------\n",
        "        log(\"Scoring...\")\n",
        "        yte_enc = df_te[\"y_enc\"].values.astype(int)\n",
        "        macro_f1, macro_rec, roc_ovr, pr_ovr = evaluate_multiclass_present(yte_enc, proba_te)\n",
        "\n",
        "        # Reliability\n",
        "        def reliability_bins(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15):\n",
        "            conf = probs.max(axis=1); preds = probs.argmax(axis=1)\n",
        "            correct = (preds==y_true).astype(int)\n",
        "            bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "            ece = mce = 0.0; rows = []\n",
        "            for i in range(n_bins):\n",
        "                lo, hi = bins[i], bins[i+1]\n",
        "                idx = np.where((conf>=lo) & (conf<hi))[0]\n",
        "                if len(idx)==0:\n",
        "                    rows.append((0.5*(lo+hi), np.nan, 0)); continue\n",
        "                acc = correct[idx].mean(); conf_mean = conf[idx].mean()\n",
        "                gap = abs(acc-conf_mean)\n",
        "                ece += (len(idx)/len(conf))*gap; mce = max(mce, gap)\n",
        "                rows.append((conf_mean, acc, len(idx)))\n",
        "            return ece, mce, rows\n",
        "\n",
        "        ece, mce, rel = reliability_bins(proba_te, yte_enc, n_bins=15)\n",
        "\n",
        "        # Low-FPR using tuned thresholds learned on validation\n",
        "        atk_scores_test = 1.0 - proba_te[:, BENIGN_IDX]\n",
        "        y_bin_test = (yte_enc != BENIGN_IDX).astype(int)\n",
        "        fpr1e3, tpr1e3 = fpr_tpr_at_threshold(y_bin_test, atk_scores_test, thr_1e3)\n",
        "        fpr1e4, tpr1e4 = fpr_tpr_at_threshold(y_bin_test, atk_scores_test, thr_1e4)\n",
        "\n",
        "        # Per-class table\n",
        "        y_pred = proba_te.argmax(axis=1)\n",
        "        for c in range(NUM_CLASSES):\n",
        "            idxs = np.where(yte_enc==c)[0]\n",
        "            if len(idxs)==0:\n",
        "                rec=f1= np.nan; sup=0\n",
        "            else:\n",
        "                tp=int(np.sum(y_pred[idxs]==c)); fn=int(len(idxs)-tp); fp=int(np.sum((y_pred==c)&(yte_enc!=c)))\n",
        "                rec = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
        "                prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
        "                f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
        "                sup=len(idxs)\n",
        "            perclass_rows.append({\"setting\": f\"{train_domain}->{test_domain}\", \"model_name\": model_kind, \"use_adv\": use_adv, \"class\": IDX2CLASS[c], \"recall\": float(rec), \"f1\": float(f1), \"support\": int(sup)})\n",
        "\n",
        "        for (cm, ac, ct) in rel:\n",
        "            relbin_rows.append({\"setting\": f\"{train_domain}->{test_domain}\", \"model_name\": model_kind, \"use_adv\": use_adv, \"conf_mean\": float(0.0 if cm!=cm else cm), \"acc\": float(0.0 if ac!=ac else ac), \"count\": int(ct)})\n",
        "\n",
        "        res = RunResult(setting=f\"{train_domain}->{test_domain}\", model_name=model_kind, use_adv=use_adv,\n",
        "                        resampling=CONFIG[\"RESAMPLING\"], loss_mode=CONFIG[\"LOSS_MODE\"],\n",
        "                        macro_f1=float(macro_f1), macro_recall=float(macro_rec),\n",
        "                        roc_auc_ovr=float(roc_ovr) if not np.isnan(roc_ovr) else np.nan,\n",
        "                        pr_auc_ovr=float(pr_ovr) if not np.isnan(pr_ovr) else np.nan,\n",
        "                        ece=float(ece), mce=float(mce),\n",
        "                        fpr1e3=float(fpr1e3), tpr_at_fpr1e3=float(tpr1e3),\n",
        "                        fpr1e4=float(fpr1e4), tpr_at_fpr1e4=float(tpr1e4))\n",
        "        log(f\"Done setting: {res}\")\n",
        "        return res\n",
        "\n",
        "    all_results = []\n",
        "    for model_kind in [\"LR-only\", \"BiLSTM-only\", \"LR->BiLSTM\"]:\n",
        "        all_results.append(run_setting(\"IoMT\",\"IoMT\",model_kind, use_adv=False))\n",
        "        all_results.append(run_setting(\"IoT\",\"IoT\",model_kind, use_adv=False))\n",
        "        all_results.append(run_setting(\"IoMT\",\"IoT\",model_kind, use_adv=(model_kind!=\"LR-only\")))\n",
        "        all_results.append(run_setting(\"IoT\",\"IoMT\",model_kind, use_adv=(model_kind!=\"LR-only\")))\n",
        "\n",
        "    df = pd.DataFrame([asdict(r) for r in all_results])\n",
        "    out_csv = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_results_v3fast_patch.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "    out_pc = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_perclass_v3fast_patch.csv\")\n",
        "    pd.DataFrame(perclass_rows).to_csv(out_pc, index=False)\n",
        "\n",
        "    out_rel = os.path.join(CONFIG[\"OUTDIR\"], \"domain_shift_reliability_bins_v3fast_patch.csv\")\n",
        "    pd.DataFrame(relbin_rows).to_csv(out_rel, index=False)\n",
        "\n",
        "    log(f\"Saved: {out_csv}\")\n",
        "    log(f\"Saved: {out_pc}\")\n",
        "    log(f\"Saved: {out_rel}\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9LxqpPC4eX",
        "outputId": "39c84958-9433-4b5c-c225-b6fdef29293f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:23:30] Loading CSVs...\n",
            "[07:23:41] Mapping families...\n",
            "[07:23:42] Applying MQTT filter to NF-ToN-IoT...\n",
            "[07:23:42] [WARN] MQTT filter produced empty set; using full NF-ToN-IoT.\n",
            "[07:23:42] Splitting NF train/test...\n",
            "[07:23:43] === Setting: IoMT → IoMT | LR-only | adv=False ===\n",
            "[07:23:44] Building features (train)...\n",
            "[07:23:48] Train matrix: (1048575, 45)\n",
            "[07:23:48] Building features (test)...\n",
            "[07:23:49] Test  matrix: (1048575, 45)\n",
            "[07:23:50] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[07:23:50] Fitting LR (saga) on train-main...\n",
            "[07:41:29] Scoring...\n",
            "[07:41:31] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.3533607626083345, macro_recall=0.39870529934670623, roc_auc_ovr=0.5394370441237826, pr_auc_ovr=0.4385367075483012, ece=0.04733414566423899, mce=0.636891949805561, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[07:41:31] === Setting: IoT → IoT | LR-only | adv=False ===\n",
            "[07:41:31] Building features (train)...\n",
            "[07:41:31] Train matrix: (838860, 10)\n",
            "[07:41:31] Building features (test)...\n",
            "[07:41:31] Test  matrix: (209715, 10)\n",
            "[07:41:31] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[07:41:31] Fitting LR (saga) on train-main...\n",
            "[07:52:21] Scoring...\n",
            "[07:52:22] Done setting: RunResult(setting='IoT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.4493035426524757, macro_recall=0.44275641759270173, roc_auc_ovr=0.8271233855528296, pr_auc_ovr=0.49485549217914954, ece=0.027953155263160008, mce=0.678847786881166, fpr1e3=3.0327843993570496e-05, tpr_at_fpr1e3=0.0, fpr1e4=3.0327843993570496e-05, tpr_at_fpr1e4=0.0)\n",
            "[07:52:22] === Setting: IoMT → IoT | LR-only | adv=False ===\n",
            "[07:52:22] Building features (train)...\n",
            "[07:52:24] Train matrix: (1048575, 45)\n",
            "[07:52:24] Building features (test)...\n",
            "[07:52:24] Test  matrix: (1048575, 45)\n",
            "[07:52:25] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[07:52:25] Fitting LR (saga) on train-main...\n",
            "[08:11:15] Scoring...\n",
            "[08:11:16] Done setting: RunResult(setting='IoMT->IoT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.2, ece=0.8470905753044661, mce=0.8470905753044661, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[08:11:16] === Setting: IoT → IoMT | LR-only | adv=False ===\n",
            "[08:11:17] Building features (train)...\n",
            "[08:11:17] Train matrix: (838860, 10)\n",
            "[08:11:17] Building features (test)...\n",
            "[08:11:18] Test  matrix: (1048575, 10)\n",
            "[08:11:18] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[08:11:18] Fitting LR (saga) on train-main...\n",
            "[08:21:50] Scoring...\n",
            "[08:21:51] Done setting: RunResult(setting='IoT->IoMT', model_name='LR-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.18704153611581187, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.2, ece=0.1936993571204355, mce=0.1936993571204355, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[08:21:51] === Setting: IoMT → IoMT | BiLSTM-only | adv=False ===\n",
            "[08:21:51] Building features (train)...\n",
            "[08:21:52] Train matrix: (1048575, 45)\n",
            "[08:21:52] Building features (test)...\n",
            "[08:21:53] Test  matrix: (1048575, 45)\n",
            "[08:21:54] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[08:21:54] Training sequence model...\n",
            "Epoch 1/30\n",
            "6554/6554 - 41s - 6ms/step - categorical_accuracy: 0.9989 - loss: 0.0124 - val_categorical_accuracy: 0.9999 - val_loss: 0.0015\n",
            "Epoch 2/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0012 - val_categorical_accuracy: 0.9999 - val_loss: 7.9128e-04\n",
            "Epoch 3/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9999 - loss: 7.5132e-04 - val_categorical_accuracy: 0.9999 - val_loss: 6.4683e-04\n",
            "Epoch 4/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 6.4991e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.7518e-04\n",
            "Epoch 5/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 6.0653e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.4246e-04\n",
            "Epoch 6/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.8379e-04 - val_categorical_accuracy: 0.9998 - val_loss: 6.4028e-04\n",
            "Epoch 7/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.7031e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.8727e-04\n",
            "Epoch 8/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.6776e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.3564e-04\n",
            "Epoch 9/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.4777e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.8347e-04\n",
            "Epoch 10/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.5960e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.9386e-04\n",
            "Epoch 11/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.6520e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.9235e-04\n",
            "Epoch 12/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.5042e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.5558e-04\n",
            "Epoch 13/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.4828e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.5780e-04\n",
            "Epoch 14/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.2833e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.1496e-04\n",
            "[08:30:28] Validating for temperature/thresholds...\n",
            "[08:30:34] Predicting on test...\n",
            "[08:30:52] Scoring...\n",
            "[08:30:54] Done setting: RunResult(setting='IoMT->IoMT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.3734572773668006, macro_recall=0.3940485036997784, roc_auc_ovr=0.9574560232975061, pr_auc_ovr=0.574115963221607, ece=0.05314724922123328, mce=0.7325156535167092, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[08:30:54] === Setting: IoT → IoT | BiLSTM-only | adv=False ===\n",
            "[08:30:54] Building features (train)...\n",
            "[08:30:54] Train matrix: (838860, 10)\n",
            "[08:30:54] Building features (test)...\n",
            "[08:30:54] Test  matrix: (209715, 10)\n",
            "[08:30:55] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[08:30:55] Training sequence model...\n",
            "Epoch 1/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8519 - loss: 0.2094 - val_categorical_accuracy: 0.8630 - val_loss: 0.1823\n",
            "Epoch 2/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8614 - loss: 0.1839 - val_categorical_accuracy: 0.8688 - val_loss: 0.1768\n",
            "Epoch 3/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8643 - loss: 0.1800 - val_categorical_accuracy: 0.8691 - val_loss: 0.1744\n",
            "Epoch 4/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8654 - loss: 0.1786 - val_categorical_accuracy: 0.8682 - val_loss: 0.1732\n",
            "Epoch 5/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8655 - loss: 0.1776 - val_categorical_accuracy: 0.8711 - val_loss: 0.1722\n",
            "Epoch 6/30\n",
            "5243/5243 - 29s - 5ms/step - categorical_accuracy: 0.8654 - loss: 0.1770 - val_categorical_accuracy: 0.8685 - val_loss: 0.1715\n",
            "Epoch 7/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8659 - loss: 0.1763 - val_categorical_accuracy: 0.8694 - val_loss: 0.1710\n",
            "Epoch 8/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8661 - loss: 0.1759 - val_categorical_accuracy: 0.8701 - val_loss: 0.1714\n",
            "Epoch 9/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8658 - loss: 0.1753 - val_categorical_accuracy: 0.8714 - val_loss: 0.1707\n",
            "Epoch 10/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8661 - loss: 0.1751 - val_categorical_accuracy: 0.8703 - val_loss: 0.1700\n",
            "Epoch 11/30\n",
            "5243/5243 - 29s - 5ms/step - categorical_accuracy: 0.8663 - loss: 0.1747 - val_categorical_accuracy: 0.8725 - val_loss: 0.1698\n",
            "Epoch 12/30\n",
            "5243/5243 - 29s - 5ms/step - categorical_accuracy: 0.8665 - loss: 0.1744 - val_categorical_accuracy: 0.8728 - val_loss: 0.1695\n",
            "Epoch 13/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8665 - loss: 0.1743 - val_categorical_accuracy: 0.8710 - val_loss: 0.1696\n",
            "Epoch 14/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8664 - loss: 0.1742 - val_categorical_accuracy: 0.8725 - val_loss: 0.1697\n",
            "Epoch 15/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8664 - loss: 0.1743 - val_categorical_accuracy: 0.8717 - val_loss: 0.1694\n",
            "Epoch 16/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8665 - loss: 0.1739 - val_categorical_accuracy: 0.8729 - val_loss: 0.1693\n",
            "Epoch 17/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8666 - loss: 0.1738 - val_categorical_accuracy: 0.8719 - val_loss: 0.1690\n",
            "Epoch 18/30\n",
            "5243/5243 - 28s - 5ms/step - categorical_accuracy: 0.8667 - loss: 0.1739 - val_categorical_accuracy: 0.8736 - val_loss: 0.1691\n",
            "Epoch 19/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8668 - loss: 0.1738 - val_categorical_accuracy: 0.8731 - val_loss: 0.1691\n",
            "Epoch 20/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8666 - loss: 0.1738 - val_categorical_accuracy: 0.8714 - val_loss: 0.1687\n",
            "Epoch 21/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8668 - loss: 0.1737 - val_categorical_accuracy: 0.8693 - val_loss: 0.1688\n",
            "Epoch 22/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8670 - loss: 0.1736 - val_categorical_accuracy: 0.8728 - val_loss: 0.1688\n",
            "Epoch 23/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8671 - loss: 0.1734 - val_categorical_accuracy: 0.8742 - val_loss: 0.1687\n",
            "Epoch 24/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8670 - loss: 0.1736 - val_categorical_accuracy: 0.8722 - val_loss: 0.1684\n",
            "Epoch 25/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8672 - loss: 0.1733 - val_categorical_accuracy: 0.8717 - val_loss: 0.1682\n",
            "Epoch 26/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8671 - loss: 0.1731 - val_categorical_accuracy: 0.8730 - val_loss: 0.1681\n",
            "Epoch 27/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8674 - loss: 0.1733 - val_categorical_accuracy: 0.8730 - val_loss: 0.1681\n",
            "Epoch 28/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8675 - loss: 0.1731 - val_categorical_accuracy: 0.8744 - val_loss: 0.1683\n",
            "Epoch 29/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8673 - loss: 0.1729 - val_categorical_accuracy: 0.8738 - val_loss: 0.1680\n",
            "Epoch 30/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8674 - loss: 0.1729 - val_categorical_accuracy: 0.8732 - val_loss: 0.1679\n",
            "[08:45:30] Validating for temperature/thresholds...\n",
            "[08:45:35] Predicting on test...\n",
            "[08:45:39] Scoring...\n",
            "[08:45:39] Done setting: RunResult(setting='IoT->IoT', model_name='BiLSTM-only', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.4898535229925017, macro_recall=0.47797449422532035, roc_auc_ovr=0.8994597405556339, pr_auc_ovr=0.5315530869048679, ece=0.03217702800034481, mce=0.10744961834193423, fpr1e3=0.0010311466957813968, tpr_at_fpr1e3=0.5638161840422763, fpr1e4=6.065568798714099e-05, tpr_at_fpr1e4=0.012306073259327154)\n",
            "[08:45:39] === Setting: IoMT → IoT | BiLSTM-only | adv=True ===\n",
            "[08:45:40] Building features (train)...\n",
            "[08:45:41] Train matrix: (1048575, 45)\n",
            "[08:45:41] Building features (test)...\n",
            "[08:45:41] Test  matrix: (1048575, 45)\n",
            "[08:45:42] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[08:45:42] Warmup (3 epochs) before main training...\n",
            "[08:47:34] Training sequence model...\n",
            "Epoch 1/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 6.6654e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.8853e-04\n",
            "Epoch 2/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.9509e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.8744e-04\n",
            "Epoch 3/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.9232e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.1277e-04\n",
            "Epoch 4/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9999 - loss: 5.8694e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.3363e-04\n",
            "Epoch 5/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.7104e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.4636e-04\n",
            "Epoch 6/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 5.6434e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.9669e-04\n",
            "Epoch 7/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9999 - loss: 5.3366e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.7222e-04\n",
            "Epoch 8/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.6308e-04 - val_categorical_accuracy: 0.9999 - val_loss: 5.2981e-04\n",
            "Epoch 9/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.4412e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.7694e-04\n",
            "Epoch 10/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.4447e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.8816e-04\n",
            "Epoch 11/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.3904e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.7646e-04\n",
            "Epoch 12/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 5.3717e-04 - val_categorical_accuracy: 0.9999 - val_loss: 4.8129e-04\n",
            "[08:54:51] Validating for temperature/thresholds...\n",
            "[08:54:57] Predicting on test...\n",
            "[08:55:16] Scoring...\n",
            "[08:55:16] Done setting: RunResult(setting='IoMT->IoT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=0.5, pr_auc_ovr=0.2, ece=0.6242131324678185, mce=0.6242131324678185, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[08:55:16] === Setting: IoT → IoMT | BiLSTM-only | adv=True ===\n",
            "[08:55:18] Building features (train)...\n",
            "[08:55:18] Train matrix: (838860, 10)\n",
            "[08:55:18] Building features (test)...\n",
            "[08:55:19] Test  matrix: (1048575, 10)\n",
            "[08:55:19] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[08:55:19] Warmup (3 epochs) before main training...\n",
            "[08:56:55] Training sequence model...\n",
            "Epoch 1/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8656 - loss: 0.1785 - val_categorical_accuracy: 0.8691 - val_loss: 0.1735\n",
            "Epoch 2/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8657 - loss: 0.1776 - val_categorical_accuracy: 0.8707 - val_loss: 0.1727\n",
            "Epoch 3/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8659 - loss: 0.1766 - val_categorical_accuracy: 0.8706 - val_loss: 0.1720\n",
            "Epoch 4/30\n",
            "5243/5243 - 29s - 6ms/step - categorical_accuracy: 0.8661 - loss: 0.1762 - val_categorical_accuracy: 0.8704 - val_loss: 0.1717\n",
            "Epoch 5/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8663 - loss: 0.1755 - val_categorical_accuracy: 0.8697 - val_loss: 0.1707\n",
            "Epoch 6/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8664 - loss: 0.1751 - val_categorical_accuracy: 0.8719 - val_loss: 0.1708\n",
            "Epoch 7/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8665 - loss: 0.1749 - val_categorical_accuracy: 0.8735 - val_loss: 0.1702\n",
            "Epoch 8/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8663 - loss: 0.1747 - val_categorical_accuracy: 0.8721 - val_loss: 0.1703\n",
            "Epoch 9/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8665 - loss: 0.1744 - val_categorical_accuracy: 0.8733 - val_loss: 0.1698\n",
            "Epoch 10/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8667 - loss: 0.1743 - val_categorical_accuracy: 0.8729 - val_loss: 0.1697\n",
            "Epoch 11/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8665 - loss: 0.1739 - val_categorical_accuracy: 0.8725 - val_loss: 0.1694\n",
            "Epoch 12/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8665 - loss: 0.1739 - val_categorical_accuracy: 0.8702 - val_loss: 0.1692\n",
            "Epoch 13/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8666 - loss: 0.1737 - val_categorical_accuracy: 0.8744 - val_loss: 0.1694\n",
            "Epoch 14/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8667 - loss: 0.1737 - val_categorical_accuracy: 0.8709 - val_loss: 0.1690\n",
            "Epoch 15/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8669 - loss: 0.1737 - val_categorical_accuracy: 0.8739 - val_loss: 0.1691\n",
            "Epoch 16/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8669 - loss: 0.1736 - val_categorical_accuracy: 0.8731 - val_loss: 0.1689\n",
            "Epoch 17/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8668 - loss: 0.1736 - val_categorical_accuracy: 0.8734 - val_loss: 0.1690\n",
            "Epoch 18/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8666 - loss: 0.1735 - val_categorical_accuracy: 0.8739 - val_loss: 0.1690\n",
            "Epoch 19/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8667 - loss: 0.1735 - val_categorical_accuracy: 0.8726 - val_loss: 0.1686\n",
            "Epoch 20/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8668 - loss: 0.1734 - val_categorical_accuracy: 0.8743 - val_loss: 0.1687\n",
            "Epoch 21/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8669 - loss: 0.1734 - val_categorical_accuracy: 0.8737 - val_loss: 0.1686\n",
            "Epoch 22/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8671 - loss: 0.1732 - val_categorical_accuracy: 0.8746 - val_loss: 0.1687\n",
            "Epoch 23/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8670 - loss: 0.1731 - val_categorical_accuracy: 0.8734 - val_loss: 0.1687\n",
            "Epoch 24/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8667 - loss: 0.1732 - val_categorical_accuracy: 0.8722 - val_loss: 0.1686\n",
            "[09:09:09] Validating for temperature/thresholds...\n",
            "[09:09:14] Predicting on test...\n",
            "[09:09:32] Scoring...\n",
            "[09:09:33] Done setting: RunResult(setting='IoT->IoMT', model_name='BiLSTM-only', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.010286076389243773, macro_recall=0.2, roc_auc_ovr=0.49996661863953173, pr_auc_ovr=0.20000086389068655, ece=0.43021116123779723, mce=0.43021116123779723, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[09:09:33] === Setting: IoMT → IoMT | LR->BiLSTM | adv=False ===\n",
            "[09:09:34] Building features (train)...\n",
            "[09:09:35] Train matrix: (1048575, 45)\n",
            "[09:09:35] Building features (test)...\n",
            "[09:09:35] Test  matrix: (1048575, 45)\n",
            "[09:09:36] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[09:09:36] Pre-fitting LR for LR->BiLSTM features...\n",
            "[09:29:14] Training sequence model...\n",
            "Epoch 1/30\n",
            "6554/6554 - 43s - 7ms/step - categorical_accuracy: 0.9988 - loss: 0.0133 - val_categorical_accuracy: 0.9998 - val_loss: 0.0022\n",
            "Epoch 2/30\n",
            "6554/6554 - 41s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0017 - val_categorical_accuracy: 0.9998 - val_loss: 0.0013\n",
            "Epoch 3/30\n",
            "6554/6554 - 40s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0013 - val_categorical_accuracy: 0.9998 - val_loss: 0.0011\n",
            "Epoch 4/30\n",
            "6554/6554 - 40s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 5/30\n",
            "6554/6554 - 40s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 6/30\n",
            "6554/6554 - 40s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 7/30\n",
            "6554/6554 - 41s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 9.9619e-04\n",
            "Epoch 8/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 9.9193e-04\n",
            "Epoch 9/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.8251e-04\n",
            "Epoch 10/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7951e-04\n",
            "Epoch 11/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7501e-04\n",
            "Epoch 12/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7275e-04\n",
            "Epoch 13/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6604e-04\n",
            "Epoch 14/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6567e-04\n",
            "Epoch 15/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6351e-04\n",
            "Epoch 16/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.5808e-04\n",
            "Epoch 17/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.9969e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5721e-04\n",
            "Epoch 18/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.9548e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5447e-04\n",
            "Epoch 19/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.5324e-04\n",
            "Epoch 20/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.9669e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4719e-04\n",
            "Epoch 21/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.9101e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5111e-04\n",
            "Epoch 22/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8288e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5044e-04\n",
            "Epoch 23/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8682e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4873e-04\n",
            "Epoch 24/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8889e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5034e-04\n",
            "Epoch 25/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8948e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4901e-04\n",
            "[09:45:21] Validating for temperature/thresholds...\n",
            "[09:45:27] Predicting on test...\n",
            "[09:45:47] Scoring...\n",
            "[09:45:48] Done setting: RunResult(setting='IoMT->IoMT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.3536811316030374, macro_recall=0.39873344158130664, roc_auc_ovr=0.7088272573733628, pr_auc_ovr=0.42632629387339416, ece=0.06239828630688901, mce=0.5076621502463581, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[09:45:48] === Setting: IoT → IoT | LR->BiLSTM | adv=False ===\n",
            "[09:45:49] Building features (train)...\n",
            "[09:45:49] Train matrix: (838860, 10)\n",
            "[09:45:49] Building features (test)...\n",
            "[09:45:49] Test  matrix: (209715, 10)\n",
            "[09:45:49] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[09:45:49] Pre-fitting LR for LR->BiLSTM features...\n",
            "[09:57:10] Training sequence model...\n",
            "Epoch 1/30\n",
            "5243/5243 - 35s - 7ms/step - categorical_accuracy: 0.8396 - loss: 0.2279 - val_categorical_accuracy: 0.8495 - val_loss: 0.2070\n",
            "Epoch 2/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8454 - loss: 0.2076 - val_categorical_accuracy: 0.8547 - val_loss: 0.1997\n",
            "Epoch 3/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8480 - loss: 0.2030 - val_categorical_accuracy: 0.8513 - val_loss: 0.1973\n",
            "Epoch 4/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8493 - loss: 0.2013 - val_categorical_accuracy: 0.8556 - val_loss: 0.1962\n",
            "Epoch 5/30\n",
            "5243/5243 - 30s - 6ms/step - categorical_accuracy: 0.8496 - loss: 0.2006 - val_categorical_accuracy: 0.8554 - val_loss: 0.1957\n",
            "Epoch 6/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8501 - loss: 0.2001 - val_categorical_accuracy: 0.8571 - val_loss: 0.1951\n",
            "Epoch 7/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8511 - loss: 0.1997 - val_categorical_accuracy: 0.8572 - val_loss: 0.1945\n",
            "Epoch 8/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8518 - loss: 0.1993 - val_categorical_accuracy: 0.8604 - val_loss: 0.1942\n",
            "Epoch 9/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8524 - loss: 0.1990 - val_categorical_accuracy: 0.8614 - val_loss: 0.1936\n",
            "Epoch 10/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8536 - loss: 0.1982 - val_categorical_accuracy: 0.8616 - val_loss: 0.1924\n",
            "Epoch 11/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8546 - loss: 0.1976 - val_categorical_accuracy: 0.8622 - val_loss: 0.1918\n",
            "Epoch 12/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8548 - loss: 0.1972 - val_categorical_accuracy: 0.8620 - val_loss: 0.1914\n",
            "Epoch 13/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1970 - val_categorical_accuracy: 0.8614 - val_loss: 0.1912\n",
            "Epoch 14/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8547 - loss: 0.1968 - val_categorical_accuracy: 0.8620 - val_loss: 0.1908\n",
            "Epoch 15/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1964 - val_categorical_accuracy: 0.8612 - val_loss: 0.1902\n",
            "Epoch 16/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8553 - loss: 0.1961 - val_categorical_accuracy: 0.8612 - val_loss: 0.1898\n",
            "Epoch 17/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8549 - loss: 0.1958 - val_categorical_accuracy: 0.8618 - val_loss: 0.1896\n",
            "Epoch 18/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1956 - val_categorical_accuracy: 0.8618 - val_loss: 0.1894\n",
            "Epoch 19/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8549 - loss: 0.1955 - val_categorical_accuracy: 0.8615 - val_loss: 0.1892\n",
            "Epoch 20/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8552 - loss: 0.1952 - val_categorical_accuracy: 0.8618 - val_loss: 0.1889\n",
            "Epoch 21/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8550 - loss: 0.1952 - val_categorical_accuracy: 0.8619 - val_loss: 0.1888\n",
            "Epoch 22/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8553 - loss: 0.1951 - val_categorical_accuracy: 0.8615 - val_loss: 0.1890\n",
            "Epoch 23/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8550 - loss: 0.1951 - val_categorical_accuracy: 0.8626 - val_loss: 0.1891\n",
            "Epoch 24/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8552 - loss: 0.1949 - val_categorical_accuracy: 0.8620 - val_loss: 0.1889\n",
            "Epoch 25/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8554 - loss: 0.1950 - val_categorical_accuracy: 0.8618 - val_loss: 0.1889\n",
            "Epoch 26/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8550 - loss: 0.1950 - val_categorical_accuracy: 0.8615 - val_loss: 0.1887\n",
            "Epoch 27/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8547 - loss: 0.1950 - val_categorical_accuracy: 0.8613 - val_loss: 0.1886\n",
            "Epoch 28/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8550 - loss: 0.1949 - val_categorical_accuracy: 0.8620 - val_loss: 0.1887\n",
            "Epoch 29/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1949 - val_categorical_accuracy: 0.8613 - val_loss: 0.1888\n",
            "Epoch 30/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8549 - loss: 0.1949 - val_categorical_accuracy: 0.8616 - val_loss: 0.1887\n",
            "[10:12:59] Validating for temperature/thresholds...\n",
            "[10:13:04] Predicting on test...\n",
            "[10:13:08] Scoring...\n",
            "[10:13:08] Done setting: RunResult(setting='IoT->IoT', model_name='LR->BiLSTM', use_adv=False, resampling='smote_tomek', loss_mode='focal', macro_f1=0.47575129182367454, macro_recall=0.4619809140295792, roc_auc_ovr=0.8604957146151154, pr_auc_ovr=0.5052618994630548, ece=0.03947629690215764, mce=0.1862749375332663, fpr1e3=0.0008795074758135445, tpr_at_fpr1e3=0.166972196761381, fpr1e4=0.000181967063961423, tpr_at_fpr1e4=4.52637177354562e-05)\n",
            "[10:13:08] === Setting: IoMT → IoT | LR->BiLSTM | adv=True ===\n",
            "[10:13:09] Building features (train)...\n",
            "[10:13:12] Train matrix: (1048575, 45)\n",
            "[10:13:12] Building features (test)...\n",
            "[10:13:13] Test  matrix: (1048575, 45)\n",
            "[10:13:13] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[10:13:13] Pre-fitting LR for LR->BiLSTM features...\n",
            "[10:32:57] Warmup (3 epochs) before main training...\n",
            "[10:34:57] Training sequence model...\n",
            "Epoch 1/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 0.0010\n",
            "Epoch 2/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0011 - val_categorical_accuracy: 0.9998 - val_loss: 9.9230e-04\n",
            "Epoch 3/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.9172e-04\n",
            "Epoch 4/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.8318e-04\n",
            "Epoch 5/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7949e-04\n",
            "Epoch 6/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.8337e-04\n",
            "Epoch 7/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7904e-04\n",
            "Epoch 8/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7718e-04\n",
            "Epoch 9/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7216e-04\n",
            "Epoch 10/30\n",
            "6554/6554 - 39s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.7009e-04\n",
            "Epoch 11/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6752e-04\n",
            "Epoch 12/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6122e-04\n",
            "Epoch 13/30\n",
            "6554/6554 - 38s - 6ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.6080e-04\n",
            "Epoch 14/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 0.0010 - val_categorical_accuracy: 0.9998 - val_loss: 9.5929e-04\n",
            "Epoch 15/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.9828e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5825e-04\n",
            "Epoch 16/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.9217e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5258e-04\n",
            "Epoch 17/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.9498e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5512e-04\n",
            "Epoch 18/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.9567e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5405e-04\n",
            "Epoch 19/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8177e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5568e-04\n",
            "Epoch 20/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8892e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5434e-04\n",
            "Epoch 21/30\n",
            "6554/6554 - 36s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8681e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4751e-04\n",
            "Epoch 22/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8851e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4796e-04\n",
            "Epoch 23/30\n",
            "6554/6554 - 34s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8654e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4657e-04\n",
            "Epoch 24/30\n",
            "6554/6554 - 34s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8775e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5063e-04\n",
            "Epoch 25/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8545e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.5002e-04\n",
            "Epoch 26/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8434e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4548e-04\n",
            "Epoch 27/30\n",
            "6554/6554 - 36s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8150e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4639e-04\n",
            "Epoch 28/30\n",
            "6554/6554 - 37s - 6ms/step - categorical_accuracy: 0.9998 - loss: 9.8377e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4834e-04\n",
            "Epoch 29/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8158e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4457e-04\n",
            "Epoch 30/30\n",
            "6554/6554 - 35s - 5ms/step - categorical_accuracy: 0.9998 - loss: 9.8493e-04 - val_categorical_accuracy: 0.9998 - val_loss: 9.4688e-04\n",
            "[10:53:17] Validating for temperature/thresholds...\n",
            "[10:53:23] Predicting on test...\n",
            "[10:53:40] Scoring...\n",
            "[10:53:40] Done setting: RunResult(setting='IoMT->IoT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.053051669600434115, macro_recall=0.2, roc_auc_ovr=0.5000003875219764, pr_auc_ovr=0.20000012867676994, ece=0.8465960951715233, mce=0.8465960951715233, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[10:53:40] === Setting: IoT → IoMT | LR->BiLSTM | adv=True ===\n",
            "[10:53:41] Building features (train)...\n",
            "[10:53:42] Train matrix: (838860, 10)\n",
            "[10:53:42] Building features (test)...\n",
            "[10:53:42] Test  matrix: (1048575, 10)\n",
            "[10:53:43] [INFO] Skipping SMOTE-Tomek (too many samples).\n",
            "[10:53:43] Pre-fitting LR for LR->BiLSTM features...\n",
            "[11:05:17] Warmup (3 epochs) before main training...\n",
            "[11:06:58] Training sequence model...\n",
            "Epoch 1/30\n",
            "5243/5243 - 31s - 6ms/step - categorical_accuracy: 0.8493 - loss: 0.2013 - val_categorical_accuracy: 0.8542 - val_loss: 0.1961\n",
            "Epoch 2/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8499 - loss: 0.2005 - val_categorical_accuracy: 0.8550 - val_loss: 0.1958\n",
            "Epoch 3/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8506 - loss: 0.2000 - val_categorical_accuracy: 0.8535 - val_loss: 0.1952\n",
            "Epoch 4/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8510 - loss: 0.1998 - val_categorical_accuracy: 0.8580 - val_loss: 0.1947\n",
            "Epoch 5/30\n",
            "5243/5243 - 32s - 6ms/step - categorical_accuracy: 0.8520 - loss: 0.1993 - val_categorical_accuracy: 0.8608 - val_loss: 0.1942\n",
            "Epoch 6/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8526 - loss: 0.1990 - val_categorical_accuracy: 0.8612 - val_loss: 0.1938\n",
            "Epoch 7/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8530 - loss: 0.1986 - val_categorical_accuracy: 0.8613 - val_loss: 0.1933\n",
            "Epoch 8/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8537 - loss: 0.1981 - val_categorical_accuracy: 0.8617 - val_loss: 0.1922\n",
            "Epoch 9/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8547 - loss: 0.1972 - val_categorical_accuracy: 0.8603 - val_loss: 0.1914\n",
            "Epoch 10/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8548 - loss: 0.1968 - val_categorical_accuracy: 0.8607 - val_loss: 0.1907\n",
            "Epoch 11/30\n",
            "5243/5243 - 35s - 7ms/step - categorical_accuracy: 0.8550 - loss: 0.1963 - val_categorical_accuracy: 0.8609 - val_loss: 0.1901\n",
            "Epoch 12/30\n",
            "5243/5243 - 37s - 7ms/step - categorical_accuracy: 0.8550 - loss: 0.1960 - val_categorical_accuracy: 0.8614 - val_loss: 0.1899\n",
            "Epoch 13/30\n",
            "5243/5243 - 35s - 7ms/step - categorical_accuracy: 0.8552 - loss: 0.1957 - val_categorical_accuracy: 0.8616 - val_loss: 0.1894\n",
            "Epoch 14/30\n",
            "5243/5243 - 35s - 7ms/step - categorical_accuracy: 0.8551 - loss: 0.1956 - val_categorical_accuracy: 0.8617 - val_loss: 0.1895\n",
            "Epoch 15/30\n",
            "5243/5243 - 35s - 7ms/step - categorical_accuracy: 0.8550 - loss: 0.1954 - val_categorical_accuracy: 0.8618 - val_loss: 0.1891\n",
            "Epoch 16/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8554 - loss: 0.1952 - val_categorical_accuracy: 0.8625 - val_loss: 0.1892\n",
            "Epoch 17/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1952 - val_categorical_accuracy: 0.8622 - val_loss: 0.1889\n",
            "Epoch 18/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1953 - val_categorical_accuracy: 0.8621 - val_loss: 0.1891\n",
            "Epoch 19/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8552 - loss: 0.1951 - val_categorical_accuracy: 0.8616 - val_loss: 0.1890\n",
            "Epoch 20/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8553 - loss: 0.1951 - val_categorical_accuracy: 0.8628 - val_loss: 0.1890\n",
            "Epoch 21/30\n",
            "5243/5243 - 33s - 6ms/step - categorical_accuracy: 0.8551 - loss: 0.1952 - val_categorical_accuracy: 0.8624 - val_loss: 0.1890\n",
            "Epoch 22/30\n",
            "5243/5243 - 34s - 6ms/step - categorical_accuracy: 0.8555 - loss: 0.1951 - val_categorical_accuracy: 0.8625 - val_loss: 0.1889\n",
            "[11:19:18] Validating for temperature/thresholds...\n",
            "[11:19:23] Predicting on test...\n",
            "[11:19:45] Scoring...\n",
            "[11:19:45] Done setting: RunResult(setting='IoT->IoMT', model_name='LR->BiLSTM', use_adv=True, resampling='smote_tomek', loss_mode='focal', macro_f1=0.0, macro_recall=0.0, roc_auc_ovr=0.5000013478760561, pr_auc_ovr=0.20000024131667998, ece=0.6753235459327698, mce=0.6753235459327698, fpr1e3=0.0, tpr_at_fpr1e3=0.0, fpr1e4=0.0, tpr_at_fpr1e4=0.0)\n",
            "[11:19:46] Saved: ./outputs_v3fast_patch/domain_shift_results_v3fast_patch.csv\n",
            "[11:19:46] Saved: ./outputs_v3fast_patch/domain_shift_perclass_v3fast_patch.csv\n",
            "[11:19:46] Saved: ./outputs_v3fast_patch/domain_shift_reliability_bins_v3fast_patch.csv\n",
            "   setting  model_name  use_adv  resampling loss_mode  macro_f1  macro_recall  roc_auc_ovr  pr_auc_ovr      ece      mce   fpr1e3  tpr_at_fpr1e3   fpr1e4  tpr_at_fpr1e4\n",
            "IoMT->IoMT     LR-only    False smote_tomek     focal  0.353361      0.398705     0.539437    0.438537 0.047334 0.636892 0.000000       0.000000 0.000000       0.000000\n",
            "  IoT->IoT     LR-only    False smote_tomek     focal  0.449304      0.442756     0.827123    0.494855 0.027953 0.678848 0.000030       0.000000 0.000030       0.000000\n",
            " IoMT->IoT     LR-only    False smote_tomek     focal  0.053052      0.200000     0.500000    0.200000 0.847091 0.847091 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT     LR-only    False smote_tomek     focal  0.187042      0.200000     0.500000    0.200000 0.193699 0.193699 0.000000       0.000000 0.000000       0.000000\n",
            "IoMT->IoMT BiLSTM-only    False smote_tomek     focal  0.373457      0.394049     0.957456    0.574116 0.053147 0.732516 0.000000       0.000000 0.000000       0.000000\n",
            "  IoT->IoT BiLSTM-only    False smote_tomek     focal  0.489854      0.477974     0.899460    0.531553 0.032177 0.107450 0.001031       0.563816 0.000061       0.012306\n",
            " IoMT->IoT BiLSTM-only     True smote_tomek     focal  0.053052      0.200000     0.500000    0.200000 0.624213 0.624213 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT BiLSTM-only     True smote_tomek     focal  0.010286      0.200000     0.499967    0.200001 0.430211 0.430211 0.000000       0.000000 0.000000       0.000000\n",
            "IoMT->IoMT  LR->BiLSTM    False smote_tomek     focal  0.353681      0.398733     0.708827    0.426326 0.062398 0.507662 0.000000       0.000000 0.000000       0.000000\n",
            "  IoT->IoT  LR->BiLSTM    False smote_tomek     focal  0.475751      0.461981     0.860496    0.505262 0.039476 0.186275 0.000880       0.166972 0.000182       0.000045\n",
            " IoMT->IoT  LR->BiLSTM     True smote_tomek     focal  0.053052      0.200000     0.500000    0.200000 0.846596 0.846596 0.000000       0.000000 0.000000       0.000000\n",
            " IoT->IoMT  LR->BiLSTM     True smote_tomek     focal  0.000000      0.000000     0.500001    0.200000 0.675324 0.675324 0.000000       0.000000 0.000000       0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def zip_and_download_results():\n",
        "    output_dir = CONFIG[\"OUTDIR\"]\n",
        "    csv_files = [\n",
        "        os.path.join(output_dir, \"domain_shift_results_v3fast_patch.csv\"),\n",
        "        os.path.join(output_dir, \"domain_shift_perclass_v3fast_patch.csv\"),\n",
        "        os.path.join(output_dir, \"domain_shift_reliability_bins_v3fast_patch.csv\")\n",
        "    ]\n",
        "    zip_filename = os.path.join(output_dir, \"domain_shift_results_v3fast_patch.zip\")\n",
        "\n",
        "    print(\"\\nZipping and downloading results...\")\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "            for file in csv_files:\n",
        "                if os.path.exists(file):\n",
        "                    zipf.write(file, os.path.basename(file))\n",
        "                else:\n",
        "                    print(f\"Warning: File not found and will not be included in the zip: {file}\")\n",
        "\n",
        "        if os.path.exists(zip_filename):\n",
        "            files.download(zip_filename)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(\"Zip file was not created.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during zipping or downloading: {e}\")\n",
        "\n",
        "# Call the download function\n",
        "zip_and_download_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nRIZMIv05AJC",
        "outputId": "f9556b57-e53a-425e-ebef-65b24927e534"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zipping and downloading results...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b3221676-f455-4e8b-bba5-23bb9750d5a7\", \"domain_shift_results_v3fast_patch.zip\", 16372)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ]
    }
  ]
}