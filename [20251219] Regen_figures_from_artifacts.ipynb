{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOKJG5XF7oWRu/hmddgpF7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankeawthong/Project-1-Lita-Chatbot/blob/main/%5B20251219%5D%20Regen_figures_from_artifacts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "regen_figures_from_artifacts.py\n",
        "\n",
        "Regenerate publication figures (Reliability diagram, ROC overlay, PR overlay) from the latest\n",
        "trained artifacts/results.\n",
        "\n",
        "Supports:\n",
        "  (A) CIC-IoMT Option A (uncalibrated vs temperature-scaled vs isotonic) on HELD-OUT TEST\n",
        "  (B) NF-ToN-IoT in-domain (optional) ROC/PR/reliability from trained artifacts\n",
        "  (C) Robustness curves (optional): AUROC/AUPR/FPR@95%DR vs epsilon from metrics JSON files\n",
        "\n",
        "Key design:\n",
        "- Calibration is fit ONLY on the validation-calibration subset (Val-cal), never on Test.\n",
        "- For CIC Option A, this script matches the split sizes in your summary by default:\n",
        "    Train = provided CIC train file\n",
        "    Holdout = provided CIC test file -> split 50/50 into Validation and Test\n",
        "    Validation -> split 50/50 into Val-selection and Val-calibration\n",
        "\n",
        "Outputs:\n",
        "  outdir/\n",
        "    roc_overlay_CIC_OptionA.png\n",
        "    pr_overlay_CIC_OptionA.png\n",
        "    reliability_overlay_CIC_OptionA.png\n",
        "    optionA_split_audit.json\n",
        "    optionA_plot_metrics.json\n",
        "\n",
        "Optional NF outputs (if NF inputs provided):\n",
        "    roc_NF_in_domain.png\n",
        "    pr_NF_in_domain.png\n",
        "    reliability_NF_in_domain.png\n",
        "    nf_plot_metrics.json\n",
        "\n",
        "Optional robustness outputs (if --metrics-dir provided):\n",
        "    robust_NF_auroc_vs_eps.png\n",
        "    robust_NF_aupr_vs_eps.png\n",
        "    robust_NF_fpr95_vs_eps.png\n",
        "    robust_parse_summary.json\n",
        "\n",
        "Usage examples:\n",
        "\n",
        "1) CIC Option A figures (recommended)\n",
        "python regen_figures_from_artifacts.py \\\n",
        "  --cic-train-csv CIC_train.csv \\\n",
        "  --cic-test-csv  CIC_test.csv \\\n",
        "  --cic-label-col Label \\\n",
        "  --cic-pipe-joblib artifacts/CIC_OptionA_pipe.joblib \\\n",
        "  --cic-mlp-joblib  artifacts/CIC_OptionA_mlp.joblib \\\n",
        "  --cic-temp-meta   CIC_IoMT__OptionA__Calibrated(temperature)__meta.json \\\n",
        "  --random-state 42 \\\n",
        "  --outdir paper_exports/optionA_figures\n",
        "\n",
        "2) Add NF in-domain plots\n",
        "... plus:\n",
        "  --nf-test-csv NF_test.csv --nf-label-col Label \\\n",
        "  --nf-pipe-joblib artifacts/NF_in_domain_pipe.joblib \\\n",
        "  --nf-mlp-joblib artifacts/NF_in_domain_mlp.joblib\n",
        "\n",
        "3) Add robustness curves from JSON metrics\n",
        "... plus:\n",
        "  --metrics-dir ./results_dir_containing_metrics_jsons\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V7rCqhhReakB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r78izSEQeUKM"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import (\n",
        "    roc_curve, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score,\n",
        "    brier_score_loss\n",
        ")\n",
        "from sklearn.isotonic import IsotonicRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Custom transformer stub for joblib loading (must be in __main__)\n",
        "# -----------------------------------------------------------------\n",
        "class SafeNaNDropper(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        X = np.where(np.isfinite(X), X, np.nan)\n",
        "        self.keep_mask_ = ~np.all(np.isnan(X), axis=0)\n",
        "        if not np.any(self.keep_mask_):\n",
        "            raise ValueError(\"All features are NaN after cleaning.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        X = np.where(np.isfinite(X), X, np.nan)\n",
        "        return X[:, self.keep_mask_]"
      ],
      "metadata": {
        "id": "CMjywJJZehHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------\n",
        "# Label utilities\n",
        "# -----------------\n",
        "def ensure_binary_labels(y):\n",
        "    \"\"\"\n",
        "    Map labels to {0,1} where 1=attack, 0=benign.\n",
        "\n",
        "    Supports:\n",
        "      - numeric {0,1}\n",
        "      - other numeric: smallest value -> benign, others -> attack (heuristic)\n",
        "      - strings containing \"benign\"/\"normal\" -> benign\n",
        "      - fallback: most frequent label -> benign\n",
        "    \"\"\"\n",
        "    y = pd.Series(y)\n",
        "\n",
        "    if pd.api.types.is_numeric_dtype(y):\n",
        "        uniq = sorted(y.dropna().unique().tolist())\n",
        "        if set(uniq) <= {0, 1}:\n",
        "            return y.astype(int).to_numpy()\n",
        "        benign_val = min(uniq)\n",
        "        return (y != benign_val).astype(int).to_numpy()\n",
        "\n",
        "    y_str = y.astype(str).str.lower().str.strip()\n",
        "    benign_tokens = {\"benign\", \"normal\", \"0\", \"false\", \"legit\", \"legitimate\", \"background\"}\n",
        "    is_benign = y_str.isin(benign_tokens) | y_str.str.contains(\"benign\") | y_str.str.contains(\"normal\")\n",
        "    if is_benign.sum() == 0:\n",
        "        benign_label = y_str.value_counts().idxmax()\n",
        "        is_benign = (y_str == benign_label)\n",
        "    return (~is_benign).astype(int).to_numpy()"
      ],
      "metadata": {
        "id": "rpagijgfelM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# Split utilities (CIC Option A to match your summary counts)\n",
        "# -----------------------------------------------------------\n",
        "def cic_optionA_indices_from_provided_files(y_holdout, random_state=42):\n",
        "    \"\"\"\n",
        "    Given labels for the provided CIC test file (treated as HOLDOUT pool),\n",
        "    split into:\n",
        "      - Validation (50%)\n",
        "      - Test (50%)\n",
        "    then split Validation into:\n",
        "      - Val-selection (50% of val)\n",
        "      - Val-calibration (50% of val)\n",
        "\n",
        "    Returns: val_sel_idx, val_cal_idx, test_idx (all indices relative to holdout array)\n",
        "    \"\"\"\n",
        "    y_holdout = np.asarray(y_holdout).astype(int)\n",
        "\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=random_state)\n",
        "    val_idx, test_idx = next(sss1.split(np.zeros_like(y_holdout), y_holdout))\n",
        "\n",
        "    y_val = y_holdout[val_idx]\n",
        "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=random_state)\n",
        "    val_sel_rel, val_cal_rel = next(sss2.split(np.zeros_like(y_val), y_val))\n",
        "\n",
        "    val_sel_idx = val_idx[val_sel_rel]\n",
        "    val_cal_idx = val_idx[val_cal_rel]\n",
        "\n",
        "    return val_sel_idx, val_cal_idx, test_idx"
      ],
      "metadata": {
        "id": "Pq2oRCMOqHUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Model inference utilities\n",
        "# --------------------------\n",
        "def predict_hybrid(pipe, mlp, X_df):\n",
        "    \"\"\"\n",
        "    Hybrid inference:\n",
        "      - pipe produces a linear score (decision_function); fallback to logit(p_lr)\n",
        "      - mlp consumes that score as 1D input and returns p(attack)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        z = pipe.decision_function(X_df).reshape(-1, 1)\n",
        "    except Exception:\n",
        "        p_lr = np.clip(pipe.predict_proba(X_df)[:, 1], 1e-7, 1 - 1e-7)\n",
        "        z = np.log(p_lr / (1.0 - p_lr)).reshape(-1, 1)\n",
        "\n",
        "    p = mlp.predict_proba(z)[:, 1]\n",
        "    return np.asarray(p, dtype=float)\n",
        "\n",
        "\n",
        "def _logit(p):\n",
        "    p = np.clip(p, 1e-7, 1 - 1e-7)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "\n",
        "def _sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def apply_temperature_scaling(p_uncal, T):\n",
        "    z = _logit(p_uncal)\n",
        "    return _sigmoid(z / float(T))\n",
        "\n",
        "\n",
        "def fit_isotonic(p_val, y_val):\n",
        "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "    ir.fit(p_val, y_val)\n",
        "    return ir"
      ],
      "metadata": {
        "id": "WlCP7pHlqLF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Reliability diagram utilities\n",
        "# -------------------------------\n",
        "def reliability_curve(p, y, n_bins=15):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    y = np.asarray(y, dtype=int)\n",
        "\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(p, bins) - 1\n",
        "    bin_ids = np.clip(bin_ids, 0, n_bins - 1)\n",
        "\n",
        "    acc = np.full(n_bins, np.nan, dtype=float)\n",
        "    conf = np.full(n_bins, np.nan, dtype=float)\n",
        "    counts = np.zeros(n_bins, dtype=int)\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        mask = (bin_ids == b)\n",
        "        counts[b] = int(mask.sum())\n",
        "        if counts[b] > 0:\n",
        "            acc[b] = float(y[mask].mean())\n",
        "            conf[b] = float(p[mask].mean())\n",
        "\n",
        "    ece = 0.0\n",
        "    n = len(p)\n",
        "    for b in range(n_bins):\n",
        "        if counts[b] > 0:\n",
        "            ece += (counts[b] / n) * abs(acc[b] - conf[b])\n",
        "\n",
        "    bins_centers = (bins[:-1] + bins[1:]) / 2.0\n",
        "    return bins_centers, acc, conf, float(ece)"
      ],
      "metadata": {
        "id": "-y5hrzFuqNwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------\n",
        "#     Plotting\n",
        "# -----------------\n",
        "def plot_roc_overlay(y_true, variants, out_png, title):\n",
        "    plt.figure(figsize=(6.2, 5.2))\n",
        "    metrics = {}\n",
        "    for name, p in variants.items():\n",
        "        fpr, tpr, _ = roc_curve(y_true, p)\n",
        "        auc = roc_auc_score(y_true, p)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUROC={auc:.6f})\", linewidth=1.8)\n",
        "        metrics[name] = {\"auroc\": float(auc)}\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.0)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(title)\n",
        "    plt.legend(fontsize=8, frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_pr_overlay(y_true, variants, out_png, title):\n",
        "    plt.figure(figsize=(6.2, 5.2))\n",
        "    metrics = {}\n",
        "    for name, p in variants.items():\n",
        "        prec, rec, _ = precision_recall_curve(y_true, p)\n",
        "        aupr = average_precision_score(y_true, p)\n",
        "        plt.plot(rec, prec, label=f\"{name} (AUPR={aupr:.6f})\", linewidth=1.8)\n",
        "        metrics[name] = {\"aupr\": float(aupr)}\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(title)\n",
        "    plt.legend(fontsize=8, frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_reliability_overlay(y_true, variants, out_png, title, n_bins=15):\n",
        "    plt.figure(figsize=(6.2, 5.2))\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.0)\n",
        "    metrics = {}\n",
        "    for name, p in variants.items():\n",
        "        _, acc, conf, ece = reliability_curve(p, y_true, n_bins=n_bins)\n",
        "        mask = ~np.isnan(acc) & ~np.isnan(conf)\n",
        "        plt.plot(conf[mask], acc[mask], marker=\"o\", linewidth=1.8, label=f\"{name} (ECE={ece:.4g})\")\n",
        "        metrics[name] = {\n",
        "            \"ece\": float(ece),\n",
        "            \"brier\": float(brier_score_loss(y_true, p)),\n",
        "        }\n",
        "    plt.xlabel(\"Mean predicted probability\")\n",
        "    plt.ylabel(\"Empirical accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.legend(fontsize=8, frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "iXVPSmzdqS5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# Robustness curves from metrics JSON files (optional)\n",
        "# ----------------------------------------------------\n",
        "def parse_robustness_metrics(metrics_dir: Path):\n",
        "    pat = re.compile(r\"NF_ToN_IoT__in_domain__(FGSM|PGD)_eps=([0-9.]+)__binary_metrics\\.json$\")\n",
        "    out = {\"FGSM\": [], \"PGD\": []}\n",
        "\n",
        "    for p in metrics_dir.glob(\"NF_ToN_IoT__in_domain__*__binary_metrics.json\"):\n",
        "        m = pat.search(p.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        atk = m.group(1)\n",
        "        eps = float(m.group(2))\n",
        "        with open(p, \"r\") as f:\n",
        "            d = json.load(f)\n",
        "\n",
        "        auroc = d.get(\"auroc\", d.get(\"AUROC\"))\n",
        "        aupr = d.get(\"aupr\", d.get(\"AUPR\"))\n",
        "        fpr95 = d.get(\"fpr_at_dr95\", d.get(\"FPR@DR=0.95\", d.get(\"fpr95\")))\n",
        "\n",
        "        out[atk].append({\n",
        "            \"eps\": eps,\n",
        "            \"auroc\": float(auroc) if auroc is not None else None,\n",
        "            \"aupr\": float(aupr) if aupr is not None else None,\n",
        "            \"fpr95\": float(fpr95) if fpr95 is not None else None,\n",
        "            \"file\": p.name,\n",
        "        })\n",
        "\n",
        "    for atk in out:\n",
        "        out[atk] = sorted(out[atk], key=lambda x: x[\"eps\"])\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_robustness_curve(curves, key, out_png, title):\n",
        "    plt.figure(figsize=(6.2, 5.2))\n",
        "    any_plotted = False\n",
        "    for atk, rows in curves.items():\n",
        "        xs = [r[\"eps\"] for r in rows if r.get(key) is not None]\n",
        "        ys = [r[key] for r in rows if r.get(key) is not None]\n",
        "        if len(xs) == 0:\n",
        "            continue\n",
        "        plt.plot(xs, ys, marker=\"o\", linewidth=1.8, label=atk)\n",
        "        any_plotted = True\n",
        "    plt.xlabel(\"epsilon\")\n",
        "    plt.ylabel(key)\n",
        "    plt.title(title)\n",
        "    if any_plotted:\n",
        "        plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "\n",
        "    # CIC inputs\n",
        "    ap.add_argument(\"--cic-train-csv\", type=str, required=True)\n",
        "    ap.add_argument(\"--cic-test-csv\", type=str, required=True)\n",
        "    ap.add_argument(\"--cic-label-col\", type=str, default=\"Label\")\n",
        "    ap.add_argument(\"--cic-drop-cols\", type=str, nargs=\"*\", default=[])\n",
        "    ap.add_argument(\"--cic-pipe-joblib\", type=str, required=True)\n",
        "    ap.add_argument(\"--cic-mlp-joblib\", type=str, required=True)\n",
        "    ap.add_argument(\"--cic-temp-meta\", type=str, required=True)\n",
        "\n",
        "    # Optional NF inputs\n",
        "    ap.add_argument(\"--nf-test-csv\", type=str, default=None)\n",
        "    ap.add_argument(\"--nf-label-col\", type=str, default=\"Label\")\n",
        "    ap.add_argument(\"--nf-drop-cols\", type=str, nargs=\"*\", default=[])\n",
        "    ap.add_argument(\"--nf-pipe-joblib\", type=str, default=None)\n",
        "    ap.add_argument(\"--nf-mlp-joblib\", type=str, default=None)\n",
        "\n",
        "    # Optional metrics directory (robustness curves)\n",
        "    ap.add_argument(\"--metrics-dir\", type=str, default=None)\n",
        "\n",
        "    ap.add_argument(\"--random-state\", type=int, default=42)\n",
        "    ap.add_argument(\"--outdir\", type=str, default=\"paper_exports/regen_figures\")\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # FIX: Explicitly provide arguments for Colab execution\n",
        "    # ------------------------------------------------------------------------\n",
        "    args_list = [\n",
        "        \"--cic-train-csv\", \"CIC_IoMT_2024_WiFi_MQTT_train.csv\",\n",
        "        \"--cic-test-csv\", \"CIC_IoMT_2024_WiFi_MQTT_test.csv\",\n",
        "        \"--cic-label-col\", \"label\",\n",
        "        \"--cic-pipe-joblib\", \"CIC_OptionA_pipe.joblib\",\n",
        "        \"--cic-mlp-joblib\", \"CIC_OptionA_mlp.joblib\",\n",
        "        \"--cic-temp-meta\", \"CIC_IoMT__OptionA__Calibrated(temperature)__meta.json\",\n",
        "        \"--outdir\", \"paper_exports/regen_figures\"\n",
        "    ]\n",
        "    print(f\"Running with arguments: {args_list}\")\n",
        "    args = ap.parse_args(args_list)\n",
        "\n",
        "    outdir = Path(args.outdir)\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # -----------------\n",
        "    # CIC Option A: load and split\n",
        "    # -----------------\n",
        "    df_train = pd.read_csv(args.cic_train_csv)\n",
        "    df_holdout = pd.read_csv(args.cic_test_csv)\n",
        "\n",
        "    if args.cic_label_col not in df_train.columns or args.cic_label_col not in df_holdout.columns:\n",
        "        raise ValueError(f\"CIC label column '{args.cic_label_col}' must exist in both provided files.\")\n",
        "\n",
        "    y_holdout = ensure_binary_labels(df_holdout[args.cic_label_col].values)\n",
        "\n",
        "    drop_cic = set(args.cic_drop_cols + [args.cic_label_col])\n",
        "    X_holdout = df_holdout.drop(columns=[c for c in drop_cic if c in df_holdout.columns])\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # FIX: Handle feature mismatch (likely missing metadata columns)\n",
        "    # The pipeline expects 53 columns (based on SafeNaNDropper mask).\n",
        "    # The provided test data has 46 features (after dropping label).\n",
        "    # We pad with dummy columns to match the shape.\n",
        "    # ---------------------------------------------------------------------\n",
        "    target_dims = 53\n",
        "    current_dims = X_holdout.shape[1]\n",
        "    if current_dims < target_dims:\n",
        "        missing = target_dims - current_dims\n",
        "        print(f\"DEBUG: Padding input with {missing} dummy columns to match pipeline expectation ({target_dims}).\")\n",
        "        # Prepend columns (assuming missing metadata at start)\n",
        "        dummy_data = np.zeros((len(X_holdout), missing))\n",
        "        dummy_df = pd.DataFrame(dummy_data, columns=[f\"dummy_{i}\" for i in range(missing)], index=X_holdout.index)\n",
        "        X_holdout = pd.concat([dummy_df, X_holdout], axis=1)\n",
        "\n",
        "    val_sel_idx, val_cal_idx, test_idx = cic_optionA_indices_from_provided_files(\n",
        "        y_holdout, random_state=args.random_state\n",
        "    )\n",
        "\n",
        "    pipe_cic = joblib.load(args.cic_pipe_joblib)\n",
        "    mlp_cic = joblib.load(args.cic_mlp_joblib)\n",
        "\n",
        "    p_test_uncal = predict_hybrid(pipe_cic, mlp_cic, X_holdout.iloc[test_idx])\n",
        "    y_test = y_holdout[test_idx]\n",
        "\n",
        "    with open(args.cic_temp_meta, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    T = float(meta.get(\"T\", 1.0))\n",
        "    p_test_temp = apply_temperature_scaling(p_test_uncal, T=T)\n",
        "\n",
        "    p_val_cal_uncal = predict_hybrid(pipe_cic, mlp_cic, X_holdout.iloc[val_cal_idx])\n",
        "    y_val_cal = y_holdout[val_cal_idx]\n",
        "    ir = fit_isotonic(p_val_cal_uncal, y_val_cal)\n",
        "    p_test_iso = np.asarray(ir.transform(p_test_uncal), dtype=float)\n",
        "\n",
        "    variants_cic = {\n",
        "        \"Uncalibrated\": p_test_uncal,\n",
        "        \"Temp-scaled\": p_test_temp,\n",
        "        \"Isotonic\": p_test_iso,\n",
        "    }\n",
        "\n",
        "    audit = {\n",
        "        \"cic_train_file_n\": int(len(df_train)),\n",
        "        \"cic_holdout_file_n\": int(len(df_holdout)),\n",
        "        \"n_val_sel\": int(len(val_sel_idx)),\n",
        "        \"n_val_cal\": int(len(val_cal_idx)),\n",
        "        \"n_test\": int(len(test_idx)),\n",
        "        \"random_state\": int(args.random_state),\n",
        "        \"temperature_T\": float(T),\n",
        "    }\n",
        "    with open(outdir / \"optionA_split_audit.json\", \"w\") as f:\n",
        "        json.dump(audit, f, indent=2)\n",
        "\n",
        "    roc_m = plot_roc_overlay(y_test, variants_cic, outdir / \"roc_overlay_CIC_OptionA.png\",\n",
        "                             \"CIC-IoMT Option A: ROC Overlay (Held-out Test)\")\n",
        "    pr_m = plot_pr_overlay(y_test, variants_cic, outdir / \"pr_overlay_CIC_OptionA.png\",\n",
        "                           \"CIC-IoMT Option A: PR Overlay (Held-out Test)\")\n",
        "    rel_m = plot_reliability_overlay(y_test, variants_cic, outdir / \"reliability_overlay_CIC_OptionA.png\",\n",
        "                                     \"CIC-IoMT Option A: Reliability Diagram (Held-out Test)\", n_bins=15)\n",
        "\n",
        "    plot_metrics = {}\n",
        "    for k in variants_cic:\n",
        "        plot_metrics.setdefault(k, {}).update(roc_m.get(k, {}))\n",
        "        plot_metrics.setdefault(k, {}).update(pr_m.get(k, {}))\n",
        "        plot_metrics.setdefault(k, {}).update(rel_m.get(k, {}))\n",
        "    with open(outdir / \"optionA_plot_metrics.json\", \"w\") as f:\n",
        "        json.dump(plot_metrics, f, indent=2)\n",
        "\n",
        "    # -----------------\n",
        "    # Optional NF in-domain plots\n",
        "    # -----------------\n",
        "    if args.nf_test_csv and args.nf_pipe_joblib and args.nf_mlp_joblib:\n",
        "        df_nf = pd.read_csv(args.nf_test_csv)\n",
        "        if args.nf_label_col not in df_nf.columns:\n",
        "            raise ValueError(f\"NF label column '{args.nf_label_col}' not found in NF file.\")\n",
        "        y_nf = ensure_binary_labels(df_nf[args.nf_label_col].values)\n",
        "        drop_nf = set(args.nf_drop_cols + [args.nf_label_col])\n",
        "        X_nf = df_nf.drop(columns=[c for c in drop_nf if c in df_nf.columns])\n",
        "\n",
        "        pipe_nf = joblib.load(args.nf_pipe_joblib)\n",
        "        mlp_nf = joblib.load(args.nf_mlp_joblib)\n",
        "        p_nf = predict_hybrid(pipe_nf, mlp_nf, X_nf)\n",
        "\n",
        "        variants_nf = {\"NF in-domain\": p_nf}\n",
        "        nf_roc = plot_roc_overlay(y_nf, variants_nf, outdir / \"roc_NF_in_domain.png\",\n",
        "                                  \"NF-ToN-IoT In-domain: ROC (Test)\")\n",
        "        nf_pr = plot_pr_overlay(y_nf, variants_nf, outdir / \"pr_NF_in_domain.png\",\n",
        "                                \"NF-ToN-IoT In-domain: PR (Test)\")\n",
        "        nf_rel = plot_reliability_overlay(y_nf, variants_nf, outdir / \"reliability_NF_in_domain.png\",\n",
        "                                          \"NF-ToN-IoT In-domain: Reliability (Test)\", n_bins=15)\n",
        "\n",
        "        nf_metrics = {\"NF in-domain\": {}}\n",
        "        nf_metrics[\"NF in-domain\"].update(nf_roc[\"NF in-domain\"])\n",
        "        nf_metrics[\"NF in-domain\"].update(nf_pr[\"NF in-domain\"])\n",
        "        nf_metrics[\"NF in-domain\"].update(nf_rel[\"NF in-domain\"])\n",
        "        with open(outdir / \"nf_plot_metrics.json\", \"w\") as f:\n",
        "            json.dump(nf_metrics, f, indent=2)\n",
        "\n",
        "    # -----------------\n",
        "    # Optional robustness curves from JSON metrics\n",
        "    # -----------------\n",
        "    if args.metrics_dir:\n",
        "        curves = parse_robustness_metrics(Path(args.metrics_dir))\n",
        "        with open(outdir / \"robust_parse_summary.json\", \"w\") as f:\n",
        "            json.dump(curves, f, indent=2)\n",
        "\n",
        "        plot_robustness_curve(curves, \"auroc\", outdir / \"robust_NF_auroc_vs_eps.png\",\n",
        "                              \"NF-ToN-IoT Robustness: AUROC vs epsilon\")\n",
        "        plot_robustness_curve(curves, \"aupr\", outdir / \"robust_NF_aupr_vs_eps.png\",\n",
        "                              \"NF-ToN-IoT Robustness: AUPR vs epsilon\")\n",
        "        plot_robustness_curve(curves, \"fpr95\", outdir / \"robust_NF_fpr95_vs_eps.png\",\n",
        "                              \"NF-ToN-IoT Robustness: FPR@95%DR vs epsilon\")\n",
        "\n",
        "    print(\"Done. Outputs written to:\", str(outdir))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mTWwJ4HqqXlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e86cb76"
      },
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load pipeline to inspect\n",
        "pipe_cic = joblib.load('CIC_OptionA_pipe.joblib')\n",
        "\n",
        "# Check the first step of the pipeline\n",
        "first_step_name, first_step_obj = pipe_cic.steps[0]\n",
        "print(f\"First step: {first_step_name} -> {type(first_step_obj)}\")\n",
        "\n",
        "# If it is SafeNaNDropper, check the mask\n",
        "if hasattr(first_step_obj, 'keep_mask_'):\n",
        "    mask = first_step_obj.keep_mask_\n",
        "    print(f\"Keep mask shape: {mask.shape}\")\n",
        "    print(f\"Keep mask sum (features kept): {mask.sum()}\")\n",
        "    print(f\"Keep mask values: {mask.astype(int)}\")\n",
        "else:\n",
        "    print(\"First step does not have keep_mask_ attribute.\")\n",
        "\n",
        "# Check current data columns\n",
        "df_check = pd.read_csv('CIC_IoMT_2024_WiFi_MQTT_test.csv', nrows=1)\n",
        "print(f\"Current columns ({len(df_check.columns)}): {df_check.columns.tolist()}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}